{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature Engineering + Discovery Engine with custom features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "import itertools\n",
    "\n",
    "# Import joblib at the very top for parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "# Define the explicit list of tradable tickers (full sheet names from your All_Tickers copy.xlsx)\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "]\n",
    "\n",
    "# Define the file paths\n",
    "MAIN_DATA_FILE = 'All_Tickers copy.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_no_nan_cols.xlsx'\n",
    "\n",
    "# Setup Generation Configuration\n",
    "SETUP_LENGTHS_TO_EXPLORE = [2, 3, 4] # Explore setups with 2 and 3 conditions for faster execution\n",
    "MIN_INITIAL_SUPPORT_FILTER = 10 # Minimum number of trigger days for a setup to be considered\n",
    "\n",
    "# Option Simulation Configuration\n",
    "OPTION_SIM_HORIZON_DAYS = 10 # Days to expiration for simulated options\n",
    "RISK_FREE_RATE = 0.01 # Annual risk-free rate for option premium estimation\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooks â€¦')\n",
    "raw = None # Initialize raw to None\n",
    "\n",
    "# --- Custom Data Loading Function to handle sheet names as prefixes ---\n",
    "def load_and_merge_excel(file_path, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "\n",
    "        for sh_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name)\n",
    "            # Prepend sheet name to all columns except 'Date'\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                # Use outer merge to keep all dates from all sheets\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found. Please ensure the file is in the correct directory.\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df\n",
    "\n",
    "# Load main data\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE)\n",
    "\n",
    "# Load macro data if main data was loaded successfully\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame({'Date': pd.to_datetime([])}) # Fallback to empty DF if main load failed\n",
    "\n",
    "# Final cleaning and sorting\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # --- FIX: Handle duplicate dates and set 'Date' as index ---\n",
    "    if 'Date' in raw.columns:\n",
    "        raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "        raw = raw.drop_duplicates(subset=['Date'], keep='last') # Drop duplicates before setting index\n",
    "        raw.set_index('Date', inplace=True) # Set 'Date' as the DataFrame's index\n",
    "        raw.index = pd.to_datetime(raw.index) # Ensure it's a DatetimeIndex\n",
    "        raw.sort_index(inplace=True) # Ensure sorted index for .reindex(method='nearest')\n",
    "    else:\n",
    "        print(\"Warning: 'Date' column not found to set as index. Some lookups might be less efficient or prone to error.\")\n",
    "    # --- END FIX ---\n",
    "\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame({'Date': pd.to_datetime([])}) # Ensure raw is a DataFrame\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "print('Example columns after loading:')\n",
    "print(raw.columns[:5].tolist()) # 'Date' is now the index, so it won't be listed here\n",
    "\n",
    "\n",
    "# --- Dynamic Ticker Identification Refined ---\n",
    "# all_tickers will now refer to ALL unique ticker prefixes found in columns after loading,\n",
    "# Adjusted to access columns correctly, as 'Date' is now the index.\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c])))\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total',\n",
    "                           '30', '10', '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC',\n",
    "                           'NFP', 'JOBS', 'CPI', 'CTII10', 'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT',\n",
    "                           'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM', 'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes)))\n",
    "\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {all_tickers}')\n",
    "print(f'Actual tradable tickers for returns: {TRADABLE_TICKERS}')\n",
    "\n",
    "\n",
    "# --- Revised Helper functions (using raw.index as source) ---\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    \"\"\"\n",
    "    Finds the first column name in raw that matches the pattern 'ticker_full_name_substr'.\n",
    "    Handles cases where substr might be 'PX_LAST' and the actual column name is 'ticker_full_name_Last_Price_PX_LAST'.\n",
    "    \"\"\"\n",
    "    if substr == 'PX_LAST':\n",
    "        potential_col_name_long_price = f\"{ticker_full_name}_Last_Price_PX_LAST\"\n",
    "        if potential_col_name_long_price in raw.columns:\n",
    "            return potential_col_name_long_price\n",
    "        potential_col_name_short_px = f\"{ticker_full_name}_PX_LAST\"\n",
    "        if potential_col_name_short_px in raw.columns:\n",
    "            return potential_col_name_short_px\n",
    "    for c in raw.columns:\n",
    "        if c.startswith(ticker_full_name) and substr in c:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def safe_series(col_name):\n",
    "    \"\"\"Returns a column as a Series, or an empty Series if column does not exist.\"\"\"\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "# --- Helper function for Block Bootstrap Sharpe ---\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=1000, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"\n",
    "    Calculates the Sharpe Ratio using block bootstrapping to account for serial correlation.\n",
    "    \"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    sharpes = []\n",
    "    blocks = []\n",
    "    for i in range(0, len(returns_series), block_size):\n",
    "        block = returns_series.iloc[i : i + block_size]\n",
    "        if not block.empty:\n",
    "            blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size))\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = []\n",
    "        sampled_blocks_indices = np.random.choice(len(blocks), n_blocks_to_sample, replace=True)\n",
    "        for idx in sampled_blocks_indices:\n",
    "            resampled_returns_list.append(blocks[idx])\n",
    "\n",
    "        resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)]\n",
    "\n",
    "        if resampled_returns.std() > 1e-9:\n",
    "            daily_sharpe = resampled_returns.mean() / resampled_returns.std()\n",
    "            if annualize:\n",
    "                sharpes.append(daily_sharpe * np.sqrt(trading_days_per_year))\n",
    "            else:\n",
    "                sharpes.append(daily_sharpe)\n",
    "        else:\n",
    "            sharpes.append(0.0)\n",
    "\n",
    "    sharpes_sorted = sorted(sharpes)\n",
    "    if not sharpes_sorted:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    median_sharpe = np.median(sharpes_sorted)\n",
    "    lower_ci = sharpes_sorted[int(0.05 * num_iterations)]\n",
    "    upper_ci = sharpes_sorted[int(0.95 * num_iterations)]\n",
    "\n",
    "    return median_sharpe, lower_ci, upper_ci\n",
    "\n",
    "\n",
    "# --- Option Payoff Simulation Helpers ---\n",
    "def estimate_atm_premium(S, ivol, T_days, r_annual=RISK_FREE_RATE, option_type='call'):\n",
    "    \"\"\"Estimates ATM option premium using a simplified Black-Scholes like approach.\"\"\"\n",
    "    if S <= 0 or ivol <= 0 or T_days <= 0:\n",
    "        return 0.001\n",
    "    T_years = T_days / 252.0\n",
    "    premium_estimate = 0.4 * S * ivol * np.sqrt(T_years)\n",
    "    return max(premium_estimate, 0.001)\n",
    "\n",
    "def simulate_option_pnl(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    \"\"\"Simulates PnL for buying a simple ATM call or put.\"\"\"\n",
    "    if pd.isna(current_price) or pd.isna(future_price) or pd.isna(ivol_at_entry):\n",
    "        return np.nan\n",
    "    strike = current_price\n",
    "    if entry_direction == 'long':\n",
    "        premium = estimate_atm_premium(current_price, ivol_at_entry, horizon_days, option_type='call')\n",
    "        payoff = max(future_price - strike, 0)\n",
    "        pnl = payoff - premium\n",
    "    elif entry_direction == 'short':\n",
    "        premium = estimate_atm_premium(current_price, ivol_at_entry, horizon_days, option_type='put')\n",
    "        payoff = max(strike - future_price, 0)\n",
    "        pnl = payoff - premium\n",
    "    else:\n",
    "        pnl = np.nan\n",
    "    return pnl\n",
    "\n",
    "\n",
    "# --- Fractional Differencing Helper ---\n",
    "def frac_diff(series, d=0.5, window='full'):\n",
    "    \"\"\"Computes fractionally differenced series.\"\"\"\n",
    "    if not isinstance(series, pd.Series):\n",
    "        series = pd.Series(series)\n",
    "    weights = [1.]\n",
    "    for k in range(1, series.shape[0]):\n",
    "        weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1])\n",
    "    output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(series.shape[0]):\n",
    "        if window == 'full':\n",
    "            start = 0\n",
    "        else:\n",
    "            start = max(0, i - window + 1)\n",
    "        subset = series.iloc[start : i + 1]\n",
    "        current_weights = weights[-(i - start + 1):]\n",
    "        if len(subset) == len(current_weights):\n",
    "            output.iloc[i] = np.dot(current_weights, subset)\n",
    "    return output.dropna()\n",
    "\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "print('\\nEngineering custom features â€¦')\n",
    "feat = pd.DataFrame(index=raw.index) # Use raw.index directly as 'Date' is now the index\n",
    "\n",
    "# 1. VOLATILITY-BASED FEATURES\n",
    "print(\"  - Volatility-based features\")\n",
    "for ticker in TRADABLE_TICKERS:\n",
    "    ivol_10d_col = first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D')\n",
    "    ivol_30d_col = first_col_containing(ticker, '30_Day_Call_Implied_Volatility_CALL_IMP_VOL_30D')\n",
    "    ivol_60d_col = first_col_containing(ticker, '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D')\n",
    "    if ivol_60d_col and ivol_10d_col:\n",
    "        feat[f'{ticker}_IVOL_Term_Structure_Slope'] = safe_series(ivol_60d_col) - safe_series(ivol_10d_col)\n",
    "    call_40d_ivol_col = first_col_containing(ticker, '1st_Month_Call_Imp_Vol_40_Delta_LIVE_1M_CALL_IMP_VOL_40DELTA_DFLT')\n",
    "    put_50d_ivol_col = first_col_containing(ticker, '1st_Month_Put_Imp_Vol_50_Delta_LIVE_1M_PUT_IMP_VOL_50DELTA_DFLT')\n",
    "    if call_40d_ivol_col and put_50d_ivol_col:\n",
    "        feat[f'{ticker}_IVOL_Skew_Approx'] = safe_series(put_50d_ivol_col) - safe_series(call_40d_ivol_col)\n",
    "    specific_ivol_suffixes = ['IVOL_SIGMA', 'IVOL_DELTA', 'IVOL_MONEYNESS', 'CALL_IMP_VOL_30D', 'CALL_IMP_VOL_10D', 'CALL_IMP_VOL_60D', 'PUT_IMP_VOL_30D', 'PUT_IMP_VOL_10D', 'PUT_IMP_VOL_60D']\n",
    "    for ivol_suffix in specific_ivol_suffixes:\n",
    "        col_name = first_col_containing(ticker, ivol_suffix)\n",
    "        if col_name:\n",
    "            if '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D' in col_name and \\\n",
    "               first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D'):\n",
    "                feat[f'{ticker}_IVOL_Call_Slope'] = safe_series(col_name) - \\\n",
    "                                                    safe_series(first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D'))\n",
    "            diff = safe_series(col_name).diff()\n",
    "            z = (diff - diff.rolling(30).mean()) / diff.rolling(30).std()\n",
    "            feat[col_name+'_shock'] = (z > 2).astype(int)\n",
    "            vol_col_name = first_col_containing(ticker, 'VOLUME')\n",
    "            if vol_col_name:\n",
    "                feat[col_name+'_div'] = safe_series(col_name) / safe_series(vol_col_name)\n",
    "            else:\n",
    "                feat[col_name+'_div'] = np.nan\n",
    "\n",
    "# 2. DERIV FLOW & SENTIMENT\n",
    "print(\"  - Deriv Flow & Sentiment features\")\n",
    "for ticker in all_tickers:\n",
    "    pc_col = first_col_containing(ticker, 'PUT_CALL_VOLUME_RATIO_CUR_DAY')\n",
    "    if pc_col:\n",
    "        feat[pc_col+'_ema5']=safe_series(pc_col).ewm(span=5,adjust=False).mean()\n",
    "    open_int_col = first_col_containing(ticker, 'OPEN_INT_TOTAL_CALL')\n",
    "    if open_int_col:\n",
    "        feat[open_int_col+'_chg3']=safe_series(open_int_col).pct_change(3)\n",
    "    volm_col = first_col_containing(ticker, 'Volume_-_Realtime_VOLUME')\n",
    "    if volm_col:\n",
    "        feat[volm_col+'_z']=(safe_series(volm_col)-safe_series(volm_col).rolling(30).mean())/safe_series(volm_col).rolling(30).std()\n",
    "for ticker in all_tickers:\n",
    "    open_int_t_col = first_col_containing(ticker, 'OPEN_INT_TOTAL_CALL')\n",
    "    ivol_10d_col = first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D')\n",
    "    if open_int_t_col and ivol_10d_col:\n",
    "        feat[f'{ticker}_smart_money_flag'] = ((safe_series(open_int_t_col).pct_change() > 0) & \\\n",
    "                                              (safe_series(ivol_10d_col).pct_change() > 0)).astype(int)\n",
    "\n",
    "# 3. CROSS-ASSET CORRELATIONS\n",
    "print(\"  - Cross-Asset Correlations\")\n",
    "correlation_universe = sorted(list(set(TRADABLE_TICKERS + [t for t in all_tickers if t in ['DXY Curncy', 'USGG10YR Index', 'SPX Index', 'CO1 Comdty', 'USGG2YR Index']])))\n",
    "dynamic_pairs = []\n",
    "for i in range(len(TRADABLE_TICKERS)):\n",
    "    for j in range(i + 1, len(TRADABLE_TICKERS)):\n",
    "        dynamic_pairs.append((TRADABLE_TICKERS[i], TRADABLE_TICKERS[j]))\n",
    "dynamic_pairs.extend([\n",
    "    ('SPY US Equity', 'VIX Index'),\n",
    "    ('SPY US Equity', 'USGG10YR Index'),\n",
    "    ('SPY US Equity', 'DXY Curncy'),\n",
    "    ('SPY US Equity', 'CO1 Comdty'),\n",
    "    ('SPY US Equity', 'USGG2YR Index'),\n",
    "    ('VIX Index', 'USGG10YR Index'),\n",
    "    ('VIX Index', 'DXY Curncy'),\n",
    "    ('VIX Index', 'CO1 Comdty'),\n",
    "])\n",
    "dynamic_pairs = list(set(dynamic_pairs))\n",
    "for t1,t2 in dynamic_pairs:\n",
    "    p1=first_col_containing(t1,'PX_LAST'); p2=first_col_containing(t2,'PX_LAST')\n",
    "    if p1 and p2:\n",
    "        s1=safe_series(p1); s2=safe_series(p2)\n",
    "        if not s1.empty and not s2.empty:\n",
    "            aligned_data = pd.DataFrame({'s1': s1, 's2': s2}).dropna()\n",
    "            if len(aligned_data) > 60:\n",
    "                c20=aligned_data['s1'].rolling(20).corr(aligned_data['s2'])\n",
    "                c60=aligned_data['s1'].rolling(60).corr(aligned_data['s2'])\n",
    "                feat[f'{t1}_{t2}_c20']=c20\n",
    "                feat[f'{t1}_{t2}_c60']=c60\n",
    "                feat[f'{t1}_{t2}_cZ']=(c20-c20.rolling(60).mean())/c20.rolling(60).std()\n",
    "                feat[f'{t1}_{t2}_cDelta']=c20-c60\n",
    "                ret1 = s1.pct_change().dropna()\n",
    "                ret2 = s2.pct_change().dropna()\n",
    "                aligned_returns = pd.DataFrame({'ret1': ret1, 'ret2': ret2}).dropna()\n",
    "                if not aligned_returns.empty and aligned_returns['ret2'].var() != 0:\n",
    "                    rolling_beta = aligned_returns['ret1'].rolling(window=60).cov(aligned_returns['ret2']) / \\\n",
    "                                   aligned_returns['ret2'].rolling(window=60).var()\n",
    "                    feat[f'{t1}_{t2}_rolling_beta'] = rolling_beta\n",
    "\n",
    "# 4. MACRO TRIGGERS\n",
    "print(\"  - Macro Trigger features\")\n",
    "dxy_px=first_col_containing('DXY Curncy','PX_LAST')\n",
    "ust10_px=first_col_containing('USGG10YR Index','PX_LAST')\n",
    "spy_px=first_col_containing('SPY US Equity','PX_LAST')\n",
    "vix_px=first_col_containing('VIX Index','PX_LAST')\n",
    "feat['MPI']=(safe_series(dxy_px).pct_change().rolling(3).sum()+safe_series(ust10_px).pct_change().rolling(3).sum()).shift(0)\n",
    "feat['VIX_gt20']=(safe_series(vix_px)>20).astype(int)\n",
    "feat['DXY_rising']=(safe_series(dxy_px).pct_change()>0).astype(int)\n",
    "feat['SPY_below_MA20']=(safe_series(spy_px)<safe_series(spy_px).rolling(20).mean()).astype(int)\n",
    "feat['fear_overdrive']=((feat['VIX_gt20']==1)&(feat['DXY_rising']==1)&(feat['SPY_below_MA20']==1)).astype(int)\n",
    "xlk_px=first_col_containing('XLK US Equity','PX_LAST'); xle_px=first_col_containing('XLE US Equity','PX_LAST')\n",
    "feat['sector_rotation']=safe_series(xlk_px).pct_change(5)-safe_series(xle_px).pct_change(5)\n",
    "\n",
    "# --- NEW MACRO FEATURES from Macro_tickers_no_nan_cols.xlsx ---\n",
    "ust2_px = first_col_containing('USGG2YR Index', 'PX_LAST')\n",
    "if ust10_px and ust2_px:\n",
    "    feat['UST10Y_2Y_Spread'] = safe_series(ust10_px) - safe_series(ust2_px)\n",
    "    feat['UST10Y_2Y_Spread_chg'] = feat['UST10Y_2Y_Spread'].pct_change()\n",
    "cpi_yoy_px = first_col_containing('CPI YOY Index', 'PX_LAST')\n",
    "cpi_chng_px = first_col_containing('CPI CHNG Index', 'PX_LAST')\n",
    "if cpi_yoy_px:\n",
    "    feat['CPI_YOY_mom3'] = safe_series(cpi_yoy_px).pct_change(3)\n",
    "    feat['CPI_YOY_z'] = (safe_series(cpi_yoy_px) - safe_series(cpi_yoy_px).rolling(12).mean()) / safe_series(cpi_yoy_px).rolling(12).std()\n",
    "if cpi_chng_px:\n",
    "    feat['CPI_CHNG_mom3'] = safe_series(cpi_chng_px).pct_change(3)\n",
    "injcjc_px = first_col_containing('INJCJC Index', 'PX_LAST')\n",
    "nfp_tch_px = first_col_containing('NFP TCH Index', 'PX_LAST')\n",
    "jobs_us_equity_px = first_col_containing('JOBS US Equity', 'PX_LAST')\n",
    "if injcjc_px:\n",
    "    feat['INJCJC_shock'] = (safe_series(injcjc_px).diff() > safe_series(injcjc_px).diff().rolling(20).std() * 2).astype(int)\n",
    "if nfp_tch_px:\n",
    "    feat['NFP_TCH_mom3'] = safe_series(nfp_tch_px).pct_change(3)\n",
    "if jobs_us_equity_px:\n",
    "    feat['JOBS_US_Equity_mom3'] = safe_series(jobs_us_equity_px).pct_change(3)\n",
    "ffa_comdty_px = first_col_containing('FFA Comdty', 'PX_LAST')\n",
    "ctii10_govt_px = first_col_containing('CTII10 Govt', 'PX_LAST')\n",
    "ussw10_curncy_px = first_col_containing('USSW10 Curncy', 'PX_LAST')\n",
    "mlcx3crt_index_px = first_col_containing('MLCX3CRT Index', 'PX_LAST')\n",
    "farbast_index_px = first_col_containing('FARBAST Index', 'PX_LAST')\n",
    "bspgcpus_index_px = first_col_containing('BSPGCPUS Index', 'PX_LAST')\n",
    "spcsusa_index_px = first_col_containing('SPCSUSA Index', 'PX_LAST')\n",
    "spcs20sm_index_px = first_col_containing('SPCS20SM Index', 'PX_LAST')\n",
    "conssent_index_px = first_col_containing('CONSSENT Index', 'PX_LAST')\n",
    "lf94truu_index_vol30d = first_col_containing('LF94TRUU Index', 'VOLATILITY_30D')\n",
    "if ffa_comdty_px: feat['FFA_Spread'] = safe_series(ffa_comdty_px) - safe_series(ust2_px)\n",
    "if ctii10_govt_px: feat['CTII10_mom'] = safe_series(ctii10_govt_px).pct_change()\n",
    "if ussw10_curncy_px: feat['USSW10_chg'] = safe_series(ussw10_curncy_px).pct_change()\n",
    "if mlcx3crt_index_px: feat['MLCX3CRT_chg'] = safe_series(mlcx3crt_index_px).pct_change()\n",
    "if farbast_index_px: feat['FARBAST_mom'] = safe_series(farbast_index_px).pct_change()\n",
    "if bspgcpus_index_px: feat['BSPGCPUS_mom'] = safe_series(bspgcpus_index_px).pct_change()\n",
    "if spcsusa_index_px: feat['SPCSUSA_mom'] = safe_series(spcsusa_index_px).pct_change()\n",
    "if spcs20sm_index_px: feat['SPCS20SM_mom'] = safe_series(spcs20sm_index_px).pct_change()\n",
    "if conssent_index_px: feat['CONSSENT_mom'] = safe_series(conssent_index_px).pct_change()\n",
    "if lf94truu_index_vol30d: feat['LF94TRUU_Vol_Signal'] = safe_series(lf94truu_index_vol30d) / safe_series(lf94truu_index_vol30d).rolling(60).mean()\n",
    "\n",
    "\n",
    "# 5. MOMENTUM / VOL FRACTALS\n",
    "print(\"  - Momentum/Vol Fractals\")\n",
    "for tk in all_tickers:\n",
    "    px_col=first_col_containing(tk,'PX_LAST')\n",
    "    if not px_col: continue\n",
    "    px=safe_series(px_col)\n",
    "    mom5=px.pct_change(5)\n",
    "    vol20=px.pct_change().rolling(20).std()\n",
    "    feat[tk+'_mom5_vol20']=mom5/vol20\n",
    "    ma=px.rolling(20).mean(); std=px.rolling(20).std()\n",
    "    feat[tk+'_pctB']=(px-(ma-2*std))/(4*std)\n",
    "    if not px.empty and len(px) > 100:\n",
    "        try:\n",
    "            feat[f'{tk}_frac_diff_0_5'] = frac_diff(px, d=0.5, window=100)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute fractional differencing for {tk}: {e}\")\n",
    "            feat[f'{tk}_frac_diff_0_5'] = np.nan\n",
    "\n",
    "\n",
    "# 6. Shift +1 day and merge basic returns\n",
    "feat=feat.shift(1)\n",
    "panel = feat.copy()\n",
    "\n",
    "# Prepare returns for evaluation based *only* on TRADABLE_TICKERS\n",
    "price_cols_for_returns = []\n",
    "for ticker_full_name in TRADABLE_TICKERS:\n",
    "    px_col_name = first_col_containing(ticker_full_name, 'PX_LAST')\n",
    "    if px_col_name:\n",
    "        price_cols_for_returns.append(px_col_name)\n",
    "    else:\n",
    "        print(f\"Warning: PX_LAST column not found for tradable ticker '{ticker_full_name}'. It will be excluded from return calculations.\")\n",
    "\n",
    "# --- MODIFIED: Prices DataFrame creation (raw is already indexed by Date) ---\n",
    "prices=raw[price_cols_for_returns].copy()\n",
    "returns={h:prices.pct_change(h).shift(-h) for h in [1,3,5,10,21]}\n",
    "\n",
    "\n",
    "# Merge tradable returns into panel\n",
    "for tk_full_name in TRADABLE_TICKERS:\n",
    "    px_col_name = first_col_containing(tk_full_name, 'PX_LAST')\n",
    "    if px_col_name and px_col_name in raw.columns:\n",
    "        ret_series = raw[px_col_name].pct_change().shift(1).rename(f'{tk_full_name}_ret1')\n",
    "        panel = pd.concat([panel, ret_series], axis=1)\n",
    "\n",
    "print('Engineered panel shape:',panel.shape)\n",
    "\n",
    "\n",
    "# --- Primitive Signal Generation ---\n",
    "print('\\nBuilding primitive signals from engineered panel â€¦')\n",
    "signals={}\n",
    "# Helper functions for inferring signal direction and type (defined once globally)\n",
    "def get_primitive_direction(primitive_name):\n",
    "    \"\"\"Infers the directional bias (+1 for long, -1 for short, 0 for neutral) of a primitive signal.\"\"\"\n",
    "    if '>80' in primitive_name or 'z>1.5' in primitive_name or 'ma5>ma20' in primitive_name or 'rising' in primitive_name: return 1\n",
    "    elif '<20' in primitive_name or 'z<-1.name' in primitive_name or 'ma5<ma20' in primitive_name or 'below_MA' in primitive_name: return -1\n",
    "    elif 'shock' in primitive_name: return 0\n",
    "    elif 'slope' in primitive_name: return 0\n",
    "    elif 'Spread' in primitive_name: return 0\n",
    "    elif 'Vol_Signal' in primitive_name: return 0\n",
    "    return 0\n",
    "\n",
    "def get_primitive_signal_type(primitive_name):\n",
    "    \"\"\"Infers the broad category/type of a primitive signal based on its name.\"\"\"\n",
    "    if 'IVOL' in primitive_name or 'VIX' in primitive_name or 'vol' in primitive_name or '_shock' in primitive_name: return 'volatility'\n",
    "    elif 'mom' in primitive_name or 'pctB' in primitive_name or '_chg' in primitive_name or '_ret' in primitive_name or 'rising' in primitive_name: return 'momentum'\n",
    "    elif '_c20' in primitive_name or '_c60' in primitive_name or '_cZ' in primitive_name or '_cDelta' in primitive_name or '_beta' in primitive_name: return 'correlation'\n",
    "    elif 'DXY' in primitive_name or 'USGG' in primitive_name or 'MPI' in primitive_name or 'fear_overdrive' in primitive_name or 'CPI' in primitive_name or 'INJCJC' in primitive_name or 'NFP' in primitive_name or 'JOBS' in primitive_name or 'FFA' in primitive_name or 'CTII10' in primitive_name or 'USSW10' in primitive_name or 'MLCX3CRT' in primitive_name or 'FARBAST' in primitive_name or 'BSPGCPUS' in primitive_name or 'SPCSUSA' in primitive_name or 'SPCS20SM' in primitive_name or 'CONSSENT' in primitive_name or 'LF94TRUU' in primitive_name: return 'macro'\n",
    "    elif 'PUT_CALL_VOLUME_RATIO' in primitive_name or 'smart_money_flag' in primitive_name or 'Short_Interest_Ratio' in primitive_name: return 'sentiment'\n",
    "    elif 'VOLUME' in primitive_name: return 'volume'\n",
    "    elif 'OPEN_INT' in primitive_name: return 'open_interest'\n",
    "    elif 'frac_diff' in primitive_name: return 'fractional_differencing'\n",
    "    return 'other'\n",
    "\n",
    "for col in panel.columns.drop('Date',errors='ignore'): # 'Date' is now index, so 'drop' might not find it\n",
    "    s=panel[col]\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        s_clean = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if s_clean.empty or s_clean.std() == 0: continue\n",
    "        rank=s_clean.rank(pct=True)\n",
    "        signals[col+'>80']=rank>0.8\n",
    "        signals[col+'<20']=rank<0.2\n",
    "        rolling_std_60 = s_clean.rolling(60).std()\n",
    "        valid_std_mask = rolling_std_60 > 1e-9\n",
    "        z = pd.Series(np.nan, index=s_clean.index, dtype=float)\n",
    "        z[valid_std_mask] = (s_clean - s_clean.rolling(60).mean())[valid_std_mask] / rolling_std_60[valid_std_mask]\n",
    "        signals[col+'_z>1.5']=z>1.5\n",
    "        signals[col+'_z<-1.5']=z<-1.5\n",
    "        ma5=s_clean.rolling(5).mean(); ma20=s_clean.rolling(20).mean()\n",
    "        signals[col+'_ma5>ma20']=ma5>ma20\n",
    "print('Total primitive signals:',len(signals))\n",
    "\n",
    "# --- NEW ADDITION: Primitive Signal Pre-filtering for Combinations ---\n",
    "#print(f\"\\nEvaluating individual primitive signal performance for pre-selection...\")\n",
    "#primitive_performances = {}\n",
    "#for col_name, signal_series in signals.items():\n",
    "    #if signal_series.sum() >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "        #primitive_trigger_dates = signal_series[signal_series].index\n",
    "        #if not primitive_trigger_dates.empty:\n",
    "         #   mean_returns_on_trigger = returns[10].loc[primitive_trigger_dates].mean(axis=1).dropna()\n",
    "          #  if not mean_returns_on_trigger.empty and mean_returns_on_trigger.std() > 0.0001:\n",
    "           #     block_size_for_primitive = min(10, max(1, len(mean_returns_on_trigger) // 2))\n",
    "            #    median_sharpe, _, _ = block_bootstrap_sharpe(mean_returns_on_trigger, block_size=block_size_for_primitive)\n",
    "             #   primitive_performances[col_name] = median_sharpe\n",
    "            #else:\n",
    "             #   primitive_performances[col_name] = 0.0\n",
    "#TOP_N_PRIMITIVES = 20\n",
    "#filtered_primitive_names = [name for name, sharpe in primitive_performances.items() if sharpe > 0.01]\n",
    "#top_n_primitive_names = sorted(filtered_primitive_names, key=lambda x: primitive_performances[x], reverse=True)[:TOP_N_PRIMITIVES]\n",
    "#primitive_names = top_n_primitive_names # This updates the list used for combinations\n",
    "#print(f\"Pre-selected {len(primitive_names)} primitive signals for combination generation (from {len(signals)} total).\")\n",
    "\n",
    "primitive_names = list(signals.keys())\n",
    "\n",
    "\n",
    "# --- MODIFICATION: Setup Generation ---\n",
    "all_candidate_setups = []\n",
    "setup_id_counter = 1\n",
    "print(f'\\nGenerating setups with lengths {SETUP_LENGTHS_TO_EXPLORE} and applying initial support filter (min_support={MIN_INITIAL_SUPPORT_FILTER}) â€¦')\n",
    "for k in SETUP_LENGTHS_TO_EXPLORE:\n",
    "    if k > len(primitive_names):\n",
    "        print(f\"Warning: Cannot generate setups of length {k} as only {len(primitive_names)} primitives are available. Skipping.\")\n",
    "        continue\n",
    "    for conds_tuple in itertools.combinations(primitive_names, k):\n",
    "        conds_list = list(conds_tuple)\n",
    "        try:\n",
    "            valid_combo_signals = [signals[c] for c in conds_list if c in signals]\n",
    "            if not valid_combo_signals: continue\n",
    "            current_mask = functools.reduce(lambda a,b: a & b, valid_combo_signals)\n",
    "            current_support = current_mask.sum()\n",
    "        except KeyError:\n",
    "            current_support = 0\n",
    "        if current_support >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "            setup = {\n",
    "                'id': 'S' + str(setup_id_counter).zfill(4),\n",
    "                'conds': conds_list,\n",
    "                'setup_length': k,\n",
    "                'setup_type': 'concurrent',\n",
    "                'support': current_support\n",
    "            }\n",
    "            all_candidate_setups.append(setup)\n",
    "            setup_id_counter += 1\n",
    "setups = all_candidate_setups\n",
    "print(f'Generated and filtered {len(setups)} candidate setups after initial support filter.')\n",
    "\n",
    "\n",
    "# --- Parallel Setup Evaluation Function (defined ONCE) ---\n",
    "def evaluate_one_setup(setup):\n",
    "    \"\"\"Helper function to evaluate a single setup for parallel processing.\"\"\"\n",
    "    summary_row_for_setup = {}\n",
    "    trigger_records_for_this_setup = []\n",
    "    sid=setup['id']; conds=setup['conds']\n",
    "    valid_signals = [signals[c] for c in conds if c in signals]\n",
    "    if not valid_signals: return None, None\n",
    "    mask=functools.reduce(lambda a,b: a & b, valid_signals)\n",
    "    dates=mask[mask].index\n",
    "    support=len(dates)\n",
    "    if support < MIN_INITIAL_SUPPORT_FILTER: return None, None\n",
    "\n",
    "    direction_score = 0\n",
    "    signal_type_counts = {}\n",
    "    for cond in conds:\n",
    "        direction_score += get_primitive_direction(cond)\n",
    "        s_type = get_primitive_signal_type(cond)\n",
    "        signal_type_counts[s_type] = signal_type_counts.get(s_type, 0) + 1\n",
    "    entry_direction = 'mixed'\n",
    "    if direction_score > 0: entry_direction = 'long'\n",
    "    elif direction_score < 0: entry_direction = 'short'\n",
    "    dominant_signal_type = 'unknown'\n",
    "    if signal_type_counts: dominant_signal_type = max(signal_type_counts, key=signal_type_counts.get)\n",
    "\n",
    "    first_trigger_date = dates.min() if not dates.empty else pd.NaT\n",
    "    last_trigger_date = dates.max() if not dates.empty else pd.NaT\n",
    "    perf={'setup_id':sid,'feature_conditions':'+'.join(conds),'support':support,\n",
    "          'entry_direction': entry_direction,\n",
    "          'dominant_signal_type': dominant_signal_type,\n",
    "          'first_trigger_date': first_trigger_date,\n",
    "          'last_trigger_date': last_trigger_date\n",
    "         }\n",
    "\n",
    "    for h,label in zip([3,5,10,21],['accuracy_3d','avg_return_5d','sharpe_10d','hit_rate_21d']):\n",
    "        r = returns[h].reindex(dates)\n",
    "        if r.empty:\n",
    "            perf[label] = 0.0\n",
    "            continue\n",
    "        mean=r.mean(axis=1).dropna()\n",
    "        if mean.empty:\n",
    "            perf[label] = 0.0\n",
    "            continue\n",
    "        dir_correct=(mean>0).mean()\n",
    "        if 'accuracy' in label: perf[label]=dir_correct\n",
    "        elif 'avg' in label: perf[label]=mean.mean()\n",
    "        elif 'sharpe' in label:\n",
    "            bootstrap_block_size = min(h if h > 1 else 2, max(1, len(mean) // 2))\n",
    "            if mean.std() > 0.0001 and len(mean) >= 2:\n",
    "                median_sharpe, _, _ = block_bootstrap_sharpe(mean, block_size=bootstrap_block_size)\n",
    "                perf[label] = median_sharpe\n",
    "            else: perf[label] = 0.0\n",
    "        elif 'hit' in label: perf[label]=(r>0).stack().mean()\n",
    "    summary_row_for_setup = perf\n",
    "\n",
    "    for d_idx, d in enumerate(dates):\n",
    "        for tk_col_full_name in price_cols_for_returns:\n",
    "            tk_symbol_for_log = tk_col_full_name.split('_PX_LAST')[0].split('_Last_Price')[0]\n",
    "            current_px = raw[tk_col_full_name].get(d, np.nan)\n",
    "            ivol_col_for_sim = first_col_containing(tk_symbol_for_log, f'{OPTION_SIM_HORIZON_DAYS}_Day_Call_Implied_Volatility_CALL_IMP_VOL_{OPTION_SIM_HORIZON_DAYS}D')\n",
    "            if not ivol_col_for_sim: ivol_col_for_sim = first_col_containing(tk_symbol_for_log, '30_Day_Call_Implied_Volatility_CALL_IMP_VOL_30D')\n",
    "            if not ivol_col_for_sim: ivol_col_for_sim = first_col_containing(tk_symbol_for_log, '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D')\n",
    "            ivol_at_entry = raw[ivol_col_for_sim].get(d, np.nan) if ivol_col_for_sim else np.nan\n",
    "            option_pnl_10d = np.nan\n",
    "            if not pd.isna(current_px) and not pd.isna(ivol_at_entry) and current_px > 0 and ivol_at_entry > 0:\n",
    "                future_date_for_sim = d + pd.Timedelta(days=OPTION_SIM_HORIZON_DAYS)\n",
    "                future_px_series = raw[tk_col_full_name].reindex([future_date_for_sim], method='nearest', tolerance=pd.Timedelta(days=5))\n",
    "                future_px_for_sim = future_px_series.iloc[0] if not future_px_series.empty and not pd.isna(future_px_series.iloc[0]) else np.nan\n",
    "                if not pd.isna(future_px_for_sim):\n",
    "                    option_pnl_10d = simulate_option_pnl(current_px, future_px_for_sim, ivol_at_entry, OPTION_SIM_HORIZON_DAYS, entry_direction)\n",
    "            ret_vals={h:returns[h][tk_col_full_name].get(d, np.nan) if tk_col_full_name in returns[h].columns else np.nan for h in [1,3,5,10,21]}\n",
    "            trigger_records_for_this_setup.append({'date':d,'ticker':tk_symbol_for_log,'setup_id':sid,'matched':1,\n",
    "                                    'return_1d':ret_vals[1],'return_3d':ret_vals[3],'return_5d':ret_vals[5],\n",
    "                                    'return_10d':ret_vals[10],'return_21d':ret_vals[21],\n",
    "                                    'option_pnl_10d': option_pnl_10d})\n",
    "    return summary_row_for_setup, trigger_records_for_this_setup\n",
    "\n",
    "\n",
    "# --- Parallel processing with joblib (Executed ONCE) ---\n",
    "summary_rows=[]\n",
    "trigger_records=[]\n",
    "print(f\"Starting parallel evaluation of {len(setups)} setups using all CPU cores...\")\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup) for setup in setups)\n",
    "for perf, records in results:\n",
    "    if perf is not None:\n",
    "        summary_rows.append(perf)\n",
    "    if records is not None:\n",
    "        trigger_records.extend(records)\n",
    "\n",
    "\n",
    "# --- Final Output Generation ---\n",
    "summary_df=pd.DataFrame(summary_rows)\n",
    "\n",
    "# --- Basic Setup Lifecycle Tracking Derived Metrics ---\n",
    "if not summary_df.empty:\n",
    "    summary_df['first_trigger_date'] = pd.to_datetime(summary_df['first_trigger_date'])\n",
    "    summary_df['last_trigger_date'] = pd.to_datetime(summary_df['last_trigger_date'])\n",
    "    summary_df['setup_duration_days'] = (summary_df['last_trigger_date'] - summary_df['first_trigger_date']).dt.days\n",
    "    summary_df['avg_trigger_frequency_per_day'] = summary_df['support'] / summary_df['setup_duration_days'].replace(0, np.nan)\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "else:\n",
    "    print(\"Summary DataFrame is empty, skipping lifecycle metrics calculation.\")\n",
    "\n",
    "top = summary_df[summary_df['sharpe_10d'] != 0.0].sort_values('sharpe_10d',ascending=False).head(20)\n",
    "\n",
    "summary_df.to_csv('setup_results_summary.csv',index=False)\n",
    "trigger_df=pd.DataFrame(trigger_records)\n",
    "trigger_df.to_csv('setup_trigger_log.csv',index=False)\n",
    "top.to_json('top_setups.json',orient='records',indent=2)\n",
    "\n",
    "print('\\nDiscovery complete with engineered features')\n",
    "print(top.head())"
   ],
   "id": "125fbebc62a2fe63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature Engineering + Discovery Engine with custom features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "import itertools\n",
    "\n",
    "# Import joblib for parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "# Define the explicit list of tradable tickers (full sheet names from your All_Tickers copy.xlsx)\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "]\n",
    "\n",
    "# Define the file paths\n",
    "MAIN_DATA_FILE = 'All_Tickers copy.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_no_nan_cols.xlsx'\n",
    "\n",
    "# Setup Generation Configuration\n",
    "# Using iterative search for these lengths (1-feature to 4-feature setups)\n",
    "SETUP_LENGTHS_TO_EXPLORE = [1, 2, 3, 4]\n",
    "MIN_INITIAL_SUPPORT_FILTER = 10 # Minimum number of trigger days for a setup to be considered\n",
    "\n",
    "# Crucial: Controls how many \"best\" setups from one length are passed to the next iteration.\n",
    "# LOWER VALUE = FASTER RUNTIME (narrower search).\n",
    "# HIGHER VALUE = SLOWER RUNTIME (broader search).\n",
    "N_BEST_TO_PROPAGATE = 20 # Your current test value. Consider 500-2000 for broader search later.\n",
    "\n",
    "# Option Simulation Configuration\n",
    "OPTION_SIM_HORIZON_DAYS = 10 # Days to expiration for simulated options\n",
    "RISK_FREE_RATE = 0.01 # Annual risk-free rate for option premium estimation\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooks â€¦')\n",
    "raw = None # Initialize raw to None\n",
    "\n",
    "# --- Custom Data Loading Function to handle sheet names as prefixes ---\n",
    "def load_and_merge_excel(file_path, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "\n",
    "        for sh_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name)\n",
    "            # Prepend sheet name to all columns except 'Date'\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found. Please ensure the file is in the correct directory.\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df\n",
    "\n",
    "# Load main data\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE)\n",
    "\n",
    "# Load macro data if main data was loaded successfully\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame({'Date': pd.to_datetime([])})\n",
    "\n",
    "# Final cleaning and sorting\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # --- FIX: Handle duplicate dates and set 'Date' as index ---\n",
    "    if 'Date' in raw.columns:\n",
    "        raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "        raw = raw.drop_duplicates(subset=['Date'], keep='last')\n",
    "        raw.set_index('Date', inplace=True)\n",
    "        raw.index = pd.to_datetime(raw.index)\n",
    "        raw.sort_index(inplace=True)\n",
    "    else:\n",
    "        print(\"Warning: 'Date' column not found to set as index. Some lookups might be less efficient or prone to error.\")\n",
    "    # --- END FIX ---\n",
    "\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame({'Date': pd.to_datetime([])})\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "print('Example columns after loading:')\n",
    "print(raw.columns[:5].tolist())\n",
    "\n",
    "\n",
    "# --- Dynamic Ticker Identification Refined ---\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c])))\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total',\n",
    "                           '30', '10', '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC',\n",
    "                           'NFP', 'JOBS', 'CPI', 'CTII10', 'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT',\n",
    "                           'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM', 'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes)))\n",
    "\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {all_tickers}')\n",
    "print(f'Actual tradable tickers for returns: {TRADABLE_TICKERS}')\n",
    "\n",
    "\n",
    "# --- Revised Helper functions (using raw.index as source) ---\n",
    "# Removed @memory.cache\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    \"\"\"\n",
    "    Finds the first column name in raw that matches the pattern 'ticker_full_name_substr'.\n",
    "    \"\"\"\n",
    "    if substr == 'PX_LAST':\n",
    "        potential_col_name_long_price = f\"{ticker_full_name}_Last_Price_PX_LAST\"\n",
    "        if potential_col_name_long_price in raw.columns:\n",
    "            return potential_col_name_long_price\n",
    "\n",
    "        potential_col_name_short_px = f\"{ticker_full_name}_PX_LAST\"\n",
    "        if potential_col_name_short_px in raw.columns:\n",
    "            return potential_col_name_short_px\n",
    "\n",
    "    for c in raw.columns:\n",
    "        if c.startswith(ticker_full_name) and substr in c:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def safe_series(col_name):\n",
    "    \"\"\"Returns a column as a Series, or an empty Series if column does not exist.\"\"\"\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "# --- Helper function for Block Bootstrap Sharpe ---\n",
    "# Adjusted num_iterations for faster testing (DIAGNOSTIC VALUE)\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=10, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"\n",
    "    Calculates the Sharpe Ratio using block bootstrapping to account for serial correlation.\n",
    "    \"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    sharpes = []\n",
    "    blocks = []\n",
    "    for i in range(0, len(returns_series), block_size):\n",
    "        block = returns_series.iloc[i : i + block_size]\n",
    "        if not block.empty:\n",
    "            blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # FIX: Corrected n_blocks_to_sample calculation\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size))\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = []\n",
    "        if not blocks: # Defensive check\n",
    "            resampled_returns = pd.Series(dtype=float)\n",
    "        else:\n",
    "            sampled_blocks_indices = np.random.choice(len(blocks), n_blocks_to_sample, replace=True)\n",
    "            for idx in sampled_blocks_indices:\n",
    "                resampled_returns_list.append(blocks[idx])\n",
    "            resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)] # Trim to original length\n",
    "\n",
    "\n",
    "        if resampled_returns.std() > 1e-9:\n",
    "            daily_sharpe = resampled_returns.mean() / resampled_returns.std()\n",
    "            if annualize:\n",
    "                sharpes.append(daily_sharpe * np.sqrt(trading_days_per_year))\n",
    "            else:\n",
    "                sharpes.append(daily_sharpe)\n",
    "        else:\n",
    "            sharpes.append(0.0)\n",
    "\n",
    "    sharpes_sorted = sorted(sharpes)\n",
    "    if not sharpes_sorted:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    median_sharpe = np.median(sharpes_sorted)\n",
    "    lower_ci = sharpes_sorted[int(0.05 * num_iterations)]\n",
    "    upper_ci = sharpes_sorted[int(0.95 * num_iterations)]\n",
    "\n",
    "    return median_sharpe, lower_ci, upper_ci\n",
    "\n",
    "\n",
    "# --- Option Payoff Simulation Helpers ---\n",
    "def estimate_atm_premium(S, ivol, T_days, r_annual=RISK_FREE_RATE, option_type='call'):\n",
    "    \"\"\"Estimates ATM option premium using a simplified Black-Scholes like approach.\"\"\"\n",
    "    if S <= 0 or ivol <= 0 or T_days <= 0:\n",
    "        return 0.001\n",
    "    T_years = T_days / 252.0\n",
    "    premium_estimate = 0.4 * S * ivol * np.sqrt(T_years)\n",
    "    return max(premium_estimate, 0.001)\n",
    "\n",
    "def simulate_option_pnl(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    \"\"\"Simulates PnL for buying a simple ATM call or put.\"\"\"\n",
    "    if pd.isna(current_price) or pd.isna(future_price) or pd.isna(ivol_at_entry):\n",
    "        return np.nan\n",
    "    strike = current_price\n",
    "    if entry_direction == 'long':\n",
    "        premium = estimate_atm_premium(current_price, ivol_at_entry, horizon_days, option_type='call')\n",
    "        payoff = max(future_price - strike, 0)\n",
    "        pnl = payoff - premium\n",
    "    elif entry_direction == 'short':\n",
    "        premium = estimate_atm_premium(current_price, ivol_at_entry, horizon_days, option_type='put')\n",
    "        payoff = max(strike - future_price, 0)\n",
    "        pnl = payoff - premium\n",
    "    else:\n",
    "        pnl = np.nan\n",
    "    return pnl\n",
    "\n",
    "\n",
    "# --- Fractional Differencing Helper ---\n",
    "def frac_diff(series, d=0.5, window='full'):\n",
    "    \"\"\"Computes fractionally differenced series.\"\"\"\n",
    "    if not isinstance(series, pd.Series):\n",
    "        series = pd.Series(series)\n",
    "    weights = [1.]\n",
    "    for k in range(1, series.shape[0]):\n",
    "        weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1])\n",
    "    output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(series.shape[0]):\n",
    "        if window == 'full':\n",
    "            start = 0\n",
    "        else:\n",
    "            start = max(0, i - window + 1)\n",
    "        subset = series.iloc[start : i + 1]\n",
    "        current_weights = weights[-(i - start + 1):]\n",
    "        if len(subset) == len(current_weights):\n",
    "            output.iloc[i] = np.dot(current_weights, subset)\n",
    "    return output.dropna()\n",
    "\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "print('\\nEngineering custom features â€¦')\n",
    "feat = pd.DataFrame(index=raw.index)\n",
    "\n",
    "# 1. VOLATILITY-BASED FEATURES\n",
    "print(\"  - Volatility-based features\")\n",
    "for ticker in TRADABLE_TICKERS:\n",
    "    ivol_10d_col = first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D')\n",
    "    ivol_30d_col = first_col_containing(ticker, '30_Day_Call_Implied_Volatility_CALL_IMP_VOL_30D')\n",
    "    ivol_60d_col = first_col_containing(ticker, '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D')\n",
    "    if ivol_60d_col and ivol_10d_col:\n",
    "        feat[f'{ticker}_IVOL_Term_Structure_Slope'] = safe_series(ivol_60d_col) - safe_series(ivol_10d_col)\n",
    "    call_40d_ivol_col = first_col_containing(ticker, '1st_Month_Call_Imp_Vol_40_Delta_LIVE_1M_CALL_IMP_VOL_40DELTA_DFLT')\n",
    "    put_50d_ivol_col = first_col_containing(ticker, '1st_Month_Put_Imp_Vol_50_Delta_LIVE_1M_PUT_IMP_VOL_50DELTA_DFLT')\n",
    "    if call_40d_ivol_col and put_50d_ivol_col:\n",
    "        feat[f'{ticker}_IVOL_Skew_Approx'] = safe_series(put_50d_ivol_col) - safe_series(call_40d_ivol_col)\n",
    "    specific_ivol_suffixes = ['IVOL_SIGMA', 'IVOL_DELTA', 'IVOL_MONEYNESS', 'CALL_IMP_VOL_30D', 'CALL_IMP_VOL_10D', 'CALL_IMP_VOL_60D', 'PUT_IMP_VOL_30D', 'PUT_IMP_VOL_10D', 'PUT_IMP_VOL_60D']\n",
    "    for ivol_suffix in specific_ivol_suffixes:\n",
    "        col_name = first_col_containing(ticker, ivol_suffix)\n",
    "        if col_name:\n",
    "            if '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D' in col_name and \\\n",
    "               first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D'):\n",
    "                feat[f'{ticker}_IVOL_Call_Slope'] = safe_series(col_name) - \\\n",
    "                                                    safe_series(first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D'))\n",
    "            diff = safe_series(col_name).diff()\n",
    "            z = (diff - diff.rolling(30).mean()) / diff.rolling(30).std()\n",
    "            feat[col_name+'_shock'] = (z > 2).astype(int)\n",
    "            vol_col_name = first_col_containing(ticker, 'VOLUME')\n",
    "            if vol_col_name:\n",
    "                feat[col_name+'_div'] = safe_series(col_name) / safe_series(vol_col_name)\n",
    "            else:\n",
    "                feat[col_name+'_div'] = np.nan\n",
    "\n",
    "# 2. DERIV FLOW & SENTIMENT\n",
    "print(\"  - Deriv Flow & Sentiment features\")\n",
    "for ticker in all_tickers:\n",
    "    pc_col = first_col_containing(ticker, 'PUT_CALL_VOLUME_RATIO_CUR_DAY')\n",
    "    if pc_col:\n",
    "        feat[pc_col+'_ema5']=safe_series(pc_col).ewm(span=5,adjust=False).mean()\n",
    "    open_int_col = first_col_containing(ticker, 'OPEN_INT_TOTAL_CALL')\n",
    "    if open_int_col:\n",
    "        feat[open_int_col+'_chg3']=safe_series(open_int_col).pct_change(3)\n",
    "    volm_col = first_col_containing(ticker, 'Volume_-_Realtime_VOLUME')\n",
    "    if volm_col:\n",
    "        feat[volm_col+'_z']=(safe_series(volm_col)-safe_series(volm_col).rolling(30).mean())/safe_series(volm_col).rolling(30).std()\n",
    "for ticker in all_tickers:\n",
    "    open_int_t_col = first_col_containing(ticker, 'OPEN_INT_TOTAL_CALL')\n",
    "    ivol_10d_col = first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D')\n",
    "    if open_int_t_col and ivol_10d_col:\n",
    "        feat[f'{ticker}_smart_money_flag'] = ((safe_series(open_int_t_col).pct_change() > 0) & \\\n",
    "                                              (safe_series(ivol_10d_col).pct_change() > 0)).astype(int)\n",
    "\n",
    "# 3. CROSS-ASSET CORRELATIONS\n",
    "print(\"  - Cross-Asset Correlations\")\n",
    "correlation_universe = sorted(list(set(TRADABLE_TICKERS + [t for t in all_tickers if t in ['DXY Curncy', 'USGG10YR Index', 'SPX Index', 'CO1 Comdty', 'USGG2YR Index']])))\n",
    "dynamic_pairs = []\n",
    "for i in range(len(TRADABLE_TICKERS)):\n",
    "    for j in range(i + 1, len(TRADABLE_TICKERS)):\n",
    "        dynamic_pairs.append((TRADABLE_TICKERS[i], TRADABLE_TICKERS[j]))\n",
    "dynamic_pairs.extend([\n",
    "    ('SPY US Equity', 'VIX Index'),\n",
    "    ('SPY US Equity', 'USGG10YR Index'),\n",
    "    ('SPY US Equity', 'DXY Curncy'),\n",
    "    ('SPY US Equity', 'CO1 Comdty'),\n",
    "    ('SPY US Equity', 'USGG2YR Index'),\n",
    "    ('VIX Index', 'USGG10YR Index'),\n",
    "    ('VIX Index', 'DXY Curncy'),\n",
    "    ('VIX Index', 'CO1 Comdty'),\n",
    "])\n",
    "dynamic_pairs = list(set(dynamic_pairs))\n",
    "for t1,t2 in dynamic_pairs:\n",
    "    p1=first_col_containing(t1,'PX_LAST'); p2=first_col_containing(t2,'PX_LAST')\n",
    "    if p1 and p2:\n",
    "        s1=safe_series(p1); s2=safe_series(p2)\n",
    "        if not s1.empty and not s2.empty:\n",
    "            aligned_data = pd.DataFrame({'s1': s1, 's2': s2}).dropna()\n",
    "            if len(aligned_data) > 60:\n",
    "                c20=aligned_data['s1'].rolling(20).corr(aligned_data['s2'])\n",
    "                c60=aligned_data['s1'].rolling(60).corr(aligned_data['s2'])\n",
    "                feat[f'{t1}_{t2}_c20']=c20\n",
    "                feat[f'{t1}_{t2}_c60']=c60\n",
    "                feat[f'{t1}_{t2}_cZ']=(c20-c20.rolling(60).mean())/c20.rolling(60).std()\n",
    "                feat[f'{t1}_{t2}_cDelta']=c20-c60\n",
    "                ret1 = s1.pct_change().dropna()\n",
    "                ret2 = s2.pct_change().dropna()\n",
    "                aligned_returns = pd.DataFrame({'ret1': ret1, 'ret2': ret2}).dropna()\n",
    "                if not aligned_returns.empty and aligned_returns['ret2'].var() != 0:\n",
    "                    rolling_beta = aligned_returns['ret1'].rolling(window=60).cov(aligned_returns['ret2']) / \\\n",
    "                                   aligned_returns['ret2'].rolling(window=60).var()\n",
    "                    feat[f'{t1}_{t2}_rolling_beta'] = rolling_beta\n",
    "\n",
    "# 4. MACRO TRIGGERS\n",
    "print(\"  - Macro Trigger features\")\n",
    "dxy_px=first_col_containing('DXY Curncy','PX_LAST')\n",
    "ust10_px=first_col_containing('USGG10YR Index','PX_LAST')\n",
    "spy_px=first_col_containing('SPY US Equity','PX_LAST')\n",
    "vix_px=first_col_containing('VIX Index','PX_LAST')\n",
    "feat['MPI']=(safe_series(dxy_px).pct_change().rolling(3).sum()+safe_series(ust10_px).pct_change().rolling(3).sum()).shift(0)\n",
    "feat['VIX_gt20']=(safe_series(vix_px)>20).astype(int)\n",
    "feat['DXY_rising']=(safe_series(dxy_px).pct_change()>0).astype(int)\n",
    "feat['SPY_below_MA20']=(safe_series(spy_px)<safe_series(spy_px).rolling(20).mean()).astype(int)\n",
    "feat['fear_overdrive']=((feat['VIX_gt20']==1)&(feat['DXY_rising']==1)&(feat['SPY_below_MA20']==1)).astype(int)\n",
    "xlk_px=first_col_containing('XLK US Equity','PX_LAST'); xle_px=first_col_containing('XLE US Equity','PX_LAST')\n",
    "feat['sector_rotation']=safe_series(xlk_px).pct_change(5)-safe_series(xle_px).pct_change(5)\n",
    "\n",
    "# --- NEW MACRO FEATURES ---\n",
    "ust2_px = first_col_containing('USGG2YR Index', 'PX_LAST')\n",
    "if ust10_px and ust2_px:\n",
    "    feat['UST10Y_2Y_Spread'] = safe_series(ust10_px) - safe_series(ust2_px)\n",
    "    feat['UST10Y_2Y_Spread_chg'] = feat['UST10Y_2Y_Spread'].pct_change()\n",
    "cpi_yoy_px = first_col_containing('CPI YOY Index', 'PX_LAST')\n",
    "cpi_chng_px = first_col_containing('CPI CHNG Index', 'PX_LAST')\n",
    "if cpi_yoy_px:\n",
    "    feat['CPI_YOY_mom3'] = safe_series(cpi_yoy_px).pct_change(3)\n",
    "    feat['CPI_YOY_z'] = (safe_series(cpi_yoy_px) - safe_series(cpi_yoy_px).rolling(12).mean()) / safe_series(cpi_yoy_px).rolling(12).std()\n",
    "if cpi_chng_px:\n",
    "    feat['CPI_CHNG_mom3'] = safe_series(cpi_chng_px).pct_change(3)\n",
    "injcjc_px = first_col_containing('INJCJC Index', 'PX_LAST')\n",
    "nfp_tch_px = first_col_containing('NFP TCH Index', 'PX_LAST')\n",
    "jobs_us_equity_px = first_col_containing('JOBS US Equity', 'PX_LAST')\n",
    "if injcjc_px:\n",
    "    feat['INJCJC_shock'] = (safe_series(injcjc_px).diff() > safe_series(injcjc_px).diff().rolling(20).std() * 2).astype(int)\n",
    "if nfp_tch_px:\n",
    "    feat['NFP_TCH_mom3'] = safe_series(nfp_tch_px).pct_change(3)\n",
    "if jobs_us_equity_px:\n",
    "    feat['JOBS_US_Equity_mom3'] = safe_series(jobs_us_equity_px).pct_change(3)\n",
    "ffa_comdty_px = first_col_containing('FFA Comdty', 'PX_LAST')\n",
    "ctii10_govt_px = first_col_containing('CTII10 Govt', 'PX_LAST')\n",
    "ussw10_curncy_px = first_col_containing('USSW10 Curncy', 'PX_LAST')\n",
    "mlcx3crt_index_px = first_col_containing('MLCX3CRT Index', 'PX_LAST')\n",
    "farbast_index_px = first_col_containing('FARBAST Index', 'PX_LAST')\n",
    "bspgcpus_index_px = first_col_containing('BSPGCPUS Index', 'PX_LAST')\n",
    "spcsusa_index_px = first_col_containing('SPCSUSA Index', 'PX_LAST')\n",
    "spcs20sm_index_px = first_col_containing('SPCS20SM Index', 'PX_LAST')\n",
    "conssent_index_px = first_col_containing('CONSSENT Index', 'PX_LAST')\n",
    "lf94truu_index_vol30d = first_col_containing('LF94TRUU Index', 'VOLATILITY_30D')\n",
    "if ffa_comdty_px: feat['FFA_Spread'] = safe_series(ffa_comdty_px) - safe_series(ust2_px)\n",
    "if ctii10_govt_px: feat['CTII10_mom'] = safe_series(ctii10_govt_px).pct_change()\n",
    "if ussw10_curncy_px: feat['USSW10_chg'] = safe_series(ussw10_curncy_px).pct_change()\n",
    "if mlcx3crt_index_px: feat['MLCX3CRT_chg'] = safe_series(mlcx3crt_index_px).pct_change()\n",
    "if farbast_index_px: feat['FARBAST_mom'] = safe_series(farbast_index_px).pct_change()\n",
    "if bspgcpus_index_px: feat['BSPGCPUS_mom'] = safe_series(bspgcpus_index_px).pct_change()\n",
    "if spcsusa_index_px: feat['SPCSUSA_mom'] = safe_series(spcsusa_index_px).pct_change()\n",
    "if spcs20sm_index_px: feat['SPCS20SM_mom'] = safe_series(spcs20sm_index_px).pct_change()\n",
    "if conssent_index_px: feat['CONSSENT_mom'] = safe_series(conssent_index_px).pct_change()\n",
    "if lf94truu_index_vol30d: feat['LF94TRUU_Vol_Signal'] = safe_series(lf94truu_index_vol30d) / safe_series(lf94truu_index_vol30d).rolling(60).mean()\n",
    "\n",
    "\n",
    "# 5. MOMENTUM / VOL FRACTALS\n",
    "print(\"  - Momentum/Vol Fractals\")\n",
    "for tk in all_tickers:\n",
    "    px_col=first_col_containing(tk,'PX_LAST')\n",
    "    if not px_col: continue\n",
    "    px=safe_series(px_col)\n",
    "    mom5=px.pct_change(5)\n",
    "    vol20=px.pct_change().rolling(20).std()\n",
    "    feat[tk+'_mom5_vol20']=mom5/vol20\n",
    "    ma=px.rolling(20).mean(); std=px.rolling(20).std()\n",
    "    feat[tk+'_pctB']=(px-(ma-2*std))/(4*std)\n",
    "    if not px.empty and len(px) > 100:\n",
    "        try:\n",
    "            feat[f'{tk}_frac_diff_0_5'] = frac_diff(px, d=0.5, window=100)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute fractional differencing for {tk}: {e}\")\n",
    "            feat[f'{tk}_frac_diff_0_5'] = np.nan\n",
    "\n",
    "\n",
    "# 6. Shift +1 day and merge basic returns\n",
    "feat=feat.shift(1)\n",
    "panel = feat.copy()\n",
    "\n",
    "# Prepare returns for evaluation based *only* on TRADABLE_TICKERS\n",
    "price_cols_for_returns = []\n",
    "for ticker_full_name in TRADABLE_TICKERS:\n",
    "    px_col_name = first_col_containing(ticker_full_name, 'PX_LAST')\n",
    "    if px_col_name:\n",
    "        price_cols_for_returns.append(px_col_name)\n",
    "    else:\n",
    "        print(f\"Warning: PX_LAST column not found for tradable ticker '{ticker_full_name}'. It will be excluded from return calculations.\")\n",
    "\n",
    "prices=raw[price_cols_for_returns].copy()\n",
    "returns={h:prices.pct_change(h).shift(-h) for h in [1,3,5,10,21]}\n",
    "\n",
    "\n",
    "# Merge tradable returns into panel\n",
    "for tk_full_name in TRADABLE_TICKERS:\n",
    "    px_col_name = first_col_containing(tk_full_name, 'PX_LAST')\n",
    "    if px_col_name and px_col_name in raw.columns:\n",
    "        ret_series = raw[px_col_name].pct_change().shift(1).rename(f'{tk_full_name}_ret1')\n",
    "        panel = pd.concat([panel, ret_series], axis=1)\n",
    "\n",
    "print('Engineered panel shape:',panel.shape)\n",
    "\n",
    "\n",
    "# --- Primitive Signal Generation ---\n",
    "print('\\nBuilding primitive signals from engineered panel â€¦')\n",
    "signals={}\n",
    "# Helper functions for inferring signal direction and type (defined once globally)\n",
    "def get_primitive_direction(primitive_name):\n",
    "    \"\"\"Infers the directional bias (+1 for long, -1 for short, 0 for neutral) of a primitive signal.\"\"\"\n",
    "    if '>80' in primitive_name or 'z>1.5' in primitive_name or 'ma5>ma20' in primitive_name or 'rising' in primitive_name: return 1\n",
    "    elif '<20' in primitive_name or 'z<-1.name' in primitive_name or 'ma5<ma20' in primitive_name or 'below_MA' in primitive_name: return -1\n",
    "    elif 'shock' in primitive_name: return 0\n",
    "    elif 'slope' in primitive_name: return 0\n",
    "    elif 'Spread' in primitive_name: return 0\n",
    "    elif 'Vol_Signal' in primitive_name: return 0\n",
    "    return 0\n",
    "\n",
    "def get_primitive_signal_type(primitive_name):\n",
    "    \"\"\"Infers the broad category/type of a primitive signal based on its name.\"\"\"\n",
    "    if 'IVOL' in primitive_name or 'VIX' in primitive_name or 'vol' in primitive_name or '_shock' in primitive_name: return 'volatility'\n",
    "    elif 'mom' in primitive_name or 'pctB' in primitive_name or '_chg' in primitive_name or '_ret' in primitive_name or 'rising' in primitive_name: return 'momentum'\n",
    "    elif '_c20' in primitive_name or '_c60' in primitive_name or '_cZ' in primitive_name or '_cDelta' in primitive_name or '_beta' in primitive_name: return 'correlation'\n",
    "    elif 'DXY' in primitive_name or 'USGG' in primitive_name or 'MPI' in primitive_name or 'fear_overdrive' in primitive_name or 'CPI' in primitive_name or 'INJCJC' in primitive_name or 'NFP' in primitive_name or 'JOBS' in primitive_name or 'FFA' in primitive_name or 'CTII10' in primitive_name or 'USSW10' in primitive_name or 'MLCX3CRT' in primitive_name or 'FARBAST' in primitive_name or 'BSPGCPUS' in primitive_name or 'SPCSUSA' in primitive_name or 'SPCS20SM' in primitive_name or 'CONSSENT' in primitive_name or 'LF94TRUU' in primitive_name: return 'macro'\n",
    "    elif 'PUT_CALL_VOLUME_RATIO' in primitive_name or 'smart_money_flag' in primitive_name or 'Short_Interest_Ratio' in primitive_name: return 'sentiment'\n",
    "    elif 'VOLUME' in primitive_name: return 'volume'\n",
    "    elif 'OPEN_INT' in primitive_name: return 'open_interest'\n",
    "    elif 'frac_diff' in primitive_name: return 'fractional_differencing'\n",
    "    return 'other'\n",
    "\n",
    "for col in panel.columns.drop('Date',errors='ignore'):\n",
    "    s=panel[col]\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        s_clean = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if s_clean.empty or s_clean.std() == 0: continue\n",
    "        rank=s_clean.rank(pct=True)\n",
    "        signals[col+'>80']=rank>0.8\n",
    "        signals[col+'<20']=rank<0.2\n",
    "        rolling_std_60 = s_clean.rolling(60).std()\n",
    "        valid_std_mask = rolling_std_60 > 1e-9\n",
    "        z = pd.Series(np.nan, index=s_clean.index, dtype=float)\n",
    "        z[valid_std_mask] = (s_clean - s_clean.rolling(60).mean())[valid_std_mask] / rolling_std_60[valid_std_mask]\n",
    "        signals[col+'_z>1.5']=z>1.5\n",
    "        signals[col+'_z<-1.5']=z<-1.5\n",
    "        ma5=s_clean.rolling(5).mean(); ma20=s_clean.rolling(20).mean()\n",
    "        signals[col+'_ma5>ma20']=ma5>ma20\n",
    "print('Total primitive signals:',len(signals))\n",
    "\n",
    "\n",
    "# --- START OF NEW ADDITION: ITERATIVE/GREEDY SETUP GENERATION (Replaces old Primitive Pre-filtering and Setup Generation blocks) ---\n",
    "print(f\"\\nStarting iterative/greedy setup generation for lengths {SETUP_LENGTHS_TO_EXPLORE}â€¦\")\n",
    "\n",
    "# Define the full list of primitive names to draw from (all 4065 signals)\n",
    "primitive_names_full_list = list(signals.keys())\n",
    "\n",
    "# Define how many \"best\" setups to propagate from one length to the next\n",
    "N_BEST_TO_PROPAGATE = 20 # Your current test value.\n",
    "\n",
    "# Store the final summary rows and trigger records from all valid setups\n",
    "final_summary_rows = []\n",
    "final_trigger_records = []\n",
    "\n",
    "# Initialize best_setups_by_length with a placeholder for length 0\n",
    "best_setups_by_length = {0: [{\"feature_conditions\": [], \"setup_id\": \"S0000_Empty\"}]} # Align keys\n",
    "# Track already evaluated combinations to avoid redundant work\n",
    "seen_combinations = set()\n",
    "\n",
    "# Iterative loop to build setups of increasing length\n",
    "for current_k in SETUP_LENGTHS_TO_EXPLORE:\n",
    "    print(f\"  Generating and evaluating {current_k}-feature setups...\")\n",
    "    current_length_candidate_setups = [] # Candidates generated in this iteration before filtering\n",
    "\n",
    "    prev_length_best_setups = best_setups_by_length.get(current_k - 1, [])\n",
    "\n",
    "    if not prev_length_best_setups and current_k > 1:\n",
    "        print(f\"    No promising setups from length {current_k - 1} to build upon. Skipping length {current_k}.\")\n",
    "        break\n",
    "\n",
    "    for prev_setup_data in prev_length_best_setups:\n",
    "        prev_conds = prev_setup_data[\"feature_conditions\"] # Align key\n",
    "\n",
    "        # Determine the starting index for adding new primitives to maintain order and avoid permutations\n",
    "        start_idx = 0\n",
    "        if prev_conds:\n",
    "            # Ensure the last primitive from prev_conds is in the full list before finding its index\n",
    "            if prev_conds[-1] in primitive_names_full_list:\n",
    "                last_prev_cond_idx = primitive_names_full_list.index(prev_conds[-1])\n",
    "                start_idx = last_prev_cond_idx + 1\n",
    "            else:\n",
    "                # If a prev_cond somehow isn't in the full list, start from beginning (shouldn't happen with correct flow)\n",
    "                start_idx = 0\n",
    "\n",
    "        # Iterate through the full list of primitive names to find new conditions to add\n",
    "        for i in range(start_idx, len(primitive_names_full_list)):\n",
    "            new_primitive_name = primitive_names_full_list[i]\n",
    "\n",
    "            new_conds = sorted(prev_conds + [new_primitive_name])\n",
    "\n",
    "            new_conds_frozenset = frozenset(new_conds)\n",
    "            if new_conds_frozenset in seen_combinations:\n",
    "                continue\n",
    "\n",
    "            seen_combinations.add(new_conds_frozenset)\n",
    "\n",
    "            valid_combo_signals = [signals[c] for c in new_conds if c in signals]\n",
    "            if not valid_combo_signals:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                current_mask = functools.reduce(lambda a,b: a & b, valid_combo_signals)\n",
    "                current_support = current_mask.sum()\n",
    "            except KeyError:\n",
    "                current_support = 0\n",
    "\n",
    "            if current_support >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "                temp_setup = {\n",
    "                    'id': 'TEMP_S' + str(len(current_length_candidate_setups) + 1).zfill(4), # Temporary ID\n",
    "                    'conds': new_conds,\n",
    "                    'setup_length': current_k,\n",
    "                    'setup_type': 'concurrent',\n",
    "                    'support': current_support\n",
    "                }\n",
    "                current_length_candidate_setups.append(temp_setup)\n",
    "\n",
    "    print(f\"    Evaluating {len(current_length_candidate_setups)} candidate setups for length {current_k}...\")\n",
    "\n",
    "# --- Parallelize the evaluation of current_length_candidate_setups ---\n",
    "    # Adding verbose=10 to see joblib's progress\n",
    "    evaluated_results_for_current_k = Parallel(n_jobs=-1, verbose=10)(delayed(evaluate_one_setup)(s) for s in current_length_candidate_setups)\n",
    "\n",
    "    current_summary_rows = []\n",
    "    current_trigger_records_batch = []\n",
    "\n",
    "    for perf, records in evaluated_results_for_current_k:\n",
    "        if perf is not None:\n",
    "            current_summary_rows.append(perf)\n",
    "            if records is not None:\n",
    "                current_trigger_records_batch.extend(records)\n",
    "\n",
    "    final_trigger_records.extend(current_trigger_records_batch)\n",
    "\n",
    "    # Select the best setups for propagation to the next length\n",
    "    if current_summary_rows:\n",
    "        current_length_summary_df = pd.DataFrame(current_summary_rows)\n",
    "        # Sort by Sharpe and select top N to carry forward\n",
    "        best_for_next_length_df = current_length_summary_df.sort_values('sharpe_10d', ascending=False).head(N_BEST_TO_PROPAGATE)\n",
    "\n",
    "        best_setups_by_length[current_k] = best_for_next_length_df[['setup_id', 'feature_conditions']].to_dict(orient='records')\n",
    "\n",
    "        final_summary_rows.extend(current_summary_rows)\n",
    "\n",
    "        print(f\"    Selected {len(best_setups_by_length[current_k])} best setups to propagate to length {current_k + 1}.\")\n",
    "    else:\n",
    "        best_setups_by_length[current_k] = []\n",
    "        print(f\"    No valid setups found for length {current_k}. Stopping iteration.\")\n",
    "        break\n",
    "\n",
    "# --- End Iterative Generation Loop ---\n",
    "\n",
    "# Assign final, consecutive IDs to all collected setups\n",
    "setup_id_counter = 1\n",
    "for s in final_summary_rows:\n",
    "    s['id'] = 'S' + str(setup_id_counter).zfill(4)\n",
    "    setup_id_counter += 1\n",
    "\n",
    "# `setups` variable is now defined from the consolidated list\n",
    "setups = final_summary_rows\n",
    "\n",
    "print(f'\\nDiscovery complete via iterative search. Total {len(setups)} candidate setups evaluated across all lengths.')\n",
    "\n",
    "\n",
    "# --- Parallel Setup Evaluation Function (defined ONCE globally) ---\n",
    "# FIX: Modify function signature to accept explicitly passed global data\n",
    "def evaluate_one_setup(setup, raw_data, returns_data, signals_data, price_cols_for_returns_data):\n",
    "    \"\"\"\n",
    "    Helper function to evaluate a single setup for parallel processing.\n",
    "    Now explicitly accepts global data as arguments to reduce pickling overhead.\n",
    "    Returns (perf, trigger_records_for_this_setup)\n",
    "    \"\"\"\n",
    "    summary_row_for_setup = {}\n",
    "    trigger_records_for_this_setup = [] # Corrected typo: trigger_records_for_this_this_setup -> trigger_records_for_this_setup\n",
    "\n",
    "    sid=setup['id']; conds=setup['conds']\n",
    "\n",
    "    valid_signals = [signals_data[c] for c in conds if c in signals_data] # Use signals_data\n",
    "    if not valid_signals:\n",
    "        return None, None\n",
    "\n",
    "    mask=functools.reduce(lambda a,b: a & b, valid_signals)\n",
    "    dates=mask[mask].index\n",
    "    support=len(dates)\n",
    "\n",
    "    if support < MIN_INITIAL_SUPPORT_FILTER:\n",
    "        return None, None\n",
    "\n",
    "    direction_score = 0\n",
    "    signal_type_counts = {}\n",
    "    for cond in conds:\n",
    "        direction_score += get_primitive_direction(cond) # get_primitive_direction is global, assumes globals\n",
    "        s_type = get_primitive_signal_type(cond)       # get_primitive_signal_type is global, assumes globals\n",
    "        signal_type_counts[s_type] = signal_type_counts.get(s_type, 0) + 1\n",
    "\n",
    "    entry_direction = 'mixed'\n",
    "    if direction_score > 0: entry_direction = 'long'\n",
    "    elif direction_score < 0: entry_direction = 'short'\n",
    "    dominant_signal_type = 'unknown'\n",
    "    if signal_type_counts: dominant_signal_type = max(signal_type_counts, key=signal_type_counts.get)\n",
    "\n",
    "    first_trigger_date = dates.min() if not dates.empty else pd.NaT\n",
    "    last_trigger_date = dates.max() if not dates.empty else pd.NaT\n",
    "\n",
    "    perf={'setup_id':sid,'feature_conditions':'+'.join(conds),'support':support,\n",
    "          'entry_direction': entry_direction,\n",
    "          'dominant_signal_type': dominant_signal_type,\n",
    "          'first_trigger_date': first_trigger_date,\n",
    "          'last_trigger_date': last_trigger_date\n",
    "         }\n",
    "\n",
    "    for h,label in zip([3,5,10,21],['accuracy_3d','avg_return_5d','sharpe_10d','hit_rate_21d']):\n",
    "        r = returns_data[h].reindex(dates) # Use returns_data\n",
    "        if r.empty:\n",
    "            perf[label] = 0.0\n",
    "            continue\n",
    "        mean=r.mean(axis=1).dropna()\n",
    "        if mean.empty:\n",
    "            perf[label] = 0.0\n",
    "            continue\n",
    "        dir_correct=(mean>0).mean()\n",
    "        if 'accuracy' in label: perf[label]=dir_correct\n",
    "        elif 'avg' in label: perf[label]=mean.mean()\n",
    "        elif 'sharpe' in label:\n",
    "            bootstrap_block_size = min(h if h > 1 else 2, max(1, len(mean) // 2))\n",
    "            if mean.std() > 0.0001 and len(mean) >= 2:\n",
    "                median_sharpe, _, _ = block_bootstrap_sharpe(mean, block_size=bootstrap_block_size, num_iterations=10)\n",
    "                perf[label] = median_sharpe\n",
    "            else: perf[label] = 0.0\n",
    "        elif 'hit' in label: perf[label]=(r>0).stack().mean()\n",
    "    summary_row_for_setup = perf\n",
    "\n",
    "    # OPTIMIZATION: Pre-fetch all necessary raw data for all tickers and trigger dates\n",
    "    # Instead of repeated individual lookups in the inner loop\n",
    "    unique_tickers_for_data_slice = []\n",
    "    for tk_full_name in price_cols_for_returns_data:\n",
    "        unique_tickers_for_data_slice.append(tk_full_name)\n",
    "        # Also need IVOL columns for option simulation\n",
    "        tk_symbol_for_log_temp = tk_full_name.split('_PX_LAST')[0].split('_Last_Price')[0]\n",
    "        ivol_col_for_sim_temp = first_col_containing(tk_symbol_for_log_temp, f'{OPTION_SIM_HORIZON_DAYS}_Day_Call_Implied_Volatility_CALL_IMP_VOL_{OPTION_SIM_HORIZON_DAYS}D')\n",
    "        if not ivol_col_for_sim_temp: ivol_col_for_sim_temp = first_col_containing(tk_symbol_for_log_temp, '30_Day_Call_Implied_Volatility_CALL_IMP_VOL_30D')\n",
    "        if not ivol_col_for_sim_temp: ivol_col_for_sim_temp = first_col_containing(tk_symbol_for_log_temp, '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D')\n",
    "        if ivol_col_for_sim_temp and ivol_col_for_sim_temp not in unique_tickers_for_data_slice:\n",
    "            unique_tickers_for_data_slice.append(ivol_col_for_sim_temp)\n",
    "\n",
    "    # Get a slice of raw_data containing all needed columns for all dates a strategy triggers\n",
    "    # This is a key optimization: fetch once for all dates in `dates`\n",
    "    raw_data_subset_for_setup = raw_data.reindex(dates)[unique_tickers_for_data_slice]\n",
    "\n",
    "    # Pre-calculate future prices for all trigger dates and relevant tickers in one go\n",
    "    future_target_dates_series = dates + pd.Timedelta(days=OPTION_SIM_HORIZON_DAYS)\n",
    "    # Reindex the raw_data_subset to future dates\n",
    "    future_raw_data_subset = raw_data.reindex(future_target_dates_series, method='nearest', tolerance=pd.Timedelta(days=5))[unique_tickers_for_data_slice]\n",
    "\n",
    "\n",
    "    for d_idx, d in enumerate(dates):\n",
    "        current_data_on_d = raw_data_subset_for_setup.loc[d] # Get current data for this trigger date\n",
    "        future_data_on_target_date = future_raw_data_subset.loc[future_target_dates_series[d_idx]] # Get future data for this trigger date's target\n",
    "\n",
    "        for tk_col_full_name in price_cols_for_returns_data: # Use price_cols_for_returns_data\n",
    "            tk_symbol_for_log = tk_col_full_name.split('_PX_LAST')[0].split('_Last_Price')[0]\n",
    "\n",
    "            current_px = current_data_on_d.get(tk_col_full_name, np.nan)\n",
    "\n",
    "            ivol_col_for_sim = first_col_containing(tk_symbol_for_log, f'{OPTION_SIM_HORIZON_DAYS}_Day_Call_Implied_Volatility_CALL_IMP_VOL_{OPTION_SIM_HORIZON_DAYS}D')\n",
    "            if not ivol_col_for_sim: ivol_col_for_sim = first_col_containing(tk_symbol_for_log, '30_Day_Call_Implied_Volatility_CALL_IMP_VOL_30D')\n",
    "            if not ivol_col_for_sim: ivol_col_for_sim = first_col_containing(tk_symbol_for_log, '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D')\n",
    "            ivol_at_entry = current_data_on_d.get(ivol_col_for_sim, np.nan) if ivol_col_for_sim else np.nan\n",
    "\n",
    "            option_pnl_10d = np.nan\n",
    "            if not pd.isna(current_px) and not pd.isna(ivol_at_entry) and current_px > 0 and ivol_at_entry > 0:\n",
    "                future_px_for_sim = future_data_on_target_date.get(tk_col_full_name, np.nan)\n",
    "\n",
    "                if not pd.isna(future_px_for_sim):\n",
    "                    option_pnl_10d = simulate_option_pnl(current_px, future_px_for_sim, ivol_at_entry, OPTION_SIM_HORIZON_DAYS, entry_direction)\n",
    "\n",
    "            # Access returns from returns_data (the explicitly passed global returns dict)\n",
    "            # This is already optimized via r = returns_data[h].reindex(dates) earlier\n",
    "            ret_vals={h:returns_data[h][tk_col_full_name].get(d, np.nan) if tk_col_full_name in returns_data[h].columns else np.nan for h in [1,3,5,10,21]}\n",
    "\n",
    "            trigger_records_for_this_setup.append({'date':d,'ticker':tk_symbol_for_log,'setup_id':sid,'matched':1,\n",
    "                                    'return_1d':ret_vals[1],'return_3d':ret_vals[3],'return_5d':ret_vals[5],\n",
    "                                    'return_10d':ret_vals[10],'return_21d':ret_vals[21],\n",
    "                                    'option_pnl_10d': option_pnl_10d})\n",
    "    return summary_row_for_setup, trigger_records_for_this_setup\n",
    "\n",
    "\n",
    "# --- Final Output Generation (using the collected `final_summary_rows` and `final_trigger_records`) ---\n",
    "summary_df = pd.DataFrame(setups) # Use the collected and re-ID'd setups\n",
    "\n",
    "# --- Basic Setup Lifecycle Tracking Derived Metrics ---\n",
    "if not summary_df.empty:\n",
    "    summary_df['first_trigger_date'] = pd.to_datetime(summary_df['first_trigger_date'])\n",
    "    summary_df['last_trigger_date'] = pd.to_datetime(summary_df['last_trigger_date'])\n",
    "    summary_df['setup_duration_days'] = (summary_df['last_trigger_date'] - summary_df['first_trigger_date']).dt.days\n",
    "    summary_df['avg_trigger_frequency_per_day'] = summary_df['support'] / summary_df['setup_duration_days'].replace(0, np.nan)\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "else:\n",
    "    print(\"Summary DataFrame is empty, skipping lifecycle metrics calculation.\")\n",
    "\n",
    "top = summary_df[summary_df['sharpe_10d'] != 0.0].sort_values('sharpe_10d',ascending=False).head(20)\n",
    "\n",
    "summary_df.to_csv('setup_results_summary.csv',index=False)\n",
    "trigger_df = pd.DataFrame(final_trigger_records)\n",
    "trigger_df.to_csv('setup_trigger_log.csv',index=False)\n",
    "top.to_json('top_setups.json',orient='records',indent=2)\n",
    "\n",
    "print('\\nDiscovery complete with engineered features')\n",
    "print(top.head())"
   ],
   "id": "1a68e61c2aa1d841",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
