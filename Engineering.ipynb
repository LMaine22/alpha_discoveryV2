{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:21:56.780586Z",
     "start_time": "2025-08-10T08:26:31.257409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Engineering + Discovery Engine with FULLY Migrated Structured Features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "# --- PRIORITY 1: IMPLEMENT FULL REPRODUCIBILITY ---\n",
    "# Global seed for all sources of randomness to ensure script reproducibility.\n",
    "RANDOM_SEED = 44\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# Note: joblib.Parallel's reproducibility is handled by ensuring the functions it calls\n",
    "# are deterministic or are themselves seeded, which our new evaluation logic ensures.\n",
    "\n",
    "# Define the explicit list of tradable tickers\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "]\n",
    "\n",
    "# Define list of macro tickers to ensure their inclusion in feature generation\n",
    "MACRO_TICKERS = [\n",
    "    'DXY Curncy', 'USGG10YR Index', 'USGG2YR Index', 'CPI YOY Index',\n",
    "    'INJCJC Index', 'FFA Comdty', 'LF94TRUU Index', 'CPI CHNG Index',\n",
    "    'NFP TCH Index', 'JOBS US Equity', 'CTII10 Govt', 'USSW10 Curncy',\n",
    "    'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index',\n",
    "    'SPCS20SM Index', 'CONSSENT Index', 'CO1 Comdty'\n",
    "]\n",
    "\n",
    "# --- UPDATED FILE PATHS ---\n",
    "MAIN_DATA_FILE = 'All_tickers_new.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_new.xlsx'\n",
    "\n",
    "# --- GENETIC ALGORITHM CONFIGURATION ---\n",
    "NUM_GENERATIONS = 50     # How many evolutionary cycles to run\n",
    "POPULATION_SIZE = 250     # How many setups (individuals) in each generation\n",
    "SETUP_LENGTHS_TO_EXPLORE = [2, 3] # Allow setups of 2 or 3 conditions\n",
    "ELITISM_RATE = 0.1       # Percentage of the best setups to keep untouched for the next generation\n",
    "MUTATION_RATE = 0.20     # ---GA v2.1: More aggressive mutation rate---\n",
    "# General Configuration\n",
    "MIN_INITIAL_SUPPORT_FILTER = 5\n",
    "OPTION_SIM_HORIZONS_DAYS = [1, 3, 10, 21]\n",
    "RISK_FREE_RATE = 0.01\n",
    "RECENCY_WINDOW = 10 # How many recent trades to check for performance decay\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooksâ€¦')\n",
    "\n",
    "\n",
    "# --- MODIFIED Custom Data Loading Function ---\n",
    "def load_and_merge_excel(file_path, header_row, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "        for sh_name in xls.sheet_names:\n",
    "            # Use the specified header_row to correctly read the file\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name, header=header_row)\n",
    "\n",
    "            if 'Dates' in df.columns and 'Date' not in df.columns:\n",
    "                df.rename(columns={'Dates': 'Date'}, inplace=True)\n",
    "            if 'Date' not in df.columns:\n",
    "                print(f\"Warning: Sheet '{sh_name}' in '{file_path}' is missing a 'Date'/'Dates' column. Skipping sheet.\")\n",
    "                continue\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                df = df.loc[:,~df.columns.duplicated()]\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found.\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df\n",
    "\n",
    "\n",
    "# --- MODIFIED Load main and macro data ---\n",
    "# Load main tickers file, specifying headers are on Row 2 (index 1)\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE, header_row=1)\n",
    "if raw is not None and not raw.empty:\n",
    "    # Load macro file, specifying headers are on Row 5 (index 4)\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, header_row=4, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "# Final cleaning and indexing\n",
    "if not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "    if 'Date' in raw.columns:\n",
    "        raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "        raw = raw.drop_duplicates(subset=['Date'], keep='last')\n",
    "        raw.set_index('Date', inplace=True)\n",
    "        raw.index = pd.to_datetime(raw.index)\n",
    "        raw.sort_index(inplace=True)\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "\n",
    "# --- Dynamic Ticker Identification ---\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c])))\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total', '30', '10',\n",
    "                           '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC', 'NFP', 'JOBS', 'CPI', 'CTII10',\n",
    "                           'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT', 'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM',\n",
    "                           'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes + MACRO_TICKERS)))\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {len(all_tickers)}')\n",
    "\n",
    "\n",
    "# --- Helper functions ---\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    if substr == 'PX_LAST':\n",
    "        for potential_col in [f\"{ticker_full_name}_Last_Price_PX_LAST\", f\"{ticker_full_name}_PX_LAST\"]:\n",
    "            if potential_col in raw.columns: return potential_col\n",
    "    for c in raw.columns:\n",
    "        if c.startswith(ticker_full_name) and substr in c: return c\n",
    "    return None\n",
    "\n",
    "def safe_series(col_name):\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "def frac_diff(series, d=0.5, window=100):\n",
    "    weights = [1.];\n",
    "    for k in range(1, len(series)): weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1]); output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(window, len(series)):\n",
    "        subset = series.iloc[i - window + 1: i + 1]\n",
    "        if len(subset) == len(weights[-window:]): output.iloc[i] = np.dot(weights[-window:], subset)\n",
    "    return output.dropna()\n",
    "\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=1000, annualize=True, trading_days_per_year=252):\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2: return 0.0, 0.0, 0.0\n",
    "    blocks = [returns_series.iloc[i: i + block_size] for i in range(0, len(returns_series), block_size) if not returns_series.iloc[i: i + block_size].empty]\n",
    "    if not blocks: return 0.0, 0.0, 0.0\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size)); sharpes = []\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = [blocks[i] for i in np.random.choice(len(blocks), n_blocks_to_sample, replace=True)]\n",
    "        resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)]\n",
    "        if resampled_returns.std() > 1e-9:\n",
    "            sharpes.append((resampled_returns.mean() / resampled_returns.std()) * (np.sqrt(trading_days_per_year) if annualize else 1))\n",
    "        else: sharpes.append(0.0)\n",
    "    if not sharpes: return 0.0, 0.0, 0.0\n",
    "    return np.median(sharpes), np.percentile(sharpes, 5), np.percentile(sharpes, 95)\n",
    "\n",
    "def calculate_sortino_ratio(returns_series, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"Calculates the Sortino Ratio.\"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < 2: return 0.0\n",
    "\n",
    "    target_return = 0\n",
    "    downside_returns = returns_series[returns_series < target_return]\n",
    "\n",
    "    if len(downside_returns) == 0: return np.inf\n",
    "\n",
    "    expected_return = returns_series.mean()\n",
    "    downside_std = downside_returns.std()\n",
    "\n",
    "    if downside_std == 0: return np.inf\n",
    "\n",
    "    sortino = (expected_return - target_return) / downside_std\n",
    "    return sortino * np.sqrt(trading_days_per_year) if annualize else sortino\n",
    "\n",
    "def calculate_calmar_ratio(returns_series, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"Calculates the Calmar Ratio.\"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < 2: return 0.0\n",
    "\n",
    "    cumulative_returns = (1 + returns_series).cumprod()\n",
    "    peak = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns - peak) / peak\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    if max_drawdown == 0: return np.inf\n",
    "\n",
    "    total_return = cumulative_returns.iloc[-1] - 1\n",
    "\n",
    "    # --- FIX: Guard against negative base for power calculation ---\n",
    "    # If total return is <= -100%, the annualized geometric return is undefined.\n",
    "    if (1 + total_return) < 0:\n",
    "        return -99 # Return a large negative number to signify catastrophic performance\n",
    "\n",
    "    num_years = len(returns_series) / trading_days_per_year\n",
    "    annualized_return = (1 + total_return)**(1/num_years) - 1\n",
    "\n",
    "    calmar = annualized_return / abs(max_drawdown)\n",
    "    return calmar\n",
    "\n",
    "# --- Option Simulation Helpers ---\n",
    "def estimate_atm_premium(price, ivol, days, option_type):\n",
    "    T = days / 365.25\n",
    "    if T <= 0 or price <= 0 or ivol <= 0: return 0\n",
    "    return 0.4 * price * ivol * np.sqrt(T)\n",
    "\n",
    "def simulate_option_pnl_detailed(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    underlying_return = (future_price - current_price) / current_price if current_price and pd.notna(current_price) and pd.notna(future_price) else np.nan\n",
    "    nan_result = {'pnl_per_share': np.nan, 'option_type': None, 'strike_price': np.nan, 'entry_premium': np.nan,\n",
    "                  'exit_value': np.nan, 'pnl_dollars': np.nan, 'pnl_pct': np.nan, 'skipped_reason': 'None',\n",
    "                  'Underlying_Exit_Price': future_price if pd.notna(future_price) else np.nan, 'Return_Underlying': underlying_return,}\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Price'; return nan_result\n",
    "    if pd.isna(ivol_at_entry) or ivol_at_entry <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid IVOL'; return nan_result\n",
    "    if pd.isna(future_price):\n",
    "        nan_result['skipped_reason'] = 'Missing Future Price'; return nan_result\n",
    "    if entry_direction not in ['long', 'short']:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Direction'; return nan_result\n",
    "\n",
    "    scaled_ivol = ivol_at_entry / 100.0 if ivol_at_entry > 1.0 else ivol_at_entry\n",
    "    strike_price = current_price\n",
    "    option_type = 'call' if entry_direction == 'long' else 'put'\n",
    "    entry_premium = estimate_atm_premium(current_price, scaled_ivol, horizon_days, option_type)\n",
    "\n",
    "    if option_type == 'call': exit_value = max(future_price - strike_price, 0)\n",
    "    else: exit_value = max(strike_price - future_price, 0)\n",
    "\n",
    "    pnl_per_share = exit_value - entry_premium\n",
    "    pnl_dollars = pnl_per_share * 100\n",
    "\n",
    "    # Calculate P&L as a percentage of premium paid\n",
    "    pnl_pct = (pnl_per_share / entry_premium) * 100 if entry_premium > 0 else np.nan\n",
    "\n",
    "    return {'pnl_per_share': pnl_per_share, 'option_type': option_type, 'strike_price': strike_price,\n",
    "            'entry_premium': entry_premium, 'exit_value': exit_value, 'pnl_dollars': pnl_dollars,\n",
    "            'pnl_pct': pnl_pct, 'skipped_reason': 'None', 'Underlying_Exit_Price': future_price,\n",
    "            'Return_Underlying': underlying_return,}\n",
    "# --- 1. Define Feature Specifications ---\n",
    "print('\\n--- Defining ALL Feature Specifications ---')\n",
    "feature_specs = []\n",
    "# Volatility Features\n",
    "for ticker in all_tickers:\n",
    "    f60 = '60_Day_Call_Implied_Volatility'; f10 = '10_Day_Call_Implied_Volatility'\n",
    "    feature_specs.append({'type': 'ivol_term_structure', 'assets': [ticker], 'params': {'f_long':f60, 'f_short':f10},\n",
    "                          'unique_id': f'term_structure_{f60}-{f10}__{ticker}', 'display_name': f\"diff({f60}, {f10})__{ticker}\"})\n",
    "    put50 = '1st_Month_Put_Imp_Vol_50_Delta'; call40 = '1st_Month_Call_Imp_Vol_40_Delta'\n",
    "    feature_specs.append({'type': 'ivol_skew', 'assets': [ticker], 'params': {'put':put50, 'call':call40},\n",
    "                          'unique_id': f'skew_{put50}-{call40}__{ticker}', 'display_name': f\"diff({put50}, {call40})__{ticker}\"})\n",
    "    for suffix in ['IVOL_SIGMA', 'CALL_IMP_VOL_30D', 'PUT_IMP_VOL_30D']:\n",
    "        feature_specs.append({'type': 'ivol_shock', 'assets': [ticker], 'params': {'ivol_suffix': suffix, 'window': 30},\n",
    "                              'unique_id': f'zscore_{suffix}_30d__{ticker}', 'display_name': f\"zscore_{suffix}_30d__{ticker}\"})\n",
    "        feature_specs.append({'type': 'ivol_div_volume', 'assets': [ticker], 'params': {'ivol_suffix': suffix, 'vol_suffix':'VOLUME'},\n",
    "                              'unique_id': f'div_{suffix}_by_VOLUME__{ticker}', 'display_name': f\"div({suffix}, VOLUME)__{ticker}\"})\n",
    "# Deriv Flow & Sentiment Features\n",
    "for ticker in all_tickers:\n",
    "    pc_ratio_col = 'PUT_CALL_VOLUME_RATIO_CUR_DAY'\n",
    "    feature_specs.append({'type': 'put_call_ratio_ema', 'assets': [ticker], 'params': {'span': 5, 'col': pc_ratio_col},\n",
    "                          'unique_id': f'ema5_{pc_ratio_col}__{ticker}', 'display_name': f\"ema5_{pc_ratio_col}__{ticker}\"})\n",
    "    oi_col = 'OPEN_INT_TOTAL_CALL'\n",
    "    feature_specs.append({'type': 'open_interest_change', 'assets': [ticker], 'params': {'days': 3, 'col': oi_col},\n",
    "                          'unique_id': f'pct_change_{oi_col}_3d__{ticker}', 'display_name': f\"pct_change_{oi_col}_3d__{ticker}\"})\n",
    "    vol_col = 'Volume_-Realtime_VOLUME'\n",
    "    feature_specs.append({'type': 'volume_zscore', 'assets': [ticker], 'params': {'window': 30, 'col': vol_col},\n",
    "                          'unique_id': f'zscore_{vol_col}_30d__{ticker}', 'display_name': f\"zscore_{vol_col}_30d__{ticker}\"})\n",
    "    sm_oi = 'OPEN_INT_TOTAL_CALL'; sm_ivol='10_Day_Call_Implied_Volatility'\n",
    "    feature_specs.append({'type': 'smart_money_flag', 'assets': [ticker], 'params': {'oi_col': sm_oi, 'ivol_col': sm_ivol},\n",
    "                          'unique_id': f'smart_money_{sm_oi}_{sm_ivol}__{ticker}', 'display_name': f\"smart_money(pct_change({sm_oi}) > 0 AND pct_change({sm_ivol}) > 0)__{ticker}\"})\n",
    "# Generic Z-Score feature needed for sequential patterns\n",
    "for ticker in all_tickers:\n",
    "    for col in ['PX_LAST', 'IVOL_SIGMA', 'Volume_-Realtime_VOLUME']:\n",
    "        for window in [30, 60]:\n",
    "            feature_specs.append({'type': 'generic_zscore', 'assets': [ticker], 'params': {'col': col, 'window': window},\n",
    "                                  'unique_id': f'zscore_{col}_{window}d__{ticker}', 'display_name': f\"zscore({col}, {window}d)__{ticker}\"})\n",
    "# Cross-Asset Correlation Features\n",
    "price_col = 'PX_LAST'\n",
    "correlation_pairs = list(set(itertools.combinations(all_tickers, 2)))\n",
    "for t1, t2 in correlation_pairs:\n",
    "    for window in [20, 60]:\n",
    "        feature_specs.append({'type': 'correlation', 'assets': [t1, t2], 'params': {'window': window, 'col': price_col},\n",
    "                              'unique_id': f'corr_{t1}:{price_col}_{t2}:{price_col}_{window}d', 'display_name': f\"corr({t1}:{price_col}, {t2}:{price_col}, {window}d)\"})\n",
    "    feature_specs.append({'type': 'correlation_zscore', 'assets': [t1, t2], 'params':{'col':price_col, 'window':60},\n",
    "                          'unique_id': f'zscore_corr20d_{t1}:{price_col}_{t2}:{price_col}_60d', 'display_name': f\"zscore_corr(20d)({t1}:{price_col}, {t2}:{price_col}, 60d)\"})\n",
    "    feature_specs.append({'type': 'correlation_delta', 'assets': [t1, t2], 'params':{'col':price_col},\n",
    "                          'unique_id': f'corr_delta_{t1}:{price_col}_{t2}:{price_col}', 'display_name': f\"corr_delta(20d-60d)({t1}:{price_col}, {t2}:{price_col})\"})\n",
    "    feature_specs.append({'type': 'rolling_beta', 'assets': [t1, t2], 'params': {'window': 60, 'col':price_col},\n",
    "                          'unique_id': f'beta_{t1}:{price_col}_{t2}:{price_col}_60d', 'display_name': f\"beta({t1}:{price_col}, {t2}:{price_col}, 60d)\"})\n",
    "# Advanced Correlations\n",
    "adv_corr_defs = [\n",
    "    {'t1': 'QQQ US Equity', 'f1': 'IVOL_SIGMA', 't2': 'SPY US Equity', 'f2': 'IVOL_SIGMA', 'win': 30},\n",
    "    {'t1': 'TSLA US Equity', 'f1': 'Volume_-Realtime_VOLUME', 't2': 'VIX Index', 'f2': 'IVOL_SIGMA', 'win': 20},\n",
    "    {'t1': 'CO1 Comdty', 'f1': 'PX_LAST', 't2': 'XLE US Equity', 'f2': 'IVOL_SIGMA', 'win': 30},\n",
    "    {'t1': 'USGG10YR Index', 'f1': 'PX_LAST', 't2': 'XLF US Equity', 'f2': 'IVOL_SIGMA', 'win': 30}\n",
    "]\n",
    "for d in adv_corr_defs:\n",
    "    feature_specs.append({\n",
    "        'type': 'advanced_correlation', 'assets': [d['t1'], d['t2']], 'params': {'window': d['win'], 'col1': d['f1'], 'col2': d['f2']},\n",
    "        'unique_id': f\"corr_{d['t1']}:{d['f1']}_{d['t2']}:{d['f2']}_{d['win']}d\", 'display_name': f\"corr({d['t1']}:{d['f1']}, {d['t2']}:{d['f2']}, {d['win']}d)\"})\n",
    "# Macro Features\n",
    "feature_specs.extend([\n",
    "    {'type': 'macro_mpi', 'assets': ['DXY Curncy', 'USGG10YR Index'], 'unique_id': 'macro_mpi', 'display_name': 'Macro Pressure Index'},\n",
    "    {'type': 'macro_fear_overdrive', 'assets': ['VIX Index', 'DXY Curncy', 'SPY US Equity'], 'unique_id': 'macro_fear_overdrive', 'display_name': 'Fear Overdrive'},\n",
    "    {'type': 'macro_sector_rotation', 'assets': ['XLK US Equity', 'XLE US Equity'], 'unique_id': 'macro_xlk_xle_rotation', 'display_name': 'Sector Rotation (XLK-XLE)'},\n",
    "    {'type': 'macro_yield_spread', 'assets': ['USGG10YR Index', 'USGG2YR Index'], 'unique_id': 'macro_10y2y_spread', 'display_name': 'Yield Spread (10Y-2Y)'},\n",
    "    {'type': 'macro_cpi_zscore', 'assets': ['CPI YOY Index'], 'unique_id': 'macro_cpi_z', 'display_name': 'CPI Z-Score'},\n",
    "    {'type': 'macro_injcjc_shock', 'assets': ['INJCJC Index'], 'unique_id': 'macro_jobless_claims_shock', 'display_name': 'Jobless Claims Shock'},\n",
    "    {'type': 'macro_ffa_spread', 'assets': ['FFA Comdty', 'USGG2YR Index'], 'unique_id': 'macro_ffa_spread', 'display_name': 'Fed Funds Spread'},\n",
    "    {'type': 'macro_lf94truu_vol_signal', 'assets': ['LF94TRUU Index'], 'unique_id': 'macro_hyg_vol_signal', 'display_name': 'HYG Vol Signal'}])\n",
    "for t in ['CPI YOY Index', 'CPI CHNG Index', 'NFP TCH Index', 'JOBS US Equity']:\n",
    "    feature_specs.append({'type': 'macro_generic_mom', 'assets': [t], 'params': {'days': 3}, 'unique_id': f'macro_mom3_{t}', 'display_name': f'Macro Mom3d({t})'})\n",
    "for t in ['CTII10 Govt', 'USSW10 Curncy', 'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index', 'SPCS20SM Index', 'CONSSENT Index']:\n",
    "    feature_specs.append({'type': 'macro_generic_chg', 'assets': [t], 'unique_id': f'macro_chg_{t}', 'display_name': f'Macro Chg({t})'})\n",
    "# Momentum / Volatility Fractal Features\n",
    "for ticker in all_tickers:\n",
    "    feature_specs.append({'type': 'mom_div_vol', 'assets': [ticker], 'params': {'price_col':price_col, 'mom_win':5, 'vol_win':20},\n",
    "                          'unique_id': f'mom_div_vol_{price_col}_5d_20d__{ticker}', 'display_name': f\"mom_div_vol({price_col}, 5d, 20d)__{ticker}\"})\n",
    "    feature_specs.append({'type': 'bollinger_pctB', 'assets': [ticker], 'params': {'window': 20, 'price_col':price_col},\n",
    "                          'unique_id': f'pctB_{price_col}_20d__{ticker}', 'display_name': f\"%B({price_col}, 20d)__{ticker}\"})\n",
    "    feature_specs.append({'type': 'fractional_differencing', 'assets': [ticker], 'params': {'d': 0.5, 'window': 100, 'price_col':price_col},\n",
    "                          'unique_id': f'frac_diff_{price_col}_d0.5_100w__{ticker}', 'display_name': f\"frac_diff({price_col}, d=0.5, win=100)__{ticker}\"})\n",
    "# Market Regime and Interaction Features\n",
    "feature_specs.append({'type': 'regime_filter', 'assets': ['VIX Index'], 'params': {'threshold': 25, 'col': 'PX_LAST'},\n",
    "                      'unique_id': 'REGIME_IS_HIGH_VOL', 'display_name': 'REGIME_IS_HIGH_VOL (VIX > 25)'})\n",
    "feature_specs.append({'type': 'interaction', 'assets': [],\n",
    "                      'params': {'feature1': 'zscore_IVOL_SIGMA_30d__AAPL US Equity', 'feature2': 'REGIME_IS_HIGH_VOL'},\n",
    "                      'unique_id': 'zscore_IVOL_SIGMA_30d__AAPL US Equity_IN_HIGH_VOL',\n",
    "                      'display_name': 'zscore(IVOL_SIGMA, 30d)__AAPL US Equity IN_HIGH_VOL'})\n",
    "print(f\"Defined {len(feature_specs)} total feature specifications.\")\n",
    "\n",
    "# --- 2. Calculate Features Based on Specifications ---\n",
    "print('--- Calculating All Features ---')\n",
    "feat = pd.DataFrame(index=raw.index)\n",
    "# The order of calculation matters for interaction features, so iterate twice.\n",
    "# First pass for all primary features\n",
    "for spec in feature_specs:\n",
    "    if spec['type'] == 'interaction': continue # Skip interaction features on the first pass\n",
    "    feature_id = spec['unique_id']\n",
    "    try:\n",
    "        if spec['type'] == 'ivol_term_structure':\n",
    "            ivol60 = safe_series(first_col_containing(spec['assets'][0], spec['params']['f_long'])); ivol10 = safe_series(first_col_containing(spec['assets'][0], spec['params']['f_short']))\n",
    "            if not ivol60.empty and not ivol10.empty: feat[feature_id] = ivol60 - ivol10\n",
    "        elif spec['type'] == 'ivol_skew':\n",
    "            put50 = safe_series(first_col_containing(spec['assets'][0], spec['params']['put'])); call40 = safe_series(first_col_containing(spec['assets'][0], spec['params']['call']))\n",
    "            if not put50.empty and not call40.empty: feat[feature_id] = put50 - call40\n",
    "        elif spec['type'] == 'ivol_shock':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix']))\n",
    "            if not ivol_s.empty: feat[feature_id] = (ivol_s.diff() - ivol_s.diff().rolling(spec['params']['window']).mean()) / ivol_s.diff().rolling(spec['params']['window']).std()\n",
    "        elif spec['type'] == 'ivol_div_volume':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix'])); vol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['vol_suffix']))\n",
    "            if not ivol_s.empty and not vol_s.empty: feat[feature_id] = ivol_s / vol_s.replace(0, np.nan)\n",
    "        elif spec['type'] == 'put_call_ratio_ema':\n",
    "            pc = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not pc.empty: feat[feature_id] = pc.ewm(span=spec['params']['span'], adjust=False).mean()\n",
    "        elif spec['type'] == 'open_interest_change':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not oi.empty: feat[feature_id] = oi.pct_change(spec['params']['days'])\n",
    "        elif spec['type'] == 'volume_zscore':\n",
    "            vol = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not vol.empty: feat[feature_id] = (vol - vol.rolling(spec['params']['window']).mean()) / vol.rolling(spec['params']['window']).std()\n",
    "        elif spec['type'] == 'smart_money_flag':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], spec['params']['oi_col'])).pct_change() > 0; ivol = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_col'])).pct_change() > 0\n",
    "            if not oi.empty and not ivol.empty: feat[feature_id] = (oi & ivol).astype(int)\n",
    "        elif spec['type'] == 'generic_zscore':\n",
    "            s = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not s.empty: feat[feature_id] = (s - s.rolling(spec['params']['window']).mean()) / s.rolling(spec['params']['window']).std()\n",
    "        elif spec['type'] == 'correlation':\n",
    "            t1, t2 = spec['assets']; p1, p2 = first_col_containing(t1, spec['params']['col']), first_col_containing(t2, spec['params']['col'])\n",
    "            if p1 and p2:\n",
    "                aligned = pd.DataFrame({'s1': safe_series(p1), 's2': safe_series(p2)}).dropna()\n",
    "                if len(aligned) > spec['params']['window']: feat[feature_id] = aligned['s1'].rolling(spec['params']['window']).corr(aligned['s2'])\n",
    "        elif spec['type'] == 'advanced_correlation':\n",
    "            t1, t2 = spec['assets']; s1_col = first_col_containing(t1, spec['params']['col1']); s2_col = first_col_containing(t2, spec['params']['col2'])\n",
    "            if s1_col and s2_col:\n",
    "                aligned = pd.DataFrame({'s1': safe_series(s1_col), 's2': safe_series(s2_col)}).dropna()\n",
    "                if len(aligned) > spec['params']['window']: feat[feature_id] = aligned['s1'].rolling(spec['params']['window']).corr(aligned['s2'])\n",
    "        elif spec['type'] in ['correlation_zscore', 'correlation_delta']:\n",
    "            t1, t2 = spec['assets']; price_col_name = spec['params']['col']; c20_id = f'corr_{t1}:{price_col_name}_{t2}:{price_col_name}_20d'; c60_id = f'corr_{t1}:{price_col_name}_{t2}:{price_col_name}_60d'\n",
    "            c20 = feat.get(c20_id); c60 = feat.get(c60_id)\n",
    "            if c20 is not None and c60 is not None:\n",
    "                if spec['type'] == 'correlation_zscore': feat[feature_id] = (c20 - c20.rolling(spec['params']['window']).mean()) / c20.rolling(spec['params']['window']).std()\n",
    "                else: feat[feature_id] = c20 - c60\n",
    "        elif spec['type'] == 'rolling_beta':\n",
    "            t1, t2 = spec['assets']; p1, p2 = first_col_containing(t1, spec['params']['col']), first_col_containing(t2, spec['params']['col'])\n",
    "            if p1 and p2:\n",
    "                rets = pd.DataFrame({'r1': safe_series(p1).pct_change(), 'r2': safe_series(p2).pct_change()}).dropna()\n",
    "                if len(rets) > spec['params']['window']: feat[feature_id] = rets['r1'].rolling(spec['params']['window']).cov(rets['r2']) / rets['r2'].rolling(spec['params']['window']).var()\n",
    "        elif spec['type'] == 'macro_mpi':\n",
    "            dxy, ust10 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not dxy.empty and not ust10.empty: feat[feature_id] = dxy.pct_change().rolling(3).sum() + ust10.pct_change().rolling(3).sum()\n",
    "        elif spec['type'] == 'macro_fear_overdrive':\n",
    "            vix, dxy, spy = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][2], 'PX_LAST'))\n",
    "            if not vix.empty and not dxy.empty and not spy.empty: feat[feature_id] = ((vix > 20) & (dxy.pct_change() > 0) & (spy < spy.rolling(20).mean())).astype(int)\n",
    "        elif spec['type'] == 'macro_sector_rotation':\n",
    "            xlk, xle = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not xlk.empty and not xle.empty: feat[feature_id] = xlk.pct_change(5) - xle.pct_change(5)\n",
    "        elif spec['type'] == 'macro_yield_spread':\n",
    "            ust10, ust2 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not ust10.empty and not ust2.empty: feat[feature_id] = ust10 - ust2\n",
    "        elif spec['type'] == 'macro_generic_mom':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change(spec['params']['days'])\n",
    "        elif spec['type'] == 'macro_generic_chg':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change()\n",
    "        elif spec['type'] == 'macro_cpi_zscore':\n",
    "            cpi = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not cpi.empty: feat[feature_id] = (cpi - cpi.rolling(12).mean()) / cpi.rolling(12).std()\n",
    "        elif spec['type'] == 'macro_injcjc_shock':\n",
    "            injcjc = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not injcjc.empty: feat[feature_id] = (injcjc.diff() > injcjc.diff().rolling(20).std() * 2).astype(int)\n",
    "        elif spec['type'] == 'macro_ffa_spread':\n",
    "            ffa, ust2 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not ffa.empty and not ust2.empty: feat[feature_id] = ffa - ust2\n",
    "        elif spec['type'] == 'macro_lf94truu_vol_signal':\n",
    "            vol = safe_series(first_col_containing(spec['assets'][0], 'VOLATILITY_30D'))\n",
    "            if not vol.empty: feat[feature_id] = vol / vol.rolling(60).mean()\n",
    "        elif spec['type'] == 'regime_filter':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not px.empty: feat[feature_id] = px > spec['params']['threshold']\n",
    "        elif spec['type'] == 'mom_div_vol':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change(5) / px.pct_change().rolling(20).std()\n",
    "        elif spec['type'] == 'bollinger_pctB':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty:\n",
    "                ma = px.rolling(spec['params']['window']).mean();\n",
    "                std = px.rolling(spec['params']['window']).std()\n",
    "                feat[feature_id] = (px - (ma - 2 * std)) / (4 * std)\n",
    "        elif spec['type'] == 'fractional_differencing':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty and len(px) > spec['params']['window']: feat[feature_id] = frac_diff(px, d=spec['params']['d'], window=spec['params']['window'])\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate feature '{feature_id}': {e}\")\n",
    "# Second pass for interaction features\n",
    "for spec in feature_specs:\n",
    "    if spec['type'] == 'interaction':\n",
    "        feature_id = spec['unique_id']\n",
    "        try:\n",
    "            f1 = spec['params']['feature1']; f2 = spec['params']['feature2']\n",
    "            if f1 in feat.columns and f2 in feat.columns:\n",
    "                feat[feature_id] = feat[f1] * feat[f2]\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate interaction feature '{feature_id}': {e}\")\n",
    "feat = feat.shift(1)\n",
    "print(f\"Calculated {feat.shape[1]} feature series.\")\n",
    "\n",
    "# Sequential Features\n",
    "try:\n",
    "    vix_vol_zscore_feat_name = 'zscore_IVOL_SIGMA_30d__VIX Index'; qqq_spy_corr_zscore_feat_name = 'zscore_corr20d_QQQ US Equity:PX_LAST_SPY US Equity:PX_LAST_60d'\n",
    "    event_A_series = (feat[vix_vol_zscore_feat_name] > 1.5); event_B_series = (feat[qqq_spy_corr_zscore_feat_name] < -1.5)\n",
    "    sequential_feature_name = 'SEQ_VIX_SPIKE_THEN_CORR_DROP'; feat[sequential_feature_name] = event_B_series & event_A_series.shift(1)\n",
    "    print(f\"Successfully created sequential feature: '{sequential_feature_name}'\")\n",
    "    yield_zscore_name = 'zscore_PX_LAST_60d__USGG10YR Index'; gold_vol_zscore_name = 'zscore_IVOL_SIGMA_30d__GLD US Equity'\n",
    "    event_A_series = (feat[yield_zscore_name] < -1.5); event_B_series = (feat[gold_vol_zscore_name] > 1.5)\n",
    "    sequential_feature_name = 'SEQ_YIELD_DROP_THEN_GOLD_VOL_SPIKE'; feat[sequential_feature_name] = event_B_series & event_A_series.shift(1)\n",
    "    print(f\"Successfully created sequential feature: '{sequential_feature_name}'\")\n",
    "    nvda_vol_zscore_name = 'zscore_Volume_-Realtime_VOLUME_30d__NVDA US Equity'; qqq_price_zscore_name = 'zscore_PX_LAST_60d__QQQ US Equity'\n",
    "    event_A_series = (feat[nvda_vol_zscore_name] > 1.5); event_B_series = (feat[qqq_price_zscore_name] > 1.5)\n",
    "    sequential_feature_name = 'SEQ_NVDA_VOL_SPIKE_THEN_QQQ_PRICE_RISE'; feat[sequential_feature_name] = event_B_series & event_A_series.shift(1)\n",
    "    print(f\"Successfully created sequential feature: '{sequential_feature_name}'\")\n",
    "except KeyError as e:\n",
    "    print(f\"Warning: Could not create sequential feature. A component feature was not found: {e}\")\n",
    "\n",
    "# --- 3. Define Primitive Signals from Features ---\n",
    "print('--- Defining Primitive Signals ---')\n",
    "primitive_signals = []\n",
    "signal_series = {}\n",
    "signal_id_counter = 0\n",
    "for feature_id in feat.columns:\n",
    "    s = feat[feature_id].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty: continue\n",
    "\n",
    "    # --- FIX: Robust check for boolean-like features ---\n",
    "    # This checks if the series contains only values equivalent to True/False or 1/0.\n",
    "    is_boolean_like = set(s.unique()).issubset({0, 1, True, False})\n",
    "\n",
    "    if is_boolean_like:\n",
    "        if s.std() == 0: continue # Skip if all values are the same (e.g., all True or all False)\n",
    "        sig_id = f\"SIG_{signal_id_counter}\"; signal_id_counter += 1\n",
    "        primitive_signals.append({'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'boolean', 'operator': '==', 'value': True})\n",
    "        signal_series[sig_id] = (s == True)\n",
    "        continue # IMPORTANT: Skip numeric signal generation for boolean features\n",
    "\n",
    "    if s.std() == 0: continue # Skip non-boolean features with no variance\n",
    "\n",
    "    # --- Generate percentile signals ---\n",
    "    for op, val in [('>', 0.8), ('<', 0.2)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\"; signal_id_counter += 1\n",
    "        primitive_signals.append({'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'percentile', 'operator': op, 'value': val})\n",
    "        signal_series[sig_id] = s.rank(pct=True).apply(lambda x: x > val if op == '>' else x < val)\n",
    "\n",
    "    # --- Generate z-score signals ---\n",
    "    rolling_std = s.rolling(60).std()\n",
    "    valid_std_mask = rolling_std > 1e-9\n",
    "    if not valid_std_mask.any(): continue\n",
    "\n",
    "    z = pd.Series(np.nan, index=s.index)\n",
    "    z[valid_std_mask] = (s - s.rolling(60).mean())[valid_std_mask] / rolling_std[valid_std_mask]\n",
    "\n",
    "    for op, val in [('>', 1.5), ('<', -1.5)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\"; signal_id_counter += 1\n",
    "        primitive_signals.append({'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'z_score', 'operator': op, 'value': val})\n",
    "        signal_series[sig_id] = z.apply(lambda x: x > val if op == '>' else x < val)\n",
    "\n",
    "print(f\"Defined {len(primitive_signals)} primitive signals.\")\n",
    "\n",
    "# --- Prepare Returns for Evaluation ---\n",
    "price_cols_for_returns = [first_col_containing(t, 'PX_LAST') for t in TRADABLE_TICKERS if first_col_containing(t, 'PX_LAST')]\n",
    "prices = raw[price_cols_for_returns].copy()\n",
    "returns = {h: prices.pct_change(h).shift(-h) for h in [1, 3, 5, 10, 21]}\n",
    "\n",
    "# --- 4. GENETIC ALGORITHM: Evolve Powerful Setups ---\n",
    "\n",
    "# --- GENETIC ALGORITHM HELPERS ---\n",
    "def get_setup_dna(setup):\n",
    "    \"\"\"Creates a unique, hashable identifier for a setup based on its signals.\"\"\"\n",
    "    return tuple(sorted([s['signal_id'] for s in setup['signal_definitions']]))\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"Creates a new child setup by combining DNA from two parents.\"\"\"\n",
    "    child_signals = [random.choice(parent1['signal_definitions']), random.choice(parent2['signal_definitions'])]\n",
    "    # Combine signals from both parents for potentially larger setups\n",
    "    if len(parent1['signal_definitions']) > 1 and len(parent2['signal_definitions']) > 1:\n",
    "        child_signals.append(random.choice(parent1['signal_definitions']))\n",
    "        child_signals.append(random.choice(parent2['signal_definitions']))\n",
    "    # Ensure no duplicate signals in the child and respect max length\n",
    "    child_signals = list({s['signal_id']: s for s in child_signals}.values())\n",
    "    if len(child_signals) > max(SETUP_LENGTHS_TO_EXPLORE):\n",
    "        child_signals = random.sample(child_signals, max(SETUP_LENGTHS_TO_EXPLORE))\n",
    "    # Child is a dictionary, not a DataFrame row yet\n",
    "    return {'id': 'child', 'signal_definitions': child_signals}\n",
    "\n",
    "\n",
    "def mutate(setup, all_signal_ids, mutation_rate):\n",
    "    \"\"\"Randomly changes one signal in a setup's DNA.\"\"\"\n",
    "    if random.random() < mutation_rate:\n",
    "        idx_to_mutate = random.randint(0, len(setup['signal_definitions']) - 1)\n",
    "        new_signal_id = random.choice(all_signal_ids)\n",
    "        new_signal_def = next(p for p in primitive_signals if p['signal_id'] == new_signal_id)\n",
    "        # Avoid mutating to a signal that's already in the setup\n",
    "        if new_signal_def['signal_id'] not in [s['signal_id'] for s in setup['signal_definitions']]:\n",
    "            setup['signal_definitions'][idx_to_mutate] = new_signal_def\n",
    "    return setup\n",
    "\n",
    "# --- PRIORITY 2: NSGA-II Core Logic Helpers ---\n",
    "def non_dominated_sort(population):\n",
    "    \"\"\"\n",
    "    Performs non-dominated sorting on the population.\n",
    "    Assigns a 'rank' to each individual (setup).\n",
    "    \"\"\"\n",
    "    for ind1 in population:\n",
    "        ind1['domination_count'] = 0\n",
    "        ind1['dominated_solutions'] = []\n",
    "        for ind2 in population:\n",
    "            if ind1 is ind2:\n",
    "                continue\n",
    "            # Objectives: Higher Sortino, Higher Calmar, Higher Support are better\n",
    "            # An individual `p` dominates `q` if it is no worse in all objectives and strictly better in at least one.\n",
    "            is_dominant = (\n",
    "                (ind1['objectives'][0] >= ind2['objectives'][0] and\n",
    "                 ind1['objectives'][1] >= ind2['objectives'][1] and\n",
    "                 ind1['objectives'][2] >= ind2['objectives'][2]) and\n",
    "                (ind1['objectives'][0] > ind2['objectives'][0] or\n",
    "                 ind1['objectives'][1] > ind2['objectives'][1] or\n",
    "                 ind1['objectives'][2] > ind2['objectives'][2])\n",
    "            )\n",
    "            if is_dominant:\n",
    "                ind1['dominated_solutions'].append(ind2)\n",
    "            elif (\n",
    "                (ind2['objectives'][0] >= ind1['objectives'][0] and\n",
    "                 ind2['objectives'][1] >= ind1['objectives'][1] and\n",
    "                 ind2['objectives'][2] >= ind1['objectives'][2]) and\n",
    "                (ind2['objectives'][0] > ind1['objectives'][0] or\n",
    "                 ind2['objectives'][1] > ind1['objectives'][1] or\n",
    "                 ind2['objectives'][2] > ind1['objectives'][2])\n",
    "            ):\n",
    "                ind1['domination_count'] += 1\n",
    "\n",
    "    fronts = []\n",
    "    rank = 1\n",
    "    front1 = [ind for ind in population if ind['domination_count'] == 0]\n",
    "    for ind in front1:\n",
    "        ind['rank'] = rank\n",
    "\n",
    "    current_front = front1\n",
    "    while current_front:\n",
    "        fronts.append(current_front)\n",
    "        next_front = []\n",
    "        for ind1 in current_front:\n",
    "            for ind2 in ind1['dominated_solutions']:\n",
    "                ind2['domination_count'] -= 1\n",
    "                if ind2['domination_count'] == 0:\n",
    "                    ind2['rank'] = rank + 1\n",
    "                    next_front.append(ind2)\n",
    "        rank += 1\n",
    "        current_front = next_front\n",
    "\n",
    "    # Flatten the list of fronts back into a single population list\n",
    "    sorted_population = [ind for front in fronts for ind in front]\n",
    "    return sorted_population\n",
    "\n",
    "def calculate_crowding_distance(front):\n",
    "    \"\"\"\n",
    "    Calculates the crowding distance for each individual in a Pareto front.\n",
    "    \"\"\"\n",
    "    if not front:\n",
    "        return\n",
    "\n",
    "    num_objectives = len(front[0]['objectives'])\n",
    "    for ind in front:\n",
    "        ind['crowding_distance'] = 0\n",
    "\n",
    "    for i in range(num_objectives):\n",
    "        # Sort by the current objective\n",
    "        front.sort(key=lambda x: x['objectives'][i])\n",
    "        # Assign infinite distance to boundary solutions\n",
    "        front[0]['crowding_distance'] = float('inf')\n",
    "        front[-1]['crowding_distance'] = float('inf')\n",
    "\n",
    "        obj_min = front[0]['objectives'][i]\n",
    "        obj_max = front[-1]['objectives'][i]\n",
    "\n",
    "        if obj_max == obj_min:\n",
    "            continue\n",
    "\n",
    "        for j in range(1, len(front) - 1):\n",
    "            front[j]['crowding_distance'] += (front[j + 1]['objectives'][i] - front[j - 1]['objectives'][i]) / (obj_max - obj_min)\n",
    "\n",
    "def selection_operator(population, k=2):\n",
    "    \"\"\"\n",
    "    Selects a parent using binary tournament selection based on rank and crowding distance.\n",
    "    \"\"\"\n",
    "    tournament_contenders = random.sample(population, k)\n",
    "    contender1, contender2 = tournament_contenders[0], tournament_contenders[1]\n",
    "\n",
    "    # Lower rank is better\n",
    "    if contender1['rank'] < contender2['rank']:\n",
    "        return contender1\n",
    "    elif contender2['rank'] < contender1['rank']:\n",
    "        return contender2\n",
    "    else:\n",
    "        # If ranks are equal, higher crowding distance is better (more diversity)\n",
    "        if contender1['crowding_distance'] > contender2['crowding_distance']:\n",
    "            return contender1\n",
    "        else:\n",
    "            return contender2\n",
    "\n",
    "# --- English Description Generator ---\n",
    "def generate_english_description(setup_id, signal_defs, feature_specs_list):\n",
    "    clauses = [];\n",
    "    for s_def in signal_defs:\n",
    "        feat_name = next((f_spec['display_name'] for f_spec in feature_specs_list if f_spec['unique_id'] == s_def['feature_id']), s_def.get('feature_id', 'unknown_feature'))\n",
    "        if s_def['condition_type'] == 'boolean': clauses.append(f\"{feat_name} is true\")\n",
    "        elif s_def['condition_type'] == 'percentile':\n",
    "            level = \"is very high\" if s_def['operator'] == '>' else \"is very low\"; clauses.append(f\"{feat_name} {level}\")\n",
    "        else:\n",
    "            level = \"surges unexpectedly\" if s_def['operator'] == '>' else \"drops sharply\"; clauses.append(f\"{feat_name} {level}\")\n",
    "    description = f\"When {clauses[0]}\"\n",
    "    if len(clauses) > 1: description += f\" and {' and '.join(clauses[1:])}\"\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs); bias = 'a bullish' if direction_score > 0 else 'a bearish' if direction_score < 0 else 'an uncertain'\n",
    "    description += f\", it may indicate {bias} outlook.\"\n",
    "    return {'setup_id': setup_id, 'description': description, 'explained_description': \"DEPRECATED\"}\n",
    "\n",
    "# === START BLOCK TO REPLACE: evaluate_one_setup function ===\n",
    "def evaluate_one_setup(setup, returns_dict):\n",
    "    \"\"\"\n",
    "    Evaluates a single setup and now ALSO RETURNS THE TRIGGER DATES for phenotype hashing.\n",
    "    \"\"\"\n",
    "    sid, signal_defs = setup['id'], setup['signal_definitions']\n",
    "    try:\n",
    "        mask = functools.reduce(lambda a, b: a & b, [signal_series[s['signal_id']] for s in signal_defs])\n",
    "        dates = mask[mask].index\n",
    "    except (KeyError, TypeError):\n",
    "        return {'id': sid, 'signal_definitions': signal_defs, 'objectives': (-99, -99, 0), 'metrics_by_ticker': {}, 'trigger_dates': pd.Index([])}\n",
    "\n",
    "    support = len(dates)\n",
    "    if support < MIN_INITIAL_SUPPORT_FILTER:\n",
    "        return {'id': sid, 'signal_definitions': signal_defs, 'objectives': (-99, -99, support), 'metrics_by_ticker': {}, 'trigger_dates': pd.Index([])}\n",
    "\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs if s['condition_type'] != 'boolean')\n",
    "    if direction_score == 0 and any(s['condition_type'] != 'boolean' for s in signal_defs):\n",
    "        return {'id': sid, 'signal_definitions': signal_defs, 'objectives': (-99, -99, support), 'metrics_by_ticker': {}, 'trigger_dates': dates}\n",
    "    elif all(s['condition_type'] == 'boolean' for s in signal_defs):\n",
    "        direction_score = 1\n",
    "    entry_direction = 'long' if direction_score > 0 else 'short'\n",
    "\n",
    "    price_cols_for_returns = [first_col_containing(t, 'PX_LAST') for t in TRADABLE_TICKERS if first_col_containing(t, 'PX_LAST')]\n",
    "    all_sortinos = []\n",
    "    all_calmars = []\n",
    "    metrics_by_ticker = {}\n",
    "    perf_horizon = 10\n",
    "\n",
    "    for tk_col in price_cols_for_returns:\n",
    "        tk_symbol = next((ticker for ticker in TRADABLE_TICKERS if tk_col.startswith(ticker)), \"Unknown\")\n",
    "        r_ticker = returns_dict[perf_horizon][tk_col].reindex(dates).dropna()\n",
    "\n",
    "        if entry_direction == 'short':\n",
    "            r_ticker = -r_ticker\n",
    "\n",
    "        ### FIX: Changed from > 5 to >= 5 to be consistent with the seeder's viability check. ###\n",
    "        if len(r_ticker) >= 5 and r_ticker.std() > 1e-9:\n",
    "            sortino = calculate_sortino_ratio(r_ticker)\n",
    "            calmar = calculate_calmar_ratio(r_ticker)\n",
    "            all_sortinos.append(sortino)\n",
    "            all_calmars.append(calmar)\n",
    "            metrics_by_ticker[tk_symbol] = {'sortino': sortino, 'calmar': calmar}\n",
    "\n",
    "    median_sortino = np.median(all_sortinos) if all_sortinos else -99\n",
    "    median_calmar = np.median(all_calmars) if all_calmars else -99\n",
    "    median_sortino = np.nan_to_num(median_sortino, nan=-99.0, posinf=999.0, neginf=-999.0)\n",
    "    median_calmar = np.nan_to_num(median_calmar, nan=-99.0, posinf=999.0, neginf=-999.0)\n",
    "\n",
    "    return {\n",
    "        'id': sid,\n",
    "        'signal_definitions': signal_defs,\n",
    "        'objectives': (median_sortino, median_calmar, support),\n",
    "        'metrics_by_ticker': metrics_by_ticker,\n",
    "        'entry_direction': entry_direction,\n",
    "        'trigger_dates': dates,\n",
    "        'first_trigger_date': dates.min() if not dates.empty else pd.NaT,\n",
    "        'last_trigger_date': dates.max() if not dates.empty else pd.NaT\n",
    "    }\n",
    "# === END BLOCK TO REPLACE: evaluate_one_setup function ===\n",
    "\n",
    "# === START BLOCK TO REPLACE: The entire section from `Create Initial Population` to the end of the script ===\n",
    "# --- PRIORITY 3: Robust Initial Population Generator ---\n",
    "# --- PRIORITY 3: Robust Initial Population Generator (v3 - Pre-Validation) ---\n",
    "print('\\n--- GENETIC ALGORITHM: Creating Initial Population (Generation 0) ---')\n",
    "all_signal_ids = [s['signal_id'] for s in primitive_signals]\n",
    "current_population = []\n",
    "setup_id_counter = 0\n",
    "existing_dna = set()\n",
    "perf_horizon = 10 # Use the same horizon as the main evaluation for consistency\n",
    "\n",
    "def is_setup_viable(signal_defs, min_trades=5):\n",
    "    \"\"\"A lightweight pre-evaluation function to check if a setup is truly viable.\"\"\"\n",
    "    try:\n",
    "        mask = functools.reduce(lambda a, b: a & b, [signal_series[s['signal_id']] for s in signal_defs])\n",
    "        dates = mask[mask].index\n",
    "        if len(dates) < MIN_INITIAL_SUPPORT_FILTER:\n",
    "            return False\n",
    "\n",
    "        # Check if at least one ticker has enough valid return data points\n",
    "        for tk_col in price_cols_for_returns:\n",
    "            r_ticker = returns[perf_horizon][tk_col].reindex(dates).dropna()\n",
    "            if len(r_ticker) >= min_trades:\n",
    "                return True # Found a viable ticker, so the setup is viable\n",
    "        return False # No tickers were viable for this setup\n",
    "    except (KeyError, TypeError):\n",
    "        return False\n",
    "\n",
    "# Stage 1 & 2: Seed with single signals and simple pairs that pass the viability test\n",
    "print(\"Seeding with pre-validated single and pair setups...\")\n",
    "num_to_create = int(POPULATION_SIZE * 0.8) # Let's create more high-quality seeds\n",
    "max_attempts = len(primitive_signals) * 5\n",
    "attempts = 0\n",
    "\n",
    "# Try creating single-signal setups first\n",
    "for p_signal in primitive_signals:\n",
    "    if len(current_population) >= num_to_create: break\n",
    "    if is_setup_viable([p_signal]):\n",
    "        setup = {'id': f'S{setup_id_counter:04d}', 'signal_definitions': [p_signal]}\n",
    "        dna = get_setup_dna(setup)\n",
    "        if dna not in existing_dna:\n",
    "            current_population.append(setup)\n",
    "            existing_dna.add(dna)\n",
    "            setup_id_counter += 1\n",
    "\n",
    "# Try creating paired setups\n",
    "while len(current_population) < num_to_create and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    p_signal_1 = random.choice(primitive_signals)\n",
    "    p_signal_2 = random.choice(primitive_signals)\n",
    "    if p_signal_1['signal_id'] == p_signal_2['signal_id']: continue\n",
    "\n",
    "    sig_defs = [p_signal_1, p_signal_2]\n",
    "    if is_setup_viable(sig_defs):\n",
    "        setup = {'id': f'S{setup_id_counter:04d}', 'signal_definitions': sig_defs}\n",
    "        dna = get_setup_dna(setup)\n",
    "        if dna not in existing_dna:\n",
    "            current_population.append(setup)\n",
    "            existing_dna.add(dna)\n",
    "            setup_id_counter += 1\n",
    "\n",
    "print(f\"  - Created {len(current_population)} pre-validated setups.\")\n",
    "\n",
    "# Stage 3: Fill the remainder randomly, still using the viability test\n",
    "print(\"Filling remainder of population with pre-validated random setups...\")\n",
    "max_attempts = POPULATION_SIZE * 100\n",
    "attempts = 0\n",
    "while len(current_population) < POPULATION_SIZE and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    k = random.choice(SETUP_LENGTHS_TO_EXPLORE)\n",
    "    sig_id_list = random.sample(all_signal_ids, k)\n",
    "    sig_defs = [p for p in primitive_signals if p['signal_id'] in sig_id_list]\n",
    "\n",
    "    dna = get_setup_dna({'signal_definitions': sig_defs})\n",
    "    if dna in existing_dna: continue\n",
    "\n",
    "    if is_setup_viable(sig_defs):\n",
    "        temp_setup = {'id': f'S{setup_id_counter:04d}', 'signal_definitions': sig_defs}\n",
    "        current_population.append(temp_setup)\n",
    "        existing_dna.add(dna)\n",
    "        setup_id_counter += 1\n",
    "\n",
    "if attempts >= max_attempts:\n",
    "    print(f\"Warning: Population filling stopped after {max_attempts} attempts.\")\n",
    "\n",
    "if not current_population:\n",
    "    raise SystemExit(\"FATAL: Could not create any viable setups for the initial population. Check data or filter criteria.\")\n",
    "\n",
    "print(f\"Created initial population of {len(current_population)} guaranteed viable setups.\")\n",
    "\n",
    "# --- The Main Evolutionary Loop (NSGA-II) ---\n",
    "hall_of_fame = []\n",
    "\n",
    "for generation in range(NUM_GENERATIONS):\n",
    "    print(f\"\\n--- Evaluating Generation {generation + 1}/{NUM_GENERATIONS} ---\")\n",
    "\n",
    "    # Evaluate the current population\n",
    "    evaluated_population = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup, returns) for setup in current_population)\n",
    "\n",
    "    # For the first generation, the combined population is just the initial evaluated population\n",
    "    combined_population = evaluated_population\n",
    "\n",
    "    # After the first generation, create children and add them to the population\n",
    "    if generation > 0:\n",
    "        children = []\n",
    "        ranked_population = non_dominated_sort(evaluated_population)\n",
    "        front_num = 1\n",
    "        while True:\n",
    "            current_front = [ind for ind in ranked_population if ind.get('rank') == front_num]\n",
    "            if not current_front: break\n",
    "            calculate_crowding_distance(current_front)\n",
    "            front_num += 1\n",
    "\n",
    "        # Ensure we have a valid population to select from\n",
    "        if ranked_population:\n",
    "            while len(children) < POPULATION_SIZE:\n",
    "                parent1 = selection_operator(ranked_population)\n",
    "                parent2 = selection_operator(ranked_population)\n",
    "                child = crossover(parent1, parent2)\n",
    "                child = mutate(child, all_signal_ids, MUTATION_RATE)\n",
    "                child['id'] = f'S{setup_id_counter:04d}'\n",
    "                setup_id_counter += 1\n",
    "                children.append(child)\n",
    "\n",
    "            evaluated_children = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup, returns) for setup in children)\n",
    "            combined_population = evaluated_population + evaluated_children\n",
    "\n",
    "    ### FIX: Apply the strict phenotype filter ONLY AFTER the first generation ###\n",
    "    if generation > 0 and combined_population:\n",
    "        phenotype_dict = {}\n",
    "        for ind in combined_population:\n",
    "            trigger_dates = ind.get('trigger_dates')\n",
    "            if trigger_dates is None or trigger_dates.empty:\n",
    "                continue\n",
    "            fingerprint = tuple(trigger_dates)\n",
    "            if fingerprint not in phenotype_dict:\n",
    "                phenotype_dict[fingerprint] = ind\n",
    "        unique_phenotype_population = list(phenotype_dict.values())\n",
    "    else:\n",
    "        # For Gen 0, we allow functional duplicates to ensure a healthy gene pool for creating offspring\n",
    "        unique_phenotype_population = combined_population\n",
    "\n",
    "    if not unique_phenotype_population:\n",
    "        print(\"Population extinct. No valid individuals to select for the next generation. Stopping.\")\n",
    "        break\n",
    "\n",
    "    # NSGA-II Selection for the next generation\n",
    "    sorted_population = non_dominated_sort(unique_phenotype_population)\n",
    "    next_generation_population = []\n",
    "    front_num = 1\n",
    "    while len(next_generation_population) < POPULATION_SIZE:\n",
    "        current_front = [ind for ind in sorted_population if ind['rank'] == front_num]\n",
    "        if not current_front:\n",
    "            break\n",
    "\n",
    "        calculate_crowding_distance(current_front)\n",
    "        if len(next_generation_population) + len(current_front) <= POPULATION_SIZE:\n",
    "            next_generation_population.extend(current_front)\n",
    "        else:\n",
    "            current_front.sort(key=lambda x: x['crowding_distance'], reverse=True)\n",
    "            num_needed = POPULATION_SIZE - len(next_generation_population)\n",
    "            next_generation_population.extend(current_front[:num_needed])\n",
    "        front_num += 1\n",
    "\n",
    "    # If the next generation is empty, stop the process.\n",
    "    if not next_generation_population:\n",
    "        print(f\"Warning: Could not form next generation from {len(unique_phenotype_population)} unique individuals. Stopping.\")\n",
    "        break\n",
    "\n",
    "    current_population = next_generation_population\n",
    "\n",
    "    # Update Hall of Fame\n",
    "    current_best_front = [ind for ind in sorted_population if ind['rank'] == 1]\n",
    "\n",
    "    if current_best_front:\n",
    "        hall_of_fame_candidates = non_dominated_sort(hall_of_fame + current_best_front)\n",
    "        hof_phenotype_dict = {}\n",
    "        for ind in hall_of_fame_candidates:\n",
    "            trigger_dates = ind.get('trigger_dates')\n",
    "            if trigger_dates is None or trigger_dates.empty: continue\n",
    "            fingerprint = tuple(trigger_dates)\n",
    "            if fingerprint not in hof_phenotype_dict:\n",
    "                 hof_phenotype_dict[fingerprint] = ind\n",
    "        hall_of_fame = [ind for ind in hof_phenotype_dict.values() if ind['rank'] == 1]\n",
    "\n",
    "    # Report generation statistics\n",
    "    if hall_of_fame:\n",
    "        hall_of_fame.sort(key=lambda x: x['objectives'][0], reverse=True)\n",
    "        best_of_gen = hall_of_fame[0]\n",
    "        print(f\"Generation {generation + 1} Complete. Unique Phenotypes: {len(unique_phenotype_population)}. Hall of Fame: {len(hall_of_fame)}. Best: (S:{best_of_gen['objectives'][0]:.2f}, C:{best_of_gen['objectives'][1]:.2f}, Sup:{best_of_gen['objectives'][2]})\")\n",
    "    else:\n",
    "        print(f\"Generation {generation + 1} Complete. No valid solutions in Hall of Fame.\")\n",
    "# --- PRIORITY 4: Final Evaluation and Multi-Objective Report ---\n",
    "print(\"\\n--- Genetic Algorithm Complete. Generating Final Report from Hall of Fame ---\")\n",
    "\n",
    "if not hall_of_fame:\n",
    "    print(\"Discovery complete. The Hall of Fame is empty; no valid setups were found.\")\n",
    "else:\n",
    "    ### FIX: Create a final, unique list of solutions to process. ###\n",
    "    # This is a robust, \"belt-and-suspenders\" approach to ensure no duplicates make it to the final report.\n",
    "    final_pareto_front_df = pd.DataFrame(hall_of_fame)\n",
    "    final_pareto_front_df.drop_duplicates(subset=['id'], keep='first', inplace=True)\n",
    "    final_pareto_front = final_pareto_front_df.to_dict('records')\n",
    "\n",
    "    # --- ADD THIS BLOCK ---\n",
    "    #if FILTER_BY_LAST_TRIGGER_DATE:\n",
    "       # print(f\"\\n--- FILTERING FINAL RESULTS FOR LAST TRIGGER DATE: {FILTER_BY_LAST_TRIGGER_DATE} ---\")\n",
    "       # target_date = pd.to_datetime(FILTER_BY_LAST_TRIGGER_DATE)\n",
    "       # final_pareto_front = [\n",
    "          #  setup for setup in final_pareto_front\n",
    "           # if pd.to_datetime(setup.get('last_trigger_date')).date() == target_date.date()\n",
    "        #]\n",
    "    # --- END ADD ---\n",
    "\n",
    "    print(f\"Final unique Pareto Front contains {len(final_pareto_front)} non-dominated solutions.\")\n",
    "\n",
    "    print(f\"\\n--- Performing deep dive on {len(final_pareto_front)} unique solutions ---\")\n",
    "    all_trade_ledger_rows = []\n",
    "    all_description_records = []\n",
    "    summary_rows = []\n",
    "\n",
    "    # Iterate over the cleaned, unique list of solutions\n",
    "    for setup_solution in final_pareto_front:\n",
    "        setup_id = setup_solution['id']\n",
    "        setup_def = setup_solution['signal_definitions']\n",
    "\n",
    "        all_description_records.append(generate_english_description(setup_id, setup_def, feature_specs))\n",
    "\n",
    "        mask = functools.reduce(lambda a, b: a & b, [signal_series[s['signal_id']] for s in setup_def])\n",
    "        dates = mask[mask].index\n",
    "\n",
    "        best_ticker = \"N/A\"\n",
    "        best_sortino = -999\n",
    "        if setup_solution.get('metrics_by_ticker'):\n",
    "            for ticker, metrics in setup_solution['metrics_by_ticker'].items():\n",
    "                if metrics.get('sortino', -999) > best_sortino:\n",
    "                    best_sortino = metrics['sortino']\n",
    "                    best_ticker = ticker\n",
    "\n",
    "        recency_sharpe = np.nan\n",
    "        if len(dates) >= RECENCY_WINDOW:\n",
    "            recent_dates = dates[-RECENCY_WINDOW:]\n",
    "            h = 10\n",
    "            recent_sharpes = []\n",
    "            for tk_symbol in TRADABLE_TICKERS:\n",
    "                 price_col_name = first_col_containing(tk_symbol, 'PX_LAST')\n",
    "                 if price_col_name:\n",
    "                    r_recent = returns[h][price_col_name].reindex(recent_dates).dropna()\n",
    "                    if setup_solution['entry_direction'] == 'short': r_recent = -r_recent\n",
    "                    if r_recent.std() > 1e-9 and len(r_recent) > 2:\n",
    "                        recent_sharpes.append((r_recent.mean() / r_recent.std()) * np.sqrt(252/h))\n",
    "            if recent_sharpes:\n",
    "                recency_sharpe = np.nanmedian(recent_sharpes)\n",
    "\n",
    "        summary_rows.append({\n",
    "            'setup_id': setup_id,\n",
    "            'rank': setup_solution['rank'],\n",
    "            'best_performing_ticker': best_ticker,\n",
    "            'obj_sortino': setup_solution['objectives'][0],\n",
    "            'obj_calmar': setup_solution['objectives'][1],\n",
    "            'obj_support': setup_solution['objectives'][2],\n",
    "            'entry_direction': setup_solution['entry_direction'],\n",
    "            'first_trigger_date': setup_solution.get('first_trigger_date'),\n",
    "            'last_trigger_date': setup_solution.get('last_trigger_date'),\n",
    "            'recency_sharpe': recency_sharpe,\n",
    "        })\n",
    "\n",
    "        # Build Trade Ledger for this specific setup\n",
    "        for tk_symbol in TRADABLE_TICKERS:\n",
    "            tk_col = first_col_containing(tk_symbol, 'PX_LAST')\n",
    "            if not tk_col: continue\n",
    "\n",
    "            ivol_col = (first_col_containing(tk_symbol, '30_Day_Call_Implied_Volatility') or first_col_containing(tk_symbol, 'IVOL_SIGMA'))\n",
    "            ivol_series = raw[ivol_col].reindex(dates) if ivol_col and ivol_col in raw.columns else pd.Series(np.nan, index=dates)\n",
    "            entry_px_series = raw[tk_col].reindex(dates)\n",
    "\n",
    "            for d in dates:\n",
    "                entry_px = entry_px_series.loc[d]\n",
    "                ivol = ivol_series.loc[d] if not ivol_series.empty and pd.notna(d) and d in ivol_series.index else np.nan\n",
    "                for h_opt in OPTION_SIM_HORIZONS_DAYS:\n",
    "                    exit_date = d + pd.Timedelta(days=h_opt)\n",
    "                    future_px_series = raw.loc[raw.index >= exit_date, tk_col]\n",
    "                    final_exit_px = future_px_series.iloc[0] if not future_px_series.empty else np.nan\n",
    "                    pnl_detail = simulate_option_pnl_detailed(entry_px, final_exit_px, ivol, h_opt, setup_solution['entry_direction'])\n",
    "                    all_trade_ledger_rows.append({'setup_id': setup_id, 'trigger_date': d, 'target_ticker': tk_symbol, 'horizon_days': h_opt, **pnl_detail})\n",
    "\n",
    "    # --- Final Assembly and Output ---\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    trade_ledger_df = pd.DataFrame(all_trade_ledger_rows)\n",
    "    description_df = pd.DataFrame(all_description_records).drop_duplicates(subset=['setup_id'])\n",
    "\n",
    "    if not trade_ledger_df.empty:\n",
    "        for h_opt in OPTION_SIM_HORIZONS_DAYS:\n",
    "            pnl_dollars_map = trade_ledger_df[trade_ledger_df['horizon_days'] == h_opt].groupby('setup_id')['pnl_dollars'].mean()\n",
    "            pnl_pct_map = trade_ledger_df[trade_ledger_df['horizon_days'] == h_opt].groupby('setup_id')['pnl_pct'].mean()\n",
    "            summary_df[f'avg_option_pnl_dollars_{h_opt}d'] = summary_df['setup_id'].map(pnl_dollars_map)\n",
    "            summary_df[f'avg_option_pnl_pct_{h_opt}d'] = summary_df['setup_id'].map(pnl_pct_map)\n",
    "\n",
    "    numeric_cols = summary_df.select_dtypes(include=np.number).columns\n",
    "    summary_df[numeric_cols] = summary_df[numeric_cols].round(4)\n",
    "    if not trade_ledger_df.empty:\n",
    "        trade_ledger_df = trade_ledger_df.round({'pnl_pct': 4, 'pnl_dollars': 2})\n",
    "\n",
    "    final_summary_df = pd.merge(summary_df, description_df[['setup_id', 'description']], on='setup_id', how='left')\n",
    "    final_summary_df.sort_values(by=['obj_sortino', 'obj_calmar', 'obj_support'], ascending=[False, False, False], inplace=True)\n",
    "\n",
    "    print('\\n--- Generating Final Output Files ---')\n",
    "    final_summary_df.to_csv('pareto_front_summary.csv', index=False)\n",
    "    print(\"Saved 'pareto_front_summary.csv'\")\n",
    "    trade_ledger_df.to_csv('pareto_front_trade_ledger.csv', index=False)\n",
    "    print(\"Saved 'pareto_front_trade_ledger.csv'\")\n",
    "\n",
    "    print(\"\\n--- Generating Final JSON and Summary ---\")\n",
    "    top_setups_for_json = final_summary_df.copy()\n",
    "    top_setups_for_json.replace({np.nan: None, pd.NaT: None}, inplace=True)\n",
    "    date_cols = ['first_trigger_date', 'last_trigger_date']\n",
    "    for col in date_cols:\n",
    "        if col in top_setups_for_json.columns:\n",
    "            top_setups_for_json[col] = pd.to_datetime(top_setups_for_json[col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "            top_setups_for_json[col].replace({pd.NaT: None}, inplace=True)\n",
    "    top_setups_json = top_setups_for_json.to_dict(orient='records')\n",
    "    with open('pareto/pareto_front_setups.json', 'w') as f:\n",
    "        json.dump(top_setups_json, f, indent=2)\n",
    "    print(\"Saved 'pareto_front_setups.json'\")\n",
    "\n",
    "    print('\\nDiscovery complete.')\n",
    "    print(\"\\nSolutions on the Final Pareto Front (sorted by Sortino):\")\n",
    "    display_cols = ['setup_id', 'rank', 'best_performing_ticker', 'obj_sortino', 'obj_calmar', 'obj_support', 'recency_sharpe', 'description']\n",
    "    print(final_summary_df[display_cols].head(15).to_string())"
   ],
   "id": "3a54363df56aa888",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw workbooksâ€¦\n",
      "Raw shape: (1978, 568)\n",
      "\n",
      "Identified all relevant prefixes/tickers for feature engineering: 33\n",
      "\n",
      "--- Defining ALL Feature Specifications ---\n",
      "Defined 3359 total feature specifications.\n",
      "--- Calculating All Features ---\n",
      "Calculated 3132 feature series.\n",
      "Successfully created sequential feature: 'SEQ_VIX_SPIKE_THEN_CORR_DROP'\n",
      "Successfully created sequential feature: 'SEQ_YIELD_DROP_THEN_GOLD_VOL_SPIKE'\n",
      "Successfully created sequential feature: 'SEQ_NVDA_VOL_SPIKE_THEN_QQQ_PRICE_RISE'\n",
      "--- Defining Primitive Signals ---\n",
      "Defined 10325 primitive signals.\n",
      "\n",
      "--- GENETIC ALGORITHM: Creating Initial Population (Generation 0) ---\n",
      "Seeding with pre-validated single and pair setups...\n",
      "  - Created 200 pre-validated setups.\n",
      "Filling remainder of population with pre-validated random setups...\n",
      "Created initial population of 250 guaranteed viable setups.\n",
      "\n",
      "--- Evaluating Generation 1/50 ---\n",
      "Generation 1 Complete. Unique Phenotypes: 250. Hall of Fame: 9. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 2/50 ---\n",
      "Generation 2 Complete. Unique Phenotypes: 422. Hall of Fame: 15. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 3/50 ---\n",
      "Generation 3 Complete. Unique Phenotypes: 458. Hall of Fame: 17. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 4/50 ---\n",
      "Generation 4 Complete. Unique Phenotypes: 442. Hall of Fame: 20. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 5/50 ---\n",
      "Generation 5 Complete. Unique Phenotypes: 446. Hall of Fame: 29. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 6/50 ---\n",
      "Generation 6 Complete. Unique Phenotypes: 438. Hall of Fame: 28. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 7/50 ---\n",
      "Generation 7 Complete. Unique Phenotypes: 423. Hall of Fame: 34. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 8/50 ---\n",
      "Generation 8 Complete. Unique Phenotypes: 430. Hall of Fame: 35. Best: (S:999.00, C:999.00, Sup:9)\n",
      "\n",
      "--- Evaluating Generation 9/50 ---\n",
      "Generation 9 Complete. Unique Phenotypes: 439. Hall of Fame: 23. Best: (S:999.00, C:999.00, Sup:12)\n",
      "\n",
      "--- Evaluating Generation 10/50 ---\n",
      "Generation 10 Complete. Unique Phenotypes: 413. Hall of Fame: 27. Best: (S:999.00, C:999.00, Sup:12)\n",
      "\n",
      "--- Evaluating Generation 11/50 ---\n",
      "Generation 11 Complete. Unique Phenotypes: 414. Hall of Fame: 28. Best: (S:999.00, C:999.00, Sup:12)\n",
      "\n",
      "--- Evaluating Generation 12/50 ---\n",
      "Generation 12 Complete. Unique Phenotypes: 410. Hall of Fame: 30. Best: (S:999.00, C:999.00, Sup:12)\n",
      "\n",
      "--- Evaluating Generation 13/50 ---\n",
      "Generation 13 Complete. Unique Phenotypes: 407. Hall of Fame: 38. Best: (S:999.00, C:999.00, Sup:12)\n",
      "\n",
      "--- Evaluating Generation 14/50 ---\n",
      "Generation 14 Complete. Unique Phenotypes: 409. Hall of Fame: 39. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 15/50 ---\n",
      "Generation 15 Complete. Unique Phenotypes: 389. Hall of Fame: 44. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 16/50 ---\n",
      "Generation 16 Complete. Unique Phenotypes: 392. Hall of Fame: 47. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 17/50 ---\n",
      "Generation 17 Complete. Unique Phenotypes: 393. Hall of Fame: 50. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 18/50 ---\n",
      "Generation 18 Complete. Unique Phenotypes: 404. Hall of Fame: 53. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 19/50 ---\n",
      "Generation 19 Complete. Unique Phenotypes: 398. Hall of Fame: 55. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 20/50 ---\n",
      "Generation 20 Complete. Unique Phenotypes: 405. Hall of Fame: 53. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 21/50 ---\n",
      "Generation 21 Complete. Unique Phenotypes: 403. Hall of Fame: 54. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 22/50 ---\n",
      "Generation 22 Complete. Unique Phenotypes: 405. Hall of Fame: 56. Best: (S:999.00, C:999.00, Sup:15)\n",
      "\n",
      "--- Evaluating Generation 23/50 ---\n",
      "Generation 23 Complete. Unique Phenotypes: 410. Hall of Fame: 47. Best: (S:999.00, C:999.00, Sup:20)\n",
      "\n",
      "--- Evaluating Generation 24/50 ---\n",
      "Generation 24 Complete. Unique Phenotypes: 406. Hall of Fame: 46. Best: (S:999.00, C:999.00, Sup:20)\n",
      "\n",
      "--- Evaluating Generation 25/50 ---\n",
      "Generation 25 Complete. Unique Phenotypes: 384. Hall of Fame: 43. Best: (S:999.00, C:999.00, Sup:20)\n",
      "\n",
      "--- Evaluating Generation 26/50 ---\n",
      "Generation 26 Complete. Unique Phenotypes: 388. Hall of Fame: 44. Best: (S:999.00, C:999.00, Sup:20)\n",
      "\n",
      "--- Evaluating Generation 27/50 ---\n",
      "Generation 27 Complete. Unique Phenotypes: 391. Hall of Fame: 43. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 28/50 ---\n",
      "Generation 28 Complete. Unique Phenotypes: 400. Hall of Fame: 46. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 29/50 ---\n",
      "Generation 29 Complete. Unique Phenotypes: 400. Hall of Fame: 46. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 30/50 ---\n",
      "Generation 30 Complete. Unique Phenotypes: 403. Hall of Fame: 53. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 31/50 ---\n",
      "Generation 31 Complete. Unique Phenotypes: 415. Hall of Fame: 53. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 32/50 ---\n",
      "Generation 32 Complete. Unique Phenotypes: 412. Hall of Fame: 53. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 33/50 ---\n",
      "Generation 33 Complete. Unique Phenotypes: 413. Hall of Fame: 52. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 34/50 ---\n",
      "Generation 34 Complete. Unique Phenotypes: 397. Hall of Fame: 53. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 35/50 ---\n",
      "Generation 35 Complete. Unique Phenotypes: 414. Hall of Fame: 54. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 36/50 ---\n",
      "Generation 36 Complete. Unique Phenotypes: 402. Hall of Fame: 54. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 37/50 ---\n",
      "Generation 37 Complete. Unique Phenotypes: 412. Hall of Fame: 51. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 38/50 ---\n",
      "Generation 38 Complete. Unique Phenotypes: 413. Hall of Fame: 53. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 39/50 ---\n",
      "Generation 39 Complete. Unique Phenotypes: 420. Hall of Fame: 55. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 40/50 ---\n",
      "Generation 40 Complete. Unique Phenotypes: 415. Hall of Fame: 55. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 41/50 ---\n",
      "Generation 41 Complete. Unique Phenotypes: 426. Hall of Fame: 59. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 42/50 ---\n",
      "Generation 42 Complete. Unique Phenotypes: 416. Hall of Fame: 59. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 43/50 ---\n",
      "Generation 43 Complete. Unique Phenotypes: 428. Hall of Fame: 58. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 44/50 ---\n",
      "Generation 44 Complete. Unique Phenotypes: 410. Hall of Fame: 58. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 45/50 ---\n",
      "Generation 45 Complete. Unique Phenotypes: 416. Hall of Fame: 60. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 46/50 ---\n",
      "Generation 46 Complete. Unique Phenotypes: 415. Hall of Fame: 61. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 47/50 ---\n",
      "Generation 47 Complete. Unique Phenotypes: 408. Hall of Fame: 63. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 48/50 ---\n",
      "Generation 48 Complete. Unique Phenotypes: 428. Hall of Fame: 66. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 49/50 ---\n",
      "Generation 49 Complete. Unique Phenotypes: 412. Hall of Fame: 68. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Evaluating Generation 50/50 ---\n",
      "Generation 50 Complete. Unique Phenotypes: 421. Hall of Fame: 67. Best: (S:999.00, C:999.00, Sup:26)\n",
      "\n",
      "--- Genetic Algorithm Complete. Generating Final Report from Hall of Fame ---\n",
      "Final unique Pareto Front contains 67 non-dominated solutions.\n",
      "\n",
      "--- Performing deep dive on 67 unique solutions ---\n",
      "\n",
      "--- Generating Final Output Files ---\n",
      "Saved 'pareto_front_summary.csv'\n",
      "Saved 'pareto_front_trade_ledger.csv'\n",
      "\n",
      "--- Generating Final JSON and Summary ---\n",
      "Saved 'pareto_front_setups.json'\n",
      "\n",
      "Discovery complete.\n",
      "\n",
      "Solutions on the Final Pareto Front (sorted by Sortino):\n",
      "   setup_id  rank best_performing_ticker  obj_sortino    obj_calmar  obj_support  recency_sharpe                                                                                                                                                                                                                                                                      description\n",
      "0     S6693     1          QQQ US Equity     999.0000  9.990000e+02           26          6.5061                     When corr(QQQ US Equity:PX_LAST, USGG2YR Index:PX_LAST, 60d) is very high and corr(MLCX3CRT Index:PX_LAST, QQQ US Equity:PX_LAST, 60d) is very high and div(PUT_IMP_VOL_30D, VOLUME)__NBIS US Equity surges unexpectedly, it may indicate a bullish outlook.\n",
      "1     S1887     1          QQQ US Equity     673.9617  1.879263e+09           12          4.8194                When div(CALL_IMP_VOL_30D, VOLUME)__AAPL US Equity surges unexpectedly and corr(CO1 Comdty:PX_LAST, NBIS US Equity:PX_LAST, 60d) surges unexpectedly and corr(XLF US Equity:PX_LAST, XLK US Equity:PX_LAST, 20d) is very high, it may indicate a bullish outlook.\n",
      "2     S9056     1          QQQ US Equity     491.6955  3.928918e+10           25          7.7744                    When corr(NFP TCH Index:PX_LAST, XLF US Equity:PX_LAST, 60d) is very low and beta(DXY Curncy:PX_LAST, NVDA US Equity:PX_LAST, 60d) is very high and corr(USGG10YR Index:PX_LAST, XLK US Equity:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "3     S6581     1          QQQ US Equity     303.2071  2.542577e+15           18          6.5061       When corr(SPY US Equity:PX_LAST, XLF US Equity:PX_LAST, 60d) surges unexpectedly and beta(SPY US Equity:PX_LAST, XLK US Equity:PX_LAST, 60d) surges unexpectedly and div(CALL_IMP_VOL_30D, VOLUME)__NBIS US Equity surges unexpectedly, it may indicate a bullish outlook.\n",
      "4     S7649     1          QQQ US Equity     277.9693  2.028095e+14           26          6.5061                    When beta(CO1 Comdty:PX_LAST, DXY Curncy:PX_LAST, 60d) is very high and beta(SPY US Equity:PX_LAST, XLK US Equity:PX_LAST, 60d) surges unexpectedly and div(CALL_IMP_VOL_30D, VOLUME)__NBIS US Equity surges unexpectedly, it may indicate a bullish outlook.\n",
      "5     S9373     1          QQQ US Equity     188.6207  1.085292e+09           31          8.4029                       When corr(USGG10YR Index:PX_LAST, XLK US Equity:PX_LAST, 60d) is very high and corr(CPI CHNG Index:PX_LAST, LF94TRUU Index:PX_LAST, 60d) is very low and beta(DXY Curncy:PX_LAST, SPX Index:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "6     S5667     1         TSLA US Equity     182.3540  5.599176e+08           38          9.9750                  When beta(CO1 Comdty:PX_LAST, DXY Curncy:PX_LAST, 60d) is very high and corr(ARKK US Equity:PX_LAST, CO1 Comdty:PX_LAST, 60d) is very high and zscore_corr(20d)(DXY Curncy:PX_LAST, GLD US Equity:PX_LAST, 60d) is very low, it may indicate a bullish outlook.\n",
      "7    S11386     1          QQQ US Equity     117.3780  2.634083e+10           34          9.0678  When beta(SPY US Equity:PX_LAST, XLK US Equity:PX_LAST, 60d) surges unexpectedly and zscore_corr(20d)(DXY Curncy:PX_LAST, GLD US Equity:PX_LAST, 60d) is very low and corr(MLCX3CRT Index:PX_LAST, QQQ US Equity:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "8     S6927     1          XLK US Equity     110.8847  1.967843e+06           69          8.4029               When zscore_corr(20d)(DXY Curncy:PX_LAST, GLD US Equity:PX_LAST, 60d) is very low and beta(CO1 Comdty:PX_LAST, DXY Curncy:PX_LAST, 60d) is very high and corr(USGG10YR Index:PX_LAST, XLK US Equity:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "9    S12437     1          XLK US Equity      94.5908  1.232438e+08           47          3.1133                       When beta(CO1 Comdty:PX_LAST, DXY Curncy:PX_LAST, 60d) is very high and corr(QQQ US Equity:PX_LAST, USGG2YR Index:PX_LAST, 60d) is very high and corr(SPCS20SM Index:PX_LAST, XLE US Equity:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "10    S9660     1          QQQ US Equity      93.1161  7.583309e+12           29          5.9751                                    When div(IVOL_SIGMA, VOLUME)__NBIS US Equity surges unexpectedly and corr(CO1 Comdty:PX_LAST, USGG2YR Index:PX_LAST, 60d) is very high and beta(CO1 Comdty:PX_LAST, DXY Curncy:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "11    S8827     1         ARKK US Equity      87.5712  2.612605e+07           53          4.3719                       When corr(SPCS20SM Index:PX_LAST, XLE US Equity:PX_LAST, 60d) is very high and beta(DXY Curncy:PX_LAST, SPX Index:PX_LAST, 60d) is very high and corr(USGG10YR Index:PX_LAST, XLK US Equity:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "12   S10055     1          XLK US Equity      81.5348  1.332611e+07           56         13.6848            When zscore_corr(20d)(DXY Curncy:PX_LAST, GLD US Equity:PX_LAST, 60d) is very low and corr(QQQ US Equity:PX_LAST, USGG2YR Index:PX_LAST, 60d) is very high and beta(DXY Curncy:PX_LAST, NVDA US Equity:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "13   S10597     1         NBIS US Equity      81.0989  1.907933e+08           46         12.6940          When corr(XLF US Equity:PX_LAST, XLK US Equity:PX_LAST, 20d) is very high and zscore_corr(20d)(DXY Curncy:PX_LAST, GLD US Equity:PX_LAST, 60d) is very low and corr(QQQ US Equity:PX_LAST, USGG2YR Index:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n",
      "14    S5500     1          XLK US Equity      59.1802  4.011202e+06           59          8.4029                       When corr(USGG10YR Index:PX_LAST, XLK US Equity:PX_LAST, 60d) is very high and corr(MLCX3CRT Index:PX_LAST, QQQ US Equity:PX_LAST, 60d) is very high and beta(DXY Curncy:PX_LAST, SPX Index:PX_LAST, 60d) is very high, it may indicate a bullish outlook.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
