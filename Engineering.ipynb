{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T13:11:27.210921Z",
     "start_time": "2025-07-29T12:27:59.050300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Engineering + Discovery Engine with custom features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "import itertools # NEW: For combinatorial setup generation\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "# Define the explicit list of tradable tickers (full sheet names from your All_Tickers copy.xlsx)\n",
    "# These will be the only assets for which returns are calculated for strategy performance.\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "    # DXY Curncy, USGG10YR Index, CO1 Comdty are explicitly excluded from tradable targets\n",
    "    # as per your request (due to no options data for future simulation, etc.)\n",
    "]\n",
    "\n",
    "# Define the file paths\n",
    "MAIN_DATA_FILE = 'All_Tickers copy.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_no_nan_cols.xlsx'\n",
    "\n",
    "# Setup Generation Configuration\n",
    "SETUP_LENGTHS_TO_EXPLORE = [2, 3, 4] # Explore setups with 2, 3, and 4 conditions\n",
    "MIN_INITIAL_SUPPORT_FILTER = 10 # Minimum number of trigger days for a setup to be considered\n",
    "\n",
    "# Option Simulation Configuration\n",
    "OPTION_SIM_HORIZON_DAYS = 10 # Days to expiration for simulated options\n",
    "RISK_FREE_RATE = 0.01 # Annual risk-free rate for option premium estimation\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooks â€¦')\n",
    "raw = None # Initialize raw to None\n",
    "\n",
    "# --- Custom Data Loading Function to handle sheet names as prefixes ---\n",
    "def load_and_merge_excel(file_path, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "\n",
    "        for sh_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name)\n",
    "            # Prepend sheet name to all columns except 'Date'\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                # Use outer merge to keep all dates from all sheets\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found. Please ensure the file is in the correct directory.\")\n",
    "        return existing_df # Return existing_df or None if first file\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df # Return existing_df or None if first file\n",
    "\n",
    "# Load main data\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE)\n",
    "\n",
    "# Load macro data if main data was loaded successfully\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame({'Date': pd.to_datetime([])}) # Fallback to empty DF if main load failed\n",
    "\n",
    "# Final cleaning and sorting\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame({'Date': pd.to_datetime([])}) # Ensure raw is a DataFrame\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "print('Example columns after loading:')\n",
    "print(raw.columns[:5].tolist()) # Print first 5 columns to verify naming\n",
    "\n",
    "\n",
    "# --- NEW ADDITION (Phase 1, Item 1): Dynamic Ticker Identification Refined ---\n",
    "# all_tickers will now refer to ALL unique ticker prefixes found in columns after loading,\n",
    "# not just PX_LAST, but it will exclude 'Date' and common feature parts.\n",
    "# This is mainly for feature engineering loops.\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c and c != 'Date'])))\n",
    "# Filter out common feature names that might accidentally be parsed as tickers\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total',\n",
    "                           '30', '10', '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC',\n",
    "                           'NFP', 'JOBS', 'CPI', 'CTII10', 'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT',\n",
    "                           'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM', 'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "# Ensure that TRADABLE_TICKERS are definitely included even if they don't have a specific prefixed column.\n",
    "# This list is used for generalizing feature engineering.\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes)))\n",
    "\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {all_tickers}')\n",
    "print(f'Actual tradable tickers for returns: {TRADABLE_TICKERS}')\n",
    "\n",
    "\n",
    "# --- Revised Helper functions ---\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    \"\"\"\n",
    "    Finds the first column name in raw that matches the pattern 'ticker_full_name_substr'.\n",
    "    Handles cases where substr might be 'PX_LAST' and the actual column name is 'ticker_full_name_Last_Price_PX_LAST'.\n",
    "    \"\"\"\n",
    "    # Prefer explicit matches for PX_LAST which can have two forms\n",
    "    if substr == 'PX_LAST':\n",
    "        potential_col_name_long_price = f\"{ticker_full_name}_Last_Price_PX_LAST\"\n",
    "        if potential_col_name_long_price in raw.columns:\n",
    "            return potential_col_name_long_price\n",
    "\n",
    "        # Check for the simpler form if the above doesn't exist\n",
    "        potential_col_name_short_px = f\"{ticker_full_name}_PX_LAST\"\n",
    "        if potential_col_name_short_px in raw.columns:\n",
    "            return potential_col_name_short_px\n",
    "\n",
    "    # Generic search for other substrings\n",
    "    for c in raw.columns:\n",
    "        # Check for 'FULL_TICKER_NAME_SUBSTR' (e.g., QQQ US Equity_VOLUME)\n",
    "        if c.startswith(ticker_full_name) and substr in c:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def safe_series(col_name):\n",
    "    \"\"\"Returns a column as a Series, or an empty Series if column does not exist.\"\"\"\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "# --- NEW ADDITION (Phase 1, Item 2): Helper function for Block Bootstrap Sharpe ---\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=1000, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"\n",
    "    Calculates the Sharpe Ratio using block bootstrapping to account for serial correlation.\n",
    "    \"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    sharpes = []\n",
    "    blocks = []\n",
    "    for i in range(0, len(returns_series), block_size):\n",
    "        block = returns_series.iloc[i : i + block_size]\n",
    "        if not block.empty:\n",
    "            blocks.append(block)\n",
    "\n",
    "    if not blocks:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size))\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = []\n",
    "        sampled_blocks_indices = np.random.choice(len(blocks), n_blocks_to_sample, replace=True)\n",
    "        for idx in sampled_blocks_indices:\n",
    "            resampled_returns_list.append(blocks[idx])\n",
    "\n",
    "        resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)]\n",
    "\n",
    "        if resampled_returns.std() > 1e-9: # Original threshold for non-zero std\n",
    "            daily_sharpe = resampled_returns.mean() / resampled_returns.std()\n",
    "            if annualize:\n",
    "                sharpes.append(daily_sharpe * np.sqrt(trading_days_per_year))\n",
    "            else:\n",
    "                sharpes.append(daily_sharpe)\n",
    "        else:\n",
    "            sharpes.append(0.0)\n",
    "\n",
    "    sharpes_sorted = sorted(sharpes)\n",
    "    if not sharpes_sorted:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    median_sharpe = np.median(sharpes_sorted)\n",
    "    lower_ci = sharpes_sorted[int(0.05 * num_iterations)]\n",
    "    upper_ci = sharpes_sorted[int(0.95 * num_iterations)]\n",
    "\n",
    "    return median_sharpe, lower_ci, upper_ci\n",
    "# --- END NEW ADDITION ---\n",
    "\n",
    "\n",
    "# --- NEW ADDITION (Phase 2, Item 1): Option Payoff Simulation Helpers ---\n",
    "# Simplified Black-Scholes premium estimation (for ATM options)\n",
    "def estimate_atm_premium(S, ivol, T_days, r_annual=RISK_FREE_RATE, option_type='call'):\n",
    "    \"\"\"\n",
    "    Estimates ATM option premium using a simplified Black-Scholes like approach.\n",
    "    This is a rough proxy, not a full BSM model.\n",
    "    \"\"\"\n",
    "    if S <= 0 or ivol <= 0 or T_days <= 0:\n",
    "        return 0.001 # Return a minimal premium if inputs are invalid\n",
    "\n",
    "    T_years = T_days / 252.0 # Annualize time to expiration\n",
    "\n",
    "    # A common rule of thumb for ATM options: premium is roughly 0.4 * S * IVOL * sqrt(T_years)\n",
    "    # The 0.4 factor approximates N(d1) and N(d2) for ATM options around 0.5.\n",
    "    premium_estimate = 0.4 * S * ivol * np.sqrt(T_years)\n",
    "\n",
    "    return max(premium_estimate, 0.001) # Ensure premium is at least minimal\n",
    "\n",
    "def simulate_option_pnl(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    \"\"\"\n",
    "    Simulates PnL for buying a simple ATM call or put.\n",
    "    \"\"\"\n",
    "    if pd.isna(current_price) or pd.isna(future_price) or pd.isna(ivol_at_entry):\n",
    "        return np.nan\n",
    "\n",
    "    strike = current_price # At-The-Money option\n",
    "\n",
    "    if entry_direction == 'long':\n",
    "        # Simulate buying an ATM Call\n",
    "        premium = estimate_atm_premium(current_price, ivol_at_entry, horizon_days, option_type='call')\n",
    "        payoff = max(future_price - strike, 0)\n",
    "        pnl = payoff - premium\n",
    "    elif entry_direction == 'short':\n",
    "        # Simulate buying an ATM Put (for a 'short' signal in underlying)\n",
    "        premium = estimate_atm_premium(current_price, ivol_at_entry, horizon_days, option_type='put')\n",
    "        payoff = max(strike - future_price, 0)\n",
    "        pnl = payoff - premium\n",
    "    else: # Mixed or other, no specific option trade\n",
    "        pnl = np.nan # Not applicable\n",
    "\n",
    "    return pnl\n",
    "# --- END Option Payoff Simulation Helpers ---\n",
    "\n",
    "\n",
    "print('\\nEngineering custom features â€¦')\n",
    "feat = pd.DataFrame({'Date': raw['Date']})\n",
    "\n",
    "# --- NEW ADDITION: Fractional Differencing Helper (moved here for feature engineering use) ---\n",
    "def frac_diff(series, d=0.5, window='full'):\n",
    "    \"\"\"\n",
    "    Computes fractionally differenced series.\n",
    "    Adapted from Lopez de Prado's \"Advances in Financial Machine Learning\".\n",
    "    \"\"\"\n",
    "    if not isinstance(series, pd.Series):\n",
    "        series = pd.Series(series)\n",
    "\n",
    "    # Compute weights\n",
    "    weights = [1.]\n",
    "    for k in range(1, series.shape[0]):\n",
    "        weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1]) # Reverse for convolution\n",
    "\n",
    "    output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(series.shape[0]):\n",
    "        if window == 'full':\n",
    "            start = 0\n",
    "        else: # fixed window\n",
    "            start = max(0, i - window + 1)\n",
    "\n",
    "        subset = series.iloc[start : i + 1]\n",
    "\n",
    "        # Ensure weights are aligned with subset length\n",
    "        current_weights = weights[-(i - start + 1):]\n",
    "        if len(subset) == len(current_weights):\n",
    "            output.iloc[i] = np.dot(current_weights, subset)\n",
    "    return output.dropna()\n",
    "# --- END Fractional Differencing Helper ---\n",
    "\n",
    "\n",
    "# 1. VOLATILITY-BASED FEATURES\n",
    "print(\"  - Volatility-based features\")\n",
    "# Use TRADABLE_TICKERS to ensure we only process assets with full options data where relevant\n",
    "for ticker in TRADABLE_TICKERS:\n",
    "    # Get relevant IVOL columns for the ticker\n",
    "    ivol_10d_col = first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D')\n",
    "    ivol_30d_col = first_col_containing(ticker, '30_Day_Call_Implied_Volatility_CALL_IMP_VOL_30D')\n",
    "    ivol_60d_col = first_col_containing(ticker, '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D')\n",
    "\n",
    "    # NEW ADVANCED FEATURE: Implied Volatility Term Structure Slope\n",
    "    if ivol_60d_col and ivol_10d_col:\n",
    "        feat[f'{ticker}_IVOL_Term_Structure_Slope'] = safe_series(ivol_60d_col) - safe_series(ivol_10d_col)\n",
    "\n",
    "    # NEW ADVANCED FEATURE: Implied Volatility Skew (Approximation)\n",
    "    # Using 1M Call 40D and 1M Put 50D as proxies for OTM Call and ATM Put\n",
    "    call_40d_ivol_col = first_col_containing(ticker, '1st_Month_Call_Imp_Vol_40_Delta_LIVE_1M_CALL_IMP_VOL_40DELTA_DFLT')\n",
    "    put_50d_ivol_col = first_col_containing(ticker, '1st_Month_Put_Imp_Vol_50_Delta_LIVE_1M_PUT_IMP_VOL_50DELTA_DFLT')\n",
    "    if call_40d_ivol_col and put_50d_ivol_col:\n",
    "        feat[f'{ticker}_IVOL_Skew_Approx'] = safe_series(put_50d_ivol_col) - safe_series(call_40d_ivol_col)\n",
    "\n",
    "\n",
    "    # Original IVOL-based features, adapted to new naming\n",
    "    # Loop through specific IVOL types for this ticker\n",
    "    specific_ivol_suffixes = ['IVOL_SIGMA', 'IVOL_DELTA', 'IVOL_MONEYNESS', 'CALL_IMP_VOL_30D', 'CALL_IMP_VOL_10D', 'CALL_IMP_VOL_60D', 'PUT_IMP_VOL_30D', 'PUT_IMP_VOL_10D', 'PUT_IMP_VOL_60D']\n",
    "    for ivol_suffix in specific_ivol_suffixes:\n",
    "        col_name = first_col_containing(ticker, ivol_suffix)\n",
    "        if col_name:\n",
    "            # Original slope logic (if relevant to this specific IVOL column and its '10D' counterpart)\n",
    "            # This logic might need further refinement based on exact suffix patterns (e.g., '60_Day_Call_...' vs '10_Day_Call_...')\n",
    "            if '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D' in col_name and \\\n",
    "               first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D'):\n",
    "                feat[f'{ticker}_IVOL_Call_Slope'] = safe_series(col_name) - \\\n",
    "                                                    safe_series(first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D'))\n",
    "\n",
    "            diff = safe_series(col_name).diff()\n",
    "            z = (diff - diff.rolling(30).mean()) / diff.rolling(30).std()\n",
    "            feat[col_name+'_shock'] = (z > 2).astype(int)\n",
    "\n",
    "            vol_col_name = first_col_containing(ticker, 'VOLUME') # Use helper to find generic VOLUME\n",
    "\n",
    "            if vol_col_name:\n",
    "                feat[col_name+'_div'] = safe_series(col_name) / safe_series(vol_col_name)\n",
    "            else:\n",
    "                feat[col_name+'_div'] = np.nan # Assign NaN if volume not found for division\n",
    "\n",
    "\n",
    "# 2. DERIV FLOW & SENTIMENT\n",
    "print(\"  - Deriv Flow & Sentiment features\")\n",
    "# Iterate through all_tickers (tradable + macro factors that might have these features)\n",
    "for ticker in all_tickers:\n",
    "    pc_col = first_col_containing(ticker, 'PUT_CALL_VOLUME_RATIO_CUR_DAY')\n",
    "    if pc_col:\n",
    "        feat[pc_col+'_ema5']=safe_series(pc_col).ewm(span=5,adjust=False).mean()\n",
    "\n",
    "    open_int_col = first_col_containing(ticker, 'OPEN_INT_TOTAL_CALL')\n",
    "    if open_int_col:\n",
    "        feat[open_int_col+'_chg3']=safe_series(open_int_col).pct_change(3)\n",
    "\n",
    "    # Only process real volume columns (not VWAP or other derived volumes)\n",
    "    volm_col = first_col_containing(ticker, 'Volume_-_Realtime_VOLUME')\n",
    "    if volm_col:\n",
    "        feat[volm_col+'_z']=(safe_series(volm_col)-safe_series(volm_col).rolling(30).mean())/safe_series(volm_col).rolling(30).std()\n",
    "\n",
    "# Generalize smart_money_flag: loop through dynamically identified all_tickers\n",
    "for ticker in all_tickers:\n",
    "    open_int_t_col = first_col_containing(ticker, 'OPEN_INT_TOTAL_CALL')\n",
    "    ivol_10d_col = first_col_containing(ticker, '10_Day_Call_Implied_Volatility_CALL_IMP_VOL_10D')\n",
    "\n",
    "    if open_int_t_col and ivol_10d_col:\n",
    "        feat[f'{ticker}_smart_money_flag'] = ((safe_series(open_int_t_col).pct_change() > 0) & \\\n",
    "                                              (safe_series(ivol_10d_col).pct_change() > 0)).astype(int)\n",
    "\n",
    "# 3. CROSS-ASSET CORRELATIONS\n",
    "print(\"  - Cross-Asset Correlations\")\n",
    "# Use the TRADABLE_TICKERS for robust pair selection, and also include macro factors if their PX_LAST is available\n",
    "# TRADABLE_TICKERS and potentially key macro factors (DXY, USGG10YR, SPX)\n",
    "correlation_universe = sorted(list(set(TRADABLE_TICKERS + [t for t in all_tickers if t in ['DXY Curncy', 'USGG10YR Index', 'SPX Index', 'CO1 Comdty', 'USGG2YR Index']])))\n",
    "\n",
    "# Form pairs dynamically or with a predefined list for efficiency (all-pairs can be very large)\n",
    "# Let's use a combination of predefined and dynamically generated pairs for flexibility\n",
    "dynamic_pairs = []\n",
    "# All tradable pairs (excluding self-correlation and duplicates)\n",
    "for i in range(len(TRADABLE_TICKERS)):\n",
    "    for j in range(i + 1, len(TRADABLE_TICKERS)):\n",
    "        dynamic_pairs.append((TRADABLE_TICKERS[i], TRADABLE_TICKERS[j]))\n",
    "\n",
    "# Add specific cross-asset pairs involving macro factors if desired\n",
    "dynamic_pairs.extend([\n",
    "    ('SPY US Equity', 'VIX Index'), # Already in tradable, but useful example\n",
    "    ('SPY US Equity', 'USGG10YR Index'),\n",
    "    ('SPY US Equity', 'DXY Curncy'),\n",
    "    ('SPY US Equity', 'CO1 Comdty'),\n",
    "    ('SPY US Equity', 'USGG2YR Index'), # New macro correlation\n",
    "    ('VIX Index', 'USGG10YR Index'),\n",
    "    ('VIX Index', 'DXY Curncy'),\n",
    "    ('VIX Index', 'CO1 Comdty'),\n",
    "])\n",
    "# Remove duplicates if any\n",
    "dynamic_pairs = list(set(dynamic_pairs))\n",
    "\n",
    "for t1,t2 in dynamic_pairs:\n",
    "    p1=first_col_containing(t1,'PX_LAST'); p2=first_col_containing(t2,'PX_LAST')\n",
    "    if p1 and p2:\n",
    "        s1=safe_series(p1); s2=safe_series(p2)\n",
    "        # Ensure series are not empty after safe_series and can be aligned\n",
    "        if not s1.empty and not s2.empty:\n",
    "            aligned_data = pd.DataFrame({'s1': s1, 's2': s2}).dropna()\n",
    "            if len(aligned_data) > 60: # Need enough data for rolling correlations\n",
    "                c20=aligned_data['s1'].rolling(20).corr(aligned_data['s2'])\n",
    "                c60=aligned_data['s1'].rolling(60).corr(aligned_data['s2'])\n",
    "                feat[f'{t1}_{t2}_c20']=c20\n",
    "                feat[f'{t1}_{t2}_c60']=c60\n",
    "                feat[f'{t1}_{t2}_cZ']=(c20-c20.rolling(60).mean())/c20.rolling(60).std()\n",
    "                feat[f'{t1}_{t2}_cDelta']=c20-c60\n",
    "\n",
    "                # NEW ADVANCED FEATURE: Rolling Beta\n",
    "                # Calculate daily returns for beta\n",
    "                ret1 = s1.pct_change().dropna()\n",
    "                ret2 = s2.pct_change().dropna()\n",
    "\n",
    "                # Align indices for beta calculation\n",
    "                aligned_returns = pd.DataFrame({'ret1': ret1, 'ret2': ret2}).dropna()\n",
    "\n",
    "                if not aligned_returns.empty and aligned_returns['ret2'].var() != 0:\n",
    "                    rolling_beta = aligned_returns['ret1'].rolling(window=60).cov(aligned_returns['ret2']) / \\\n",
    "                                   aligned_returns['ret2'].rolling(window=60).var()\n",
    "                    feat[f'{t1}_{t2}_rolling_beta'] = rolling_beta\n",
    "\n",
    "\n",
    "# 4. MACRO TRIGGERS (NOW INCLUDING NEW BLOOMBERG MACRO DATA)\n",
    "print(\"  - Macro Trigger features\")\n",
    "\n",
    "# Core Macro Index PX_LAST lookups (using exact names from your files)\n",
    "dxy_px=first_col_containing('DXY Curncy','PX_LAST')\n",
    "ust10_px=first_col_containing('USGG10YR Index','PX_LAST')\n",
    "spy_px=first_col_containing('SPY US Equity','PX_LAST')\n",
    "vix_px=first_col_containing('VIX Index','PX_LAST')\n",
    "\n",
    "feat['MPI']=(safe_series(dxy_px).pct_change().rolling(3).sum()+safe_series(ust10_px).pct_change().rolling(3).sum()).shift(0)\n",
    "feat['VIX_gt20']=(safe_series(vix_px)>20).astype(int)\n",
    "feat['DXY_rising']=(safe_series(dxy_px).pct_change()>0).astype(int)\n",
    "feat['SPY_below_MA20']=(safe_series(spy_px)<safe_series(spy_px).rolling(20).mean()).astype(int)\n",
    "feat['fear_overdrive']=((feat['VIX_gt20']==1)&(feat['DXY_rising']==1)&(feat['SPY_below_MA20']==1)).astype(int)\n",
    "\n",
    "xlk_px=first_col_containing('XLK US Equity','PX_LAST'); xle_px=first_col_containing('XLE US Equity','PX_LAST')\n",
    "feat['sector_rotation']=safe_series(xlk_px).pct_change(5)-safe_series(xle_px).pct_change(5)\n",
    "\n",
    "# --- NEW MACRO FEATURES from Macro_tickers_no_nan_cols.xlsx ---\n",
    "# Yield Curve: USGG10YR vs USGG2YR spread\n",
    "ust2_px = first_col_containing('USGG2YR Index', 'PX_LAST')\n",
    "if ust10_px and ust2_px:\n",
    "    feat['UST10Y_2Y_Spread'] = safe_series(ust10_px) - safe_series(ust2_px)\n",
    "    feat['UST10Y_2Y_Spread_chg'] = feat['UST10Y_2Y_Spread'].pct_change()\n",
    "\n",
    "# Inflation\n",
    "cpi_yoy_px = first_col_containing('CPI YOY Index', 'PX_LAST')\n",
    "cpi_chng_px = first_col_containing('CPI CHNG Index', 'PX_LAST')\n",
    "if cpi_yoy_px:\n",
    "    feat['CPI_YOY_mom3'] = safe_series(cpi_yoy_px).pct_change(3)\n",
    "    feat['CPI_YOY_z'] = (safe_series(cpi_yoy_px) - safe_series(cpi_yoy_px).rolling(12).mean()) / safe_series(cpi_yoy_px).rolling(12).std()\n",
    "if cpi_chng_px:\n",
    "    feat['CPI_CHNG_mom3'] = safe_series(cpi_chng_px).pct_change(3)\n",
    "\n",
    "# Employment Data\n",
    "injcjc_px = first_col_containing('INJCJC Index', 'PX_LAST')\n",
    "nfp_tch_px = first_col_containing('NFP TCH Index', 'PX_LAST')\n",
    "jobs_us_equity_px = first_col_containing('JOBS US Equity', 'PX_LAST') # Treated as macro factor\n",
    "if injcjc_px:\n",
    "    feat['INJCJC_shock'] = (safe_series(injcjc_px).diff() > safe_series(injcjc_px).diff().rolling(20).std() * 2).astype(int)\n",
    "if nfp_tch_px:\n",
    "    feat['NFP_TCH_mom3'] = safe_series(nfp_tch_px).pct_change(3)\n",
    "if jobs_us_equity_px:\n",
    "    feat['JOBS_US_Equity_mom3'] = safe_series(jobs_us_equity_px).pct_change(3)\n",
    "\n",
    "# Other Macro / Credit / Rates\n",
    "ffa_comdty_px = first_col_containing('FFA Comdty', 'PX_LAST')\n",
    "ctii10_govt_px = first_col_containing('CTII10 Govt', 'PX_LAST')\n",
    "ussw10_curncy_px = first_col_containing('USSW10 Curncy', 'PX_LAST')\n",
    "mlcx3crt_index_px = first_col_containing('MLCX3CRT Index', 'PX_LAST')\n",
    "farbast_index_px = first_col_containing('FARBAST Index', 'PX_LAST')\n",
    "bspgcpus_index_px = first_col_containing('BSPGCPUS Index', 'PX_LAST')\n",
    "spcsusa_index_px = first_col_containing('SPCSUSA Index', 'PX_LAST')\n",
    "spcs20sm_index_px = first_col_containing('SPCS20SM Index', 'PX_LAST')\n",
    "conssent_index_px = first_col_containing('CONSSENT Index', 'PX_LAST')\n",
    "lf94truu_index_vol30d = first_col_containing('LF94TRUU Index', 'VOLATILITY_30D')\n",
    "\n",
    "\n",
    "if ffa_comdty_px: feat['FFA_Spread'] = safe_series(ffa_comdty_px) - safe_series(ust2_px) # Example spread, needs ust2_px\n",
    "if ctii10_govt_px: feat['CTII10_mom'] = safe_series(ctii10_govt_px).pct_change()\n",
    "if ussw10_curncy_px: feat['USSW10_chg'] = safe_series(ussw10_curncy_px).pct_change()\n",
    "if mlcx3crt_index_px: feat['MLCX3CRT_chg'] = safe_series(mlcx3crt_index_px).pct_change()\n",
    "if farbast_index_px: feat['FARBAST_mom'] = safe_series(farbast_index_px).pct_change()\n",
    "if bspgcpus_index_px: feat['BSPGCPUS_mom'] = safe_series(bspgcpus_index_px).pct_change()\n",
    "if spcsusa_index_px: feat['SPCSUSA_mom'] = safe_series(spcsusa_index_px).pct_change()\n",
    "if spcs20sm_index_px: feat['SPCS20SM_mom'] = safe_series(spcs20sm_index_px).pct_change()\n",
    "if conssent_index_px: feat['CONSSENT_mom'] = safe_series(conssent_index_px).pct_change()\n",
    "if lf94truu_index_vol30d: feat['LF94TRUU_Vol_Signal'] = safe_series(lf94truu_index_vol30d) / safe_series(lf94truu_index_vol30d).rolling(60).mean()\n",
    "\n",
    "\n",
    "# 5. MOMENTUM / VOL FRACTALS\n",
    "print(\"  - Momentum/Vol Fractals\")\n",
    "for tk in all_tickers: # Loop through all prefixes/tickers\n",
    "    px_col=first_col_containing(tk,'PX_LAST')\n",
    "    if not px_col: continue\n",
    "    px=safe_series(px_col)\n",
    "\n",
    "    mom5=px.pct_change(5)\n",
    "    vol20=px.pct_change().rolling(20).std()\n",
    "    feat[tk+'_mom5_vol20']=mom5/vol20\n",
    "    ma=px.rolling(20).mean(); std=px.rolling(20).std()\n",
    "    feat[tk+'_pctB']=(px-(ma-2*std))/(4*std)\n",
    "\n",
    "    # NEW ADVANCED FEATURE: Fractional Differencing\n",
    "    # Applying fractional differencing to the primary price series of each ticker\n",
    "    if not px.empty and len(px) > 100: # Ensure enough data points for fractional differencing\n",
    "        try:\n",
    "            feat[f'{tk}_frac_diff_0_5'] = frac_diff(px, d=0.5, window=100) # Window for efficiency\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute fractional differencing for {tk}: {e}\")\n",
    "            feat[f'{tk}_frac_diff_0_5'] = np.nan # Assign NaN on error\n",
    "\n",
    "\n",
    "# 6. Shift +1 day and merge basic returns\n",
    "feat=feat.shift(1) # Shift all engineered features\n",
    "\n",
    "panel = feat.copy() # Start panel with shifted features\n",
    "\n",
    "# Prepare returns for evaluation based *only* on TRADABLE_TICKERS\n",
    "price_cols_for_returns = []\n",
    "for ticker_full_name in TRADABLE_TICKERS:\n",
    "    px_col_name = first_col_containing(ticker_full_name, 'PX_LAST')\n",
    "    if px_col_name:\n",
    "        price_cols_for_returns.append(px_col_name)\n",
    "    else:\n",
    "        print(f\"Warning: PX_LAST column not found for tradable ticker '{ticker_full_name}'. It will be excluded from return calculations.\")\n",
    "\n",
    "prices=raw[['Date']+price_cols_for_returns].copy(); prices.set_index('Date',inplace=True)\n",
    "returns={h:prices.pct_change(h).shift(-h) for h in [1,3,5,10,21]}\n",
    "\n",
    "\n",
    "# Merge tradable returns into panel\n",
    "for tk_full_name in TRADABLE_TICKERS:\n",
    "    px_col_name = first_col_containing(tk_full_name, 'PX_LAST')\n",
    "    if px_col_name and px_col_name in raw.columns: # Ensure the column exists in raw\n",
    "        # Calculate return and rename it consistently with the full ticker name\n",
    "        ret_series = raw[px_col_name].pct_change().shift(1).rename(f'{tk_full_name}_ret1')\n",
    "        panel = pd.concat([panel, ret_series], axis=1)\n",
    "\n",
    "panel.index=raw['Date']\n",
    "print('Engineered panel shape:',panel.shape)\n",
    "\n",
    "\n",
    "# Generate primitive signals\n",
    "print('\\nBuilding primitive signals from engineered panel â€¦')\n",
    "signals={}\n",
    "# --- NEW ADDITION (Phase 2, Item 2): Helper functions for inferring signal direction and type ---\n",
    "def get_primitive_direction(primitive_name):\n",
    "    \"\"\"Infers the directional bias (+1 for long, -1 for short, 0 for neutral) of a primitive signal.\"\"\"\n",
    "    if '>80' in primitive_name or 'z>1.5' in primitive_name or 'ma5>ma20' in primitive_name or 'rising' in primitive_name:\n",
    "        return 1 # Long bias\n",
    "    elif '<20' in primitive_name or 'z<-1.name' in primitive_name or 'ma5<ma20' in primitive_name or 'below_MA' in primitive_name:\n",
    "        return -1 # Short bias\n",
    "    elif 'shock' in primitive_name and '_shock' in primitive_name: # Shocks are generally neutral directionally on their own\n",
    "        return 0\n",
    "    elif 'slope' in primitive_name and '_slope' in primitive_name: # Slopes could be directional, but often neutral depending on context\n",
    "        return 0\n",
    "    elif 'Spread' in primitive_name and '_Spread' in primitive_name: # Spreads can be directional, depending on interpretation\n",
    "        return 0\n",
    "    elif 'Vol_Signal' in primitive_name: # Volatility signals are often neutral\n",
    "        return 0\n",
    "    return 0 # Default to neutral if no clear directional bias\n",
    "\n",
    "def get_primitive_signal_type(primitive_name):\n",
    "    \"\"\"Infers the broad category/type of a primitive signal based on its name.\"\"\"\n",
    "    # Order matters: more specific keywords first\n",
    "    if 'IVOL_Term_Structure_Slope' in primitive_name or 'IVOL_Skew_Approx' in primitive_name or 'IVOL' in primitive_name or 'VIX' in primitive_name or 'vol' in primitive_name or '_shock' in primitive_name:\n",
    "        return 'volatility'\n",
    "    elif 'mom' in primitive_name or 'pctB' in primitive_name or '_chg' in primitive_name or '_ret' in primitive_name or 'rising' in primitive_name:\n",
    "        return 'momentum'\n",
    "    elif '_c20' in primitive_name or '_c60' in primitive_name or '_cZ' in primitive_name or '_cDelta' in primitive_name or '_beta' in primitive_name:\n",
    "        return 'correlation'\n",
    "    elif 'DXY' in primitive_name or 'USGG' in primitive_name or 'MPI' in primitive_name or 'fear_overdrive' in primitive_name or 'CPI' in primitive_name or 'INJCJC' in primitive_name or 'NFP' in primitive_name or 'JOBS' in primitive_name or 'FFA' in primitive_name or 'CTII10' in primitive_name or 'USSW10' in primitive_name or 'MLCX3CRT' in primitive_name or 'FARBAST' in primitive_name or 'BSPGCPUS' in primitive_name or 'SPCSUSA' in primitive_name or 'SPCS20SM' in primitive_name or 'CONSSENT' in primitive_name or 'LF94TRUU' in primitive_name:\n",
    "        return 'macro'\n",
    "    elif 'PUT_CALL_VOLUME_RATIO' in primitive_name or 'smart_money_flag' in primitive_name or 'Short_Interest_Ratio' in primitive_name:\n",
    "        return 'sentiment'\n",
    "    elif 'VOLUME' in primitive_name:\n",
    "        return 'volume'\n",
    "    elif 'OPEN_INT' in primitive_name:\n",
    "        return 'open_interest'\n",
    "    elif 'frac_diff' in primitive_name:\n",
    "        return 'fractional_differencing' # New category for this advanced feature\n",
    "    return 'other'\n",
    "# --- END NEW ADDITION ---\n",
    "\n",
    "for col in panel.columns.drop('Date',errors='ignore'):\n",
    "    s=panel[col]\n",
    "\n",
    "    # Ensure the series is numeric before trying rank, mean, std\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        s_clean = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        if s_clean.empty or s_clean.std() == 0:\n",
    "            continue\n",
    "\n",
    "        rank=s_clean.rank(pct=True)\n",
    "        signals[col+'>80']=rank>0.8\n",
    "        signals[col+'<20']=rank<0.2\n",
    "\n",
    "        rolling_std_60 = s_clean.rolling(60).std()\n",
    "        valid_std_mask = rolling_std_60 > 1e-9\n",
    "\n",
    "        z = pd.Series(np.nan, index=s_clean.index, dtype=float)\n",
    "        z[valid_std_mask] = (s_clean - s_clean.rolling(60).mean())[valid_std_mask] / rolling_std_60[valid_std_mask]\n",
    "\n",
    "        signals[col+'_z>1.5']=z>1.5\n",
    "        signals[col+'_z<-1.5']=z<-1.5\n",
    "\n",
    "        ma5=s_clean.rolling(5).mean(); ma20=s_clean.rolling(20).mean()\n",
    "        signals[col+'_ma5>ma20']=ma5>ma20\n",
    "print('Total primitive signals:',len(signals))\n",
    "\n",
    "\n",
    "# --- MODIFICATION (Phase 2, Item 4): Relaxing 3-Signal Restriction & Initial Filtering ---\n",
    "# Replaced random sampling with combinatorial generation and support filter.\n",
    "primitive_names=list(signals.keys())\n",
    "all_candidate_setups = []\n",
    "setup_id_counter = 1\n",
    "\n",
    "print(f'\\nGenerating setups with lengths {SETUP_LENGTHS_TO_EXPLORE} and applying initial support filter (min_support={MIN_INITIAL_SUPPORT_FILTER}) â€¦')\n",
    "\n",
    "for k in SETUP_LENGTHS_TO_EXPLORE:\n",
    "    # Ensure there are enough primitive names for the current combination length\n",
    "    if k > len(primitive_names):\n",
    "        print(f\"Warning: Cannot generate setups of length {k} as only {len(primitive_names)} primitives are available. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for conds_tuple in itertools.combinations(primitive_names, k):\n",
    "        conds_list = list(conds_tuple)\n",
    "\n",
    "        # Calculate initial support for this combination\n",
    "        try:\n",
    "            # Ensure all signals for the combination actually exist\n",
    "            valid_combo_signals = [signals[c] for c in conds_list if c in signals]\n",
    "            if not valid_combo_signals: # Skip if no valid signals for this combination\n",
    "                continue\n",
    "\n",
    "            current_mask = functools.reduce(lambda a,b: a & b, valid_combo_signals)\n",
    "            current_support = current_mask.sum()\n",
    "        except KeyError:\n",
    "            current_support = 0 # If any signal is missing, support is 0\n",
    "\n",
    "        if current_support >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "            setup = {\n",
    "                'id': 'S' + str(setup_id_counter).zfill(4),\n",
    "                'conds': conds_list,\n",
    "                'setup_length': k,\n",
    "                'setup_type': 'concurrent', # Default for now, 'sequential' to be added later\n",
    "                'support': current_support # Store initial support\n",
    "            }\n",
    "            all_candidate_setups.append(setup)\n",
    "            setup_id_counter += 1\n",
    "\n",
    "setups = all_candidate_setups # Use this filtered list for the full evaluation loop\n",
    "print(f'Generated and filtered {len(setups)} candidate setups after initial support filter.')\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "\n",
    "# Prepare returns for evaluation (already updated in previous block)\n",
    "# price_cols_for_returns correctly defined from TRADABLE_TICKERS earlier\n",
    "# prices and returns dictionaries are already based on price_cols_for_returns\n",
    "# so this section remains as is.\n",
    "\n",
    "summary_rows=[]\n",
    "trigger_records=[]\n",
    "\n",
    "for setup in setups:\n",
    "    sid=setup['id']; conds=setup['conds']\n",
    "\n",
    "    valid_signals = [signals[c] for c in conds if c in signals]\n",
    "    if not valid_signals:\n",
    "        continue\n",
    "\n",
    "    mask=functools.reduce(lambda a,b: a & b, valid_signals)\n",
    "    dates=mask[mask].index\n",
    "    support=len(dates)\n",
    "\n",
    "    # We already filtered by MIN_INITIAL_SUPPORT_FILTER earlier, but keep this for sanity\n",
    "    if support<5: # Can set this to MIN_INITIAL_SUPPORT_FILTER if desired for final check\n",
    "        continue\n",
    "\n",
    "    # --- NEW ADDITION (Phase 2, Item 2): Infer Directional Labels and Type ---\n",
    "    direction_score = 0\n",
    "    signal_type_counts = {}\n",
    "    for cond in conds:\n",
    "        direction_score += get_primitive_direction(cond)\n",
    "        s_type = get_primitive_signal_type(cond)\n",
    "        signal_type_counts[s_type] = signal_type_counts.get(s_type, 0) + 1\n",
    "\n",
    "    entry_direction = 'mixed'\n",
    "    if direction_score > 0: # Threshold for 'long' bias (e.g., >0 indicates more long signals)\n",
    "        entry_direction = 'long'\n",
    "    elif direction_score < 0: # Threshold for 'short' bias\n",
    "        entry_direction = 'short'\n",
    "\n",
    "    dominant_signal_type = 'unknown'\n",
    "    if signal_type_counts:\n",
    "        dominant_signal_type = max(signal_type_counts, key=signal_type_counts.get)\n",
    "\n",
    "    # --- NEW ADDITION (Phase 2, Item 5): Basic Setup Lifecycle Tracking fields ---\n",
    "    first_trigger_date = dates.min() if not dates.empty else pd.NaT\n",
    "    last_trigger_date = dates.max() if not dates.empty else pd.NaT\n",
    "\n",
    "    perf={'setup_id':sid,'feature_conditions':'+'.join(conds),'support':support,\n",
    "          'entry_direction': entry_direction, # NEW\n",
    "          'dominant_signal_type': dominant_signal_type, # NEW\n",
    "          'first_trigger_date': first_trigger_date, # NEW\n",
    "          'last_trigger_date': last_trigger_date # NEW\n",
    "         }\n",
    "    # --- END NEW ADDITION ---\n",
    "\n",
    "    for h,label in zip([3,5,10,21],['accuracy_3d','avg_return_5d','sharpe_10d','hit_rate_21d']):\n",
    "        r=returns[h].loc[dates]\n",
    "\n",
    "        if r.empty:\n",
    "            perf[label] = 0.0\n",
    "            continue\n",
    "\n",
    "        mean=r.mean(axis=1) # The mean return across all TRADABLE assets for each trigger date.\n",
    "\n",
    "        mean = mean.dropna()\n",
    "        if mean.empty:\n",
    "            perf[label] = 0.0\n",
    "            continue\n",
    "\n",
    "        dir_correct=(mean>0).mean()\n",
    "        if 'accuracy' in label:\n",
    "            perf[label]=dir_correct\n",
    "        elif 'avg' in label:\n",
    "            perf[label]=mean.mean()\n",
    "        elif 'sharpe' in label:\n",
    "            bootstrap_block_size = min(h if h > 1 else 2, max(1, len(mean) // 2))\n",
    "\n",
    "            # --- MODIFICATION (Phase 1, Item 1.2): Refined Sharpe Threshold ---\n",
    "            # Penalize setups with extremely low standard deviation of mean returns\n",
    "            if mean.std() > 0.0001 and len(mean) >= 2: # Increased threshold to 0.01% daily std dev\n",
    "                median_sharpe, _, _ = block_bootstrap_sharpe(mean, block_size=bootstrap_block_size)\n",
    "                perf[label] = median_sharpe\n",
    "            else:\n",
    "                perf[label] = 0.0 # Assign 0 if std dev is too small (meaning unrealistically stable) or not enough data\n",
    "            # --- END MODIFICATION ---\n",
    "        elif 'hit' in label:\n",
    "            perf[label]=(r>0).stack().mean()\n",
    "    summary_rows.append(perf)\n",
    "\n",
    "    # Log individual ticker returns and NEW option PnL for triggered dates - ONLY TRADABLE TICKERS\n",
    "    for d_idx, d in enumerate(dates): # Iterate with index to align with future price lookup\n",
    "        for tk_col_full_name in price_cols_for_returns:\n",
    "            tk_symbol_for_log = tk_col_full_name.split('_PX_LAST')[0].split('_Last_Price')[0]\n",
    "\n",
    "            # Get current price and IVOL for option simulation\n",
    "            current_px = safe_series(tk_col_full_name).loc[d]\n",
    "            # Try to get 10-day call IVOL, or 30-day, or default to some other IVOL.\n",
    "            # Use the most appropriate IVOL for the OPTION_SIM_HORIZON_DAYS\n",
    "            ivol_col_for_sim = first_col_containing(tk_symbol_for_log, f'{OPTION_SIM_HORIZON_DAYS}_Day_Call_Implied_Volatility_CALL_IMP_VOL_{OPTION_SIM_HORIZON_DAYS}D')\n",
    "            if not ivol_col_for_sim: # Fallback to 30D or 60D if 10D not found\n",
    "                ivol_col_for_sim = first_col_containing(tk_symbol_for_log, '30_Day_Call_Implied_Volatility_CALL_IMP_VOL_30D')\n",
    "            if not ivol_col_for_sim:\n",
    "                ivol_col_for_sim = first_col_containing(tk_symbol_for_log, '60_Day_Call_Implied_Volatility_CALL_IMP_VOL_60D')\n",
    "\n",
    "            ivol_at_entry = safe_series(ivol_col_for_sim).loc[d] if ivol_col_for_sim else np.nan\n",
    "\n",
    "            option_pnl_10d = np.nan # Default to NaN\n",
    "            # Ensure we have valid current price and IVOL before attempting option simulation\n",
    "            if not pd.isna(current_px) and not pd.isna(ivol_at_entry):\n",
    "                # Find future price for option simulation horizon\n",
    "                future_date_for_sim = d + pd.Timedelta(days=OPTION_SIM_HORIZON_DAYS)\n",
    "                # Find the actual row in raw for future_date_for_sim\n",
    "                future_px_row = raw[raw['Date'] == future_date_for_sim]\n",
    "\n",
    "                if not future_px_row.empty and tk_col_full_name in future_px_row.columns:\n",
    "                    future_px_for_sim = future_px_row[tk_col_full_name].iloc[0]\n",
    "                    # --- NEW ADDITION (Phase 2, Item 3): Simulate Option PnL ---\n",
    "                    option_pnl_10d = simulate_option_pnl(current_px, future_px_for_sim, ivol_at_entry, OPTION_SIM_HORIZON_DAYS, entry_direction)\n",
    "                    # --- END NEW ADDITION ---\n",
    "\n",
    "            ret_vals={h:returns[h].loc[d,tk_col_full_name] if tk_col_full_name in returns[h].columns else np.nan for h in [1,3,5,10,21]}\n",
    "            trigger_records.append({'date':d,'ticker':tk_symbol_for_log,'setup_id':sid,'matched':1,\n",
    "                                    'return_1d':ret_vals[1],'return_3d':ret_vals[3],'return_5d':ret_vals[5],\n",
    "                                    'return_10d':ret_vals[10],'return_21d':ret_vals[21],\n",
    "                                    'option_pnl_10d': option_pnl_10d}) # NEW: Option PnL\n",
    "\n",
    "summary_df=pd.DataFrame(summary_rows)\n",
    "\n",
    "# --- NEW ADDITION (Phase 2, Item 5): Basic Setup Lifecycle Tracking Derived Metrics ---\n",
    "if not summary_df.empty:\n",
    "    summary_df['first_trigger_date'] = pd.to_datetime(summary_df['first_trigger_date'])\n",
    "    summary_df['last_trigger_date'] = pd.to_datetime(summary_df['last_trigger_date'])\n",
    "\n",
    "    # Calculate duration a setup was \"active\" from its first to last trigger\n",
    "    summary_df['setup_duration_days'] = (summary_df['last_trigger_date'] - summary_df['first_trigger_date']).dt.days\n",
    "\n",
    "    # Calculate frequency within its active duration, or against total data span\n",
    "    # Replace 0 duration with NaN to avoid division by zero\n",
    "    summary_df['avg_trigger_frequency_per_day'] = summary_df['support'] / summary_df['setup_duration_days'].replace(0, np.nan)\n",
    "    # Replace inf with NaN for single-day triggers where duration is 0\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "else:\n",
    "    print(\"Summary DataFrame is empty, skipping lifecycle metrics calculation.\")\n",
    "# --- END NEW ADDITION ---\n",
    "\n",
    "top = summary_df[summary_df['sharpe_10d'] != 0.0].sort_values('sharpe_10d',ascending=False).head(20)\n",
    "\n",
    "summary_df.to_csv('setup_results_summary.csv',index=False)\n",
    "trigger_df=pd.DataFrame(trigger_records) # Ensure trigger_df is created from the collected records\n",
    "trigger_df.to_csv('setup_trigger_log.csv',index=False)\n",
    "top.to_json('top_setups.json',orient='records',indent=2)\n",
    "\n",
    "print('\\nDiscovery complete with engineered features')\n",
    "print(top.head())"
   ],
   "id": "125fbebc62a2fe63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw workbooks â€¦\n",
      "An unexpected error occurred during Excel loading of 'Macro_tickers_no_nan_cols.xlsx': 'Date'\n",
      "Raw shape: (1173, 404)\n",
      "Example columns after loading:\n",
      "['Date', 'QQQ US Equity_Last_Price_PX_LAST', 'QQQ US Equity_Open_Price_PX_OPEN', 'QQQ US Equity_High_Price_PX_HIGH', 'QQQ US Equity_Low_Price_PX_LOW']\n",
      "\n",
      "Identified all relevant prefixes/tickers for feature engineering: ['AAPL US Equity', 'ARKK US Equity', 'CO1 Comdty', 'DXY Curncy', 'GLD US Equity', 'LLY US Equity', 'NBIS US Equity', 'NVDA US Equity', 'QQQ US Equity', 'SPY US Equity', 'TSLA US Equity', 'USGG10YR Index', 'VIX Index', 'XLE US Equity', 'XLF US Equity', 'XLK US Equity']\n",
      "Actual tradable tickers for returns: ['QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity', 'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity', 'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity', 'NVDA US Equity']\n",
      "\n",
      "Engineering custom features â€¦\n",
      "  - Volatility-based features\n",
      "  - Deriv Flow & Sentiment features\n",
      "  - Cross-Asset Correlations\n",
      "  - Macro Trigger features\n",
      "  - Momentum/Vol Fractals\n",
      "Engineered panel shape: (1173, 814)\n",
      "\n",
      "Building primitive signals from engineered panel â€¦\n",
      "Total primitive signals: 4065\n",
      "\n",
      "Generating setups with lengths [2, 3, 4] and applying initial support filter (min_support=10) â€¦\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 613\u001B[39m\n\u001B[32m    610\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_combo_signals: \u001B[38;5;66;03m# Skip if no valid signals for this combination\u001B[39;00m\n\u001B[32m    611\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     current_mask = \u001B[43mfunctools\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43mb\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[43m&\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_combo_signals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    614\u001B[39m     current_support = current_mask.sum()\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 613\u001B[39m, in \u001B[36m<lambda>\u001B[39m\u001B[34m(a, b)\u001B[39m\n\u001B[32m    610\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_combo_signals: \u001B[38;5;66;03m# Skip if no valid signals for this combination\u001B[39;00m\n\u001B[32m    611\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     current_mask = functools.reduce(\u001B[38;5;28;01mlambda\u001B[39;00m a,b: \u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[43m&\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m, valid_combo_signals)\n\u001B[32m    614\u001B[39m     current_support = current_mask.sum()\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/alpha_discoveryV2/.venv/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001B[39m, in \u001B[36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[39m\u001B[34m(self, other)\u001B[39m\n\u001B[32m     72\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[32m     74\u001B[39m other = item_from_zerodim(other)\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/alpha_discoveryV2/.venv/lib/python3.13/site-packages/pandas/core/arraylike.py:70\u001B[39m, in \u001B[36mOpsMixin.__and__\u001B[39m\u001B[34m(self, other)\u001B[39m\n\u001B[32m     68\u001B[39m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m__and__\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__and__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[32m---> \u001B[39m\u001B[32m70\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_logical_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mand_\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/alpha_discoveryV2/.venv/lib/python3.13/site-packages/pandas/core/series.py:6142\u001B[39m, in \u001B[36mSeries._logical_method\u001B[39m\u001B[34m(self, other, op)\u001B[39m\n\u001B[32m   6139\u001B[39m rvalues = extract_array(other, extract_numpy=\u001B[38;5;28;01mTrue\u001B[39;00m, extract_range=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m   6141\u001B[39m res_values = ops.logical_op(lvalues, rvalues, op)\n\u001B[32m-> \u001B[39m\u001B[32m6142\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_construct_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mres_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/alpha_discoveryV2/.venv/lib/python3.13/site-packages/pandas/core/series.py:6212\u001B[39m, in \u001B[36mSeries._construct_result\u001B[39m\u001B[34m(self, result, name)\u001B[39m\n\u001B[32m   6209\u001B[39m     out = this._construct_result(result, name)\n\u001B[32m   6210\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Series, out)\n\u001B[32m-> \u001B[39m\u001B[32m6212\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_construct_result\u001B[39m(\n\u001B[32m   6213\u001B[39m     \u001B[38;5;28mself\u001B[39m, result: ArrayLike | \u001B[38;5;28mtuple\u001B[39m[ArrayLike, ArrayLike], name: Hashable\n\u001B[32m   6214\u001B[39m ) -> Series | \u001B[38;5;28mtuple\u001B[39m[Series, Series]:\n\u001B[32m   6215\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   6216\u001B[39m \u001B[33;03m    Construct an appropriately-labelled Series from the result of an op.\u001B[39;00m\n\u001B[32m   6217\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   6226\u001B[39m \u001B[33;03m        In the case of __divmod__ or __rdivmod__, a 2-tuple of Series.\u001B[39;00m\n\u001B[32m   6227\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   6228\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m   6229\u001B[39m         \u001B[38;5;66;03m# produced by divmod or rdivmod\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
