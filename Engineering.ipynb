{
 "cells": [
  {
   "cell_type": "code",
   "id": "778b18d7a9ed708f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T00:05:11.626929Z",
     "start_time": "2025-08-03T00:04:46.813093Z"
    }
   },
   "source": [
    "# Feature Engineering + Discovery Engine with FULLY Migrated Structured Features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import linregress\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "# Define the explicit list of tradable tickers\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "]\n",
    "\n",
    "# --- ADDITION: Define list of macro tickers to ensure their inclusion in feature generation ---\n",
    "MACRO_TICKERS = [\n",
    "    'DXY Curncy', 'USGG10YR Index', 'USGG2YR Index', 'CPI YOY Index',\n",
    "    'INJCJC Index', 'FFA Comdty', 'LF94TRUU Index', 'CPI CHNG Index',\n",
    "    'NFP TCH Index', 'JOBS US Equity', 'CTII10 Govt', 'USSW10 Curncy',\n",
    "    'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index',\n",
    "    'SPCS20SM Index', 'CONSSENT Index'\n",
    "]\n",
    "\n",
    "# Define the file paths\n",
    "MAIN_DATA_FILE = 'All_Tickers copy.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_no_nan_cols.xlsx'\n",
    "\n",
    "# Setup Generation Configuration\n",
    "NUM_RANDOM_SETUPS_TO_SAMPLE = 100\n",
    "SETUP_LENGTHS_TO_EXPLORE = [2]\n",
    "MIN_INITIAL_SUPPORT_FILTER = 5\n",
    "\n",
    "# Option Simulation Configuration\n",
    "OPTION_SIM_HORIZONS_DAYS = [3, 10, 21]\n",
    "RISK_FREE_RATE = 0.01\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooksâ€¦')\n",
    "\n",
    "\n",
    "# --- Custom Data Loading Function (Unchanged) ---\n",
    "def load_and_merge_excel(file_path, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "        for sh_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name)\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found.\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df\n",
    "\n",
    "\n",
    "# Load main and macro data\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE)\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "# Final cleaning and indexing\n",
    "if not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "    if 'Date' in raw.columns:\n",
    "        raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "        raw = raw.drop_duplicates(subset=['Date'], keep='last')\n",
    "        raw.set_index('Date', inplace=True)\n",
    "        raw.index = pd.to_datetime(raw.index)\n",
    "        raw.sort_index(inplace=True)\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "\n",
    "# --- Dynamic Ticker Identification (Unchanged) ---\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c])))\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total', '30', '10',\n",
    "                           '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC', 'NFP', 'JOBS', 'CPI', 'CTII10',\n",
    "                           'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT', 'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM',\n",
    "                           'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "# --- FIX: Ensure all macro tickers are included in the feature engineering universe ---\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes + MACRO_TICKERS)))\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {len(all_tickers)}')\n",
    "\n",
    "\n",
    "# --- Helper functions (Unchanged) ---\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    \"\"\"Finds the first column name in raw that matches the pattern 'ticker_full_name_substr'.\"\"\"\n",
    "    if substr == 'PX_LAST':\n",
    "        for potential_col in [f\"{ticker_full_name}_Last_Price_PX_LAST\", f\"{ticker_full_name}_PX_LAST\"]:\n",
    "            if potential_col in raw.columns: return potential_col\n",
    "    for c in raw.columns:\n",
    "        if c.startswith(ticker_full_name) and substr in c: return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def safe_series(col_name):\n",
    "    \"\"\"Returns a column as a Series, or an empty Series if column does not exist.\"\"\"\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "\n",
    "def frac_diff(series, d=0.5, window=100):\n",
    "    \"\"\"Computes fractionally differenced series.\"\"\"\n",
    "    weights = [1.]\n",
    "    for k in range(1, len(series)): weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1])\n",
    "    output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(window, len(series)):\n",
    "        subset = series.iloc[i - window + 1: i + 1]\n",
    "        if len(subset) == len(weights[-window:]):\n",
    "            output.iloc[i] = np.dot(weights[-window:], subset)\n",
    "    return output.dropna()\n",
    "\n",
    "\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=1000, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"Calculates the Sharpe Ratio using block bootstrapping.\"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2: return 0.0, 0.0, 0.0\n",
    "    blocks = [returns_series.iloc[i: i + block_size] for i in range(0, len(returns_series), block_size) if\n",
    "              not returns_series.iloc[i: i + block_size].empty]\n",
    "    if not blocks: return 0.0, 0.0, 0.0\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size))\n",
    "    sharpes = []\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = [blocks[i] for i in np.random.choice(len(blocks), n_blocks_to_sample, replace=True)]\n",
    "        resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)]\n",
    "        if resampled_returns.std() > 1e-9:\n",
    "            sharpes.append((resampled_returns.mean() / resampled_returns.std()) * (\n",
    "                np.sqrt(trading_days_per_year) if annualize else 1))\n",
    "        else:\n",
    "            sharpes.append(0.0)\n",
    "    if not sharpes: return 0.0, 0.0, 0.0\n",
    "    return np.median(sharpes), np.percentile(sharpes, 5), np.percentile(sharpes, 95)\n",
    "\n",
    "\n",
    "# --- Option Simulation Helpers (UPDATED) ---\n",
    "def estimate_atm_premium(price, ivol, days, option_type):\n",
    "    \"\"\"Placeholder for a real option pricing model like Black-Scholes.\"\"\"\n",
    "    T = days / 365.25\n",
    "    if T <= 0 or price <= 0 or ivol <= 0: return 0\n",
    "    # This is a highly simplified placeholder. A real model (e.g., scipy.stats.norm.cdf) would be needed for accuracy.\n",
    "    return 0.4 * price * ivol * np.sqrt(T)\n",
    "\n",
    "\n",
    "def simulate_option_pnl_detailed(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    \"\"\"Simulates PnL and returns a dictionary with all components for verification.\"\"\"\n",
    "    # Return dictionary with NaNs if inputs are invalid\n",
    "    nan_result = {\n",
    "        'pnl_per_share': np.nan, 'option_type': None, 'strike_price': np.nan,\n",
    "        'underlying_price_at_exit': np.nan, 'entry_premium': np.nan,\n",
    "        'exit_value': np.nan, 'pnl_dollars': np.nan, 'skipped_reason': 'None'\n",
    "    }\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Price'\n",
    "        return nan_result\n",
    "    if pd.isna(ivol_at_entry) or ivol_at_entry <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid IVOL'\n",
    "        return nan_result\n",
    "    if pd.isna(future_price):\n",
    "        nan_result['skipped_reason'] = 'Missing Future Price'\n",
    "        return nan_result\n",
    "    if entry_direction not in ['long', 'short']:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Direction'\n",
    "        return nan_result\n",
    "\n",
    "    scaled_ivol = ivol_at_entry / 100.0 if ivol_at_entry > 1.0 else ivol_at_entry\n",
    "    strike_price = current_price\n",
    "    option_type = 'call' if entry_direction == 'long' else 'put'\n",
    "\n",
    "    # Calculate mechanics\n",
    "    entry_premium = estimate_atm_premium(current_price, scaled_ivol, horizon_days, option_type)\n",
    "    if option_type == 'call':\n",
    "        exit_value = max(future_price - strike_price, 0)\n",
    "    else:  # put\n",
    "        exit_value = max(strike_price - future_price, 0)\n",
    "\n",
    "    pnl_per_share = exit_value - entry_premium\n",
    "    pnl_dollars = pnl_per_share * 100  # Assuming 1 contract = 100 shares\n",
    "\n",
    "    return {\n",
    "        'pnl_per_share': pnl_per_share,\n",
    "        'option_type': option_type,\n",
    "        'strike_price': strike_price,\n",
    "        'underlying_price_at_exit': future_price,\n",
    "        'entry_premium': entry_premium,\n",
    "        'exit_value': exit_value,\n",
    "        'pnl_dollars': pnl_dollars,\n",
    "        'skipped_reason': 'None'\n",
    "    }\n",
    "\n",
    "\n",
    "# --- ARCHITECTURE REWRITE: Structured Feature & Signal Generation ---\n",
    "\n",
    "# --- 1. Define Feature Specifications (Complete Migration) ---\n",
    "print('\\n--- Defining ALL Feature Specifications ---')\n",
    "feature_specs = []\n",
    "\n",
    "# Volatility Features\n",
    "# UPDATED: Iterate over all_tickers for feature creation\n",
    "for ticker in all_tickers:\n",
    "    feature_specs.append({'type': 'ivol_term_structure', 'assets': [ticker], 'unique_id': f'ivol_slope_{ticker}',\n",
    "                          'display_name': f'IVOL Slope({ticker})'})\n",
    "    feature_specs.append({'type': 'ivol_skew', 'assets': [ticker], 'unique_id': f'ivol_skew_{ticker}',\n",
    "                          'display_name': f'IVOL Skew({ticker})'})\n",
    "    for suffix in ['IVOL_SIGMA', 'CALL_IMP_VOL_30D', 'PUT_IMP_VOL_30D']:\n",
    "        feature_specs.append({'type': 'ivol_shock', 'assets': [ticker], 'params': {'ivol_suffix': suffix},\n",
    "                              'unique_id': f'ivol_shock_{ticker}_{suffix}',\n",
    "                              'display_name': f'IVOL Shock({ticker},{suffix})'})\n",
    "        feature_specs.append({'type': 'ivol_div_volume', 'assets': [ticker], 'params': {'ivol_suffix': suffix},\n",
    "                              'unique_id': f'ivol_div_vol_{ticker}_{suffix}',\n",
    "                              'display_name': f'IVOL/Vol({ticker},{suffix})'})\n",
    "\n",
    "# Deriv Flow & Sentiment Features\n",
    "for ticker in all_tickers:\n",
    "    feature_specs.append({'type': 'put_call_ratio_ema', 'assets': [ticker], 'params': {'span': 5},\n",
    "                          'unique_id': f'pc_ratio_ema5_{ticker}', 'display_name': f'PC Ratio EMA5({ticker})'})\n",
    "    feature_specs.append(\n",
    "        {'type': 'open_interest_change', 'assets': [ticker], 'params': {'days': 3}, 'unique_id': f'oi_chg3_{ticker}',\n",
    "         'display_name': f'OI Chg3d({ticker})'})\n",
    "    feature_specs.append(\n",
    "        {'type': 'volume_zscore', 'assets': [ticker], 'params': {'window': 30}, 'unique_id': f'vol_z30_{ticker}',\n",
    "         'display_name': f'Vol Z30d({ticker})'})\n",
    "    feature_specs.append({'type': 'smart_money_flag', 'assets': [ticker], 'unique_id': f'smart_money_{ticker}',\n",
    "                          'display_name': f'Smart Money({ticker})'})\n",
    "\n",
    "# Cross-Asset Correlation Features\n",
    "correlation_pairs = list(set(itertools.combinations(all_tickers, 2)))\n",
    "for t1, t2 in correlation_pairs:\n",
    "    for window in [20, 60]:\n",
    "        feature_specs.append({'type': 'correlation', 'assets': [t1, t2], 'params': {'window': window},\n",
    "                              'unique_id': f'corr_{t1}_{t2}_{window}d', 'display_name': f'Corr({t1},{t2},{window}d)'})\n",
    "    feature_specs.append({'type': 'correlation_zscore', 'assets': [t1, t2], 'unique_id': f'corr_z_{t1}_{t2}',\n",
    "                          'display_name': f'Corr Z({t1},{t2})'})\n",
    "    feature_specs.append({'type': 'correlation_delta', 'assets': [t1, t2], 'unique_id': f'corr_delta_{t1}_{t2}',\n",
    "                          'display_name': f'Corr Delta({t1},{t2})'})\n",
    "    feature_specs.append(\n",
    "        {'type': 'rolling_beta', 'assets': [t1, t2], 'params': {'window': 60}, 'unique_id': f'beta_{t1}_{t2}_60d',\n",
    "         'display_name': f'Beta({t1},{t2},60d)'})\n",
    "\n",
    "# Macro Features (Original & New)\n",
    "feature_specs.extend([\n",
    "    {'type': 'macro_mpi', 'assets': ['DXY Curncy', 'USGG10YR Index'], 'unique_id': 'macro_mpi',\n",
    "     'display_name': 'Macro Pressure Index'},\n",
    "    {'type': 'macro_fear_overdrive', 'assets': ['VIX Index', 'DXY Curncy', 'SPY US Equity'],\n",
    "     'unique_id': 'macro_fear_overdrive', 'display_name': 'Fear Overdrive'},\n",
    "    {'type': 'macro_sector_rotation', 'assets': ['XLK US Equity', 'XLE US Equity'],\n",
    "     'unique_id': 'macro_xlk_xle_rotation', 'display_name': 'Sector Rotation (XLK-XLE)'},\n",
    "    {'type': 'macro_yield_spread', 'assets': ['USGG10YR Index', 'USGG2YR Index'], 'unique_id': 'macro_10y2y_spread',\n",
    "     'display_name': 'Yield Spread (10Y-2Y)'},\n",
    "    {'type': 'macro_cpi_zscore', 'assets': ['CPI YOY Index'], 'unique_id': 'macro_cpi_z',\n",
    "     'display_name': 'CPI Z-Score'},\n",
    "    {'type': 'macro_injcjc_shock', 'assets': ['INJCJC Index'], 'unique_id': 'macro_jobless_claims_shock',\n",
    "     'display_name': 'Jobless Claims Shock'},\n",
    "    {'type': 'macro_ffa_spread', 'assets': ['FFA Comdty', 'USGG2YR Index'], 'unique_id': 'macro_ffa_spread',\n",
    "     'display_name': 'Fed Funds Spread'},\n",
    "    {'type': 'macro_lf94truu_vol_signal', 'assets': ['LF94TRUU Index'], 'unique_id': 'macro_hyg_vol_signal',\n",
    "     'display_name': 'HYG Vol Signal'}\n",
    "])\n",
    "for t in ['CPI YOY Index', 'CPI CHNG Index', 'NFP TCH Index', 'JOBS US Equity']:\n",
    "    feature_specs.append(\n",
    "        {'type': 'macro_generic_mom', 'assets': [t], 'params': {'days': 3}, 'unique_id': f'macro_mom3_{t}',\n",
    "         'display_name': f'Macro Mom3d({t})'})\n",
    "for t in ['CTII10 Govt', 'USSW10 Curncy', 'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index',\n",
    "          'SPCS20SM Index', 'CONSSENT Index']:\n",
    "    feature_specs.append(\n",
    "        {'type': 'macro_generic_chg', 'assets': [t], 'unique_id': f'macro_chg_{t}', 'display_name': f'Macro Chg({t})'})\n",
    "\n",
    "# Momentum / Volatility Fractal Features\n",
    "for ticker in all_tickers:\n",
    "    feature_specs.append({'type': 'mom_div_vol', 'assets': [ticker], 'unique_id': f'mom_div_vol_{ticker}',\n",
    "                          'display_name': f'Mom/Vol({ticker})'})\n",
    "    feature_specs.append(\n",
    "        {'type': 'bollinger_pctB', 'assets': [ticker], 'params': {'window': 20}, 'unique_id': f'pctB_{ticker}',\n",
    "         'display_name': f'%B({ticker})'})\n",
    "    feature_specs.append({'type': 'fractional_differencing', 'assets': [ticker], 'params': {'d': 0.5, 'window': 100},\n",
    "                          'unique_id': f'frac_diff_{ticker}_0.5', 'display_name': f'FracDiff({ticker},d=0.5)'})\n",
    "\n",
    "print(f\"Defined {len(feature_specs)} total feature specifications.\")\n",
    "\n",
    "# --- 2. Calculate Features Based on Specifications (Complete Calculation Engine) ---\n",
    "print('--- Calculating All Features ---')\n",
    "feat = pd.DataFrame(index=raw.index)\n",
    "for spec in feature_specs:\n",
    "    feature_id = spec['unique_id']\n",
    "    try:\n",
    "        # VOLATILITY\n",
    "        if spec['type'] == 'ivol_term_structure':\n",
    "            ivol60 = safe_series(first_col_containing(spec['assets'][0], '60_Day_Call_Implied_Volatility'))\n",
    "            ivol10 = safe_series(first_col_containing(spec['assets'][0], '10_Day_Call_Implied_Volatility'))\n",
    "            if not ivol60.empty and not ivol10.empty: feat[feature_id] = ivol60 - ivol10\n",
    "        elif spec['type'] == 'ivol_skew':\n",
    "            put50 = safe_series(first_col_containing(spec['assets'][0], '1st_Month_Put_Imp_Vol_50_Delta'))\n",
    "            call40 = safe_series(first_col_containing(spec['assets'][0], '1st_Month_Call_Imp_Vol_40_Delta'))\n",
    "            if not put50.empty and not call40.empty: feat[feature_id] = put50 - call40\n",
    "        elif spec['type'] == 'ivol_shock':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix']))\n",
    "            if not ivol_s.empty: feat[feature_id] = (ivol_s.diff() - ivol_s.diff().rolling(\n",
    "                30).mean()) / ivol_s.diff().rolling(30).std()\n",
    "        elif spec['type'] == 'ivol_div_volume':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix']))\n",
    "            vol_s = safe_series(first_col_containing(spec['assets'][0], 'VOLUME'))\n",
    "            if not ivol_s.empty and not vol_s.empty: feat[feature_id] = ivol_s / vol_s.replace(0, np.nan)\n",
    "        # SENTIMENT\n",
    "        elif spec['type'] == 'put_call_ratio_ema':\n",
    "            pc = safe_series(first_col_containing(spec['assets'][0], 'PUT_CALL_VOLUME_RATIO_CUR_DAY'))\n",
    "            if not pc.empty: feat[feature_id] = pc.ewm(span=spec['params']['span'], adjust=False).mean()\n",
    "        elif spec['type'] == 'open_interest_change':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], 'OPEN_INT_TOTAL_CALL'))\n",
    "            if not oi.empty: feat[feature_id] = oi.pct_change(spec['params']['days'])\n",
    "        elif spec['type'] == 'volume_zscore':\n",
    "            vol = safe_series(first_col_containing(spec['assets'][0], 'Volume_-_Realtime_VOLUME'))\n",
    "            if not vol.empty: feat[feature_id] = (vol - vol.rolling(spec['params']['window']).mean()) / vol.rolling(\n",
    "                spec['params']['window']).std()\n",
    "        elif spec['type'] == 'smart_money_flag':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], 'OPEN_INT_TOTAL_CALL')).pct_change() > 0\n",
    "            ivol = safe_series(\n",
    "                first_col_containing(spec['assets'][0], '10_Day_Call_Implied_Volatility')).pct_change() > 0\n",
    "            if not oi.empty and not ivol.empty: feat[feature_id] = (oi & ivol).astype(int)\n",
    "        # CORRELATION\n",
    "        elif spec['type'] == 'correlation':\n",
    "            t1, t2 = spec['assets'];\n",
    "            p1, p2 = first_col_containing(t1, 'PX_LAST'), first_col_containing(t2, 'PX_LAST')\n",
    "            if p1 and p2:\n",
    "                aligned = pd.DataFrame({'s1': safe_series(p1), 's2': safe_series(p2)}).dropna()\n",
    "                if len(aligned) > spec['params']['window']: feat[feature_id] = aligned['s1'].rolling(\n",
    "                    spec['params']['window']).corr(aligned['s2'])\n",
    "        elif spec['type'] in ['correlation_zscore', 'correlation_delta']:\n",
    "            t1, t2 = spec['assets']\n",
    "            c20_id = f'corr_{t1}_{t2}_20d'\n",
    "            c60_id = f'corr_{t1}_{t2}_60d'\n",
    "            # Ensure correlation features are calculated first\n",
    "            c20 = feat.get(c20_id) if c20_id in feat.columns else None\n",
    "            c60 = feat.get(c60_id) if c60_id in feat.columns else None\n",
    "            if c20 is not None and c60 is not None:\n",
    "                if spec['type'] == 'correlation_zscore':\n",
    "                    feat[feature_id] = (c20 - c20.rolling(60).mean()) / c20.rolling(60).std()\n",
    "                else:\n",
    "                    feat[feature_id] = c20 - c60\n",
    "        elif spec['type'] == 'rolling_beta':\n",
    "            t1, t2 = spec['assets'];\n",
    "            p1, p2 = first_col_containing(t1, 'PX_LAST'), first_col_containing(t2, 'PX_LAST')\n",
    "            if p1 and p2:\n",
    "                rets = pd.DataFrame({'r1': safe_series(p1).pct_change(), 'r2': safe_series(p2).pct_change()}).dropna()\n",
    "                if len(rets) > spec['params']['window']: feat[feature_id] = rets['r1'].rolling(\n",
    "                    spec['params']['window']).cov(rets['r2']) / rets['r2'].rolling(spec['params']['window']).var()\n",
    "        # MACRO\n",
    "        elif spec['type'] == 'macro_mpi':\n",
    "            dxy, ust10 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not dxy.empty and not ust10.empty: feat[feature_id] = dxy.pct_change().rolling(\n",
    "                3).sum() + ust10.pct_change().rolling(3).sum()\n",
    "        elif spec['type'] == 'macro_fear_overdrive':\n",
    "            vix, dxy, spy = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][2], 'PX_LAST'))\n",
    "            if not vix.empty and not dxy.empty and not spy.empty: feat[feature_id] = (\n",
    "                        (vix > 20) & (dxy.pct_change() > 0) & (spy < spy.rolling(20).mean())).astype(int)\n",
    "        elif spec['type'] == 'macro_sector_rotation':\n",
    "            xlk, xle = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not xlk.empty and not xle.empty: feat[feature_id] = xlk.pct_change(5) - xle.pct_change(5)\n",
    "        elif spec['type'] == 'macro_yield_spread':\n",
    "            ust10, ust2 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not ust10.empty and not ust2.empty: feat[feature_id] = ust10 - ust2\n",
    "        elif spec['type'] == 'macro_generic_mom':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change(spec['params']['days'])\n",
    "        elif spec['type'] == 'macro_generic_chg':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change()\n",
    "        elif spec['type'] == 'macro_cpi_zscore':\n",
    "            cpi = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not cpi.empty: feat[feature_id] = (cpi - cpi.rolling(12).mean()) / cpi.rolling(12).std()\n",
    "        elif spec['type'] == 'macro_injcjc_shock':\n",
    "            injcjc = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not injcjc.empty: feat[feature_id] = (injcjc.diff() > injcjc.diff().rolling(20).std() * 2).astype(int)\n",
    "        elif spec['type'] == 'macro_ffa_spread':\n",
    "            ffa, ust2 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not ffa.empty and not ust2.empty: feat[feature_id] = ffa - ust2\n",
    "        elif spec['type'] == 'macro_lf94truu_vol_signal':\n",
    "            vol = safe_series(first_col_containing(spec['assets'][0], 'VOLATILITY_30D'))\n",
    "            if not vol.empty: feat[feature_id] = vol / vol.rolling(60).mean()\n",
    "        # MOMENTUM / FRACTALS\n",
    "        elif spec['type'] == 'mom_div_vol':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change(5) / px.pct_change().rolling(20).std()\n",
    "        elif spec['type'] == 'bollinger_pctB':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty:\n",
    "                ma = px.rolling(spec['params']['window']).mean();\n",
    "                std = px.rolling(spec['params']['window']).std()\n",
    "                feat[feature_id] = (px - (ma - 2 * std)) / (4 * std)\n",
    "        elif spec['type'] == 'fractional_differencing':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty and len(px) > spec['params']['window']: feat[feature_id] = frac_diff(px,\n",
    "                                                                                                 d=spec['params']['d'],\n",
    "                                                                                                 window=spec['params'][\n",
    "                                                                                                     'window'])\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate feature '{feature_id}': {e}\")\n",
    "feat = feat.shift(1)  # Shift all features to prevent lookahead bias\n",
    "print(f\"Calculated {feat.shape[1]} feature series.\")\n",
    "\n",
    "# --- 3. Define Primitive Signals from Features (with multiple condition types) ---\n",
    "print('--- Defining Primitive Signals ---')\n",
    "primitive_signals = []\n",
    "signal_series = {}\n",
    "signal_id_counter = 0\n",
    "\n",
    "for feature_id in feat.columns:\n",
    "    s = feat[feature_id].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty or s.std() == 0: continue\n",
    "\n",
    "    # Percentile-based signals\n",
    "    for op, val in [('>', 0.8), ('<', 0.2)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\";\n",
    "        signal_id_counter += 1\n",
    "        primitive_signals.append(\n",
    "            {'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'percentile', 'operator': op,\n",
    "             'value': val})\n",
    "        signal_series[sig_id] = s.rank(pct=True).apply(lambda x: x > val if op == '>' else x < val)\n",
    "\n",
    "    # Z-score signals\n",
    "    rolling_std = s.rolling(60).std()\n",
    "    valid_std_mask = rolling_std > 1e-9\n",
    "    z = pd.Series(np.nan, index=s.index)\n",
    "    z[valid_std_mask] = (s - s.rolling(60).mean())[valid_std_mask] / rolling_std[valid_std_mask]\n",
    "    for op, val in [('>', 1.5), ('<', -1.5)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\";\n",
    "        signal_id_counter += 1\n",
    "        primitive_signals.append(\n",
    "            {'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'z_score', 'operator': op, 'value': val})\n",
    "        signal_series[sig_id] = z.apply(lambda x: x > val if op == '>' else x < val)\n",
    "\n",
    "print(f\"Defined {len(primitive_signals)} primitive signals.\")\n",
    "\n",
    "# --- 4. Generate Setups, Evaluate, and Output ---\n",
    "print('--- Generating Candidate Setups ---')\n",
    "all_candidate_setups = []\n",
    "setup_id_counter = 1\n",
    "for k in SETUP_LENGTHS_TO_EXPLORE:\n",
    "    signal_ids = [s['signal_id'] for s in primitive_signals]\n",
    "    if len(signal_ids) < k: continue\n",
    "\n",
    "    # Use random sampling to keep computation manageable\n",
    "    combinations_to_test = [random.sample(signal_ids, k) for _ in range(NUM_RANDOM_SETUPS_TO_SAMPLE)]\n",
    "\n",
    "    for sig_id_list in combinations_to_test:\n",
    "        try:\n",
    "            mask = functools.reduce(lambda a, b: a & b, [signal_series[sid] for sid in sig_id_list])\n",
    "            if mask.sum() >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "                # For each signal_id, find its full definition\n",
    "                signal_definitions = [p for p in primitive_signals if p['signal_id'] in sig_id_list]\n",
    "                all_candidate_setups.append({\n",
    "                    'id': f'S{setup_id_counter:04d}',\n",
    "                    'signal_definitions': signal_definitions\n",
    "                })\n",
    "                setup_id_counter += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "print(f\"Generated {len(all_candidate_setups)} candidate setups.\")\n",
    "\n",
    "# Prepare Returns for Evaluation\n",
    "price_cols_for_returns = [first_col_containing(t, 'PX_LAST') for t in TRADABLE_TICKERS if\n",
    "                          first_col_containing(t, 'PX_LAST')]\n",
    "prices = raw[price_cols_for_returns].copy()\n",
    "returns = {h: prices.pct_change(h).shift(-h) for h in [1, 3, 5, 10, 21]}\n",
    "\n",
    "\n",
    "# --- NEW: Plain-English Description Generator ---\n",
    "def generate_english_description(setup_id, signal_defs, feature_specs_list):\n",
    "    \"\"\"Generates a human-readable description of a setup.\"\"\"\n",
    "    clauses = []\n",
    "    explained_clauses = []\n",
    "    for s_def in signal_defs:\n",
    "        feat_name = next(\n",
    "            (f_spec['display_name'] for f_spec in feature_specs_list if f_spec['unique_id'] == s_def['feature_id']),\n",
    "            s_def['feature_id'])\n",
    "\n",
    "        # NEW: Fully unpack the feature name for the explained description\n",
    "        feature_details = feat_name.replace('(', ' ').replace(')', ' ').replace(',', ' ').split()\n",
    "        feature_type = feature_details[0]\n",
    "        assets = [asset for asset in feature_details if 'US Equity' in asset or 'Index' in asset]\n",
    "        params = [p for p in feature_details if p.isdigit() or 'd=' in p]\n",
    "\n",
    "        explained_clause = f\"The {feature_type} \"\n",
    "        if len(assets) > 1:\n",
    "            explained_clause += f\"between {assets[0]} and {assets[1]} \"\n",
    "        elif assets:\n",
    "            explained_clause += f\"of {assets[0]} \"\n",
    "        if params:\n",
    "            explained_clause += f\"over a {params[0]} day window \"\n",
    "\n",
    "        if s_def['condition_type'] == 'percentile':\n",
    "            level = \"is very high (top 20%)\" if s_def['operator'] == '>' else \"is very low (bottom 20%)\"\n",
    "            clauses.append(f\"{feat_name} is {'very high' if s_def['operator'] == '>' else 'very low'}\")\n",
    "            explained_clauses.append(explained_clause + level)\n",
    "        else:  # z_score\n",
    "            level = \"surges unexpectedly\" if s_def['operator'] == '>' else \"drops sharply\"\n",
    "            clauses.append(f\"{feat_name} {level}\")\n",
    "            explained_clauses.append(explained_clause + level)\n",
    "\n",
    "    description = f\"When {clauses[0]}\"\n",
    "    if len(clauses) > 1:\n",
    "        description += f\" and {' and '.join(clauses[1:])}\"\n",
    "\n",
    "    explained_description = f\"When {explained_clauses[0]}\"\n",
    "    if len(explained_clauses) > 1:\n",
    "        explained_description += f\" and {' and '.join(explained_clauses[1:])}\"\n",
    "\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs)\n",
    "    bias = 'a bullish' if direction_score > 0 else 'a bearish' if direction_score < 0 else 'an uncertain'\n",
    "    description += f\", it may indicate {bias} outlook.\"\n",
    "    explained_description += f\", it may indicate {bias} outlook.\"\n",
    "\n",
    "    return {'setup_id': setup_id, 'description': description, 'explained_description': explained_description}\n",
    "\n",
    "\n",
    "# --- Parallel Setup Evaluation Function (UPDATED) ---\n",
    "def evaluate_one_setup(setup):\n",
    "    \"\"\"Helper function to evaluate a single setup for parallel processing.\"\"\"\n",
    "    sid, signal_defs = setup['id'], setup['signal_definitions']\n",
    "    try:\n",
    "        mask = functools.reduce(lambda a, b: a & b, [signal_series[s['signal_id']] for s in signal_defs])\n",
    "        dates = mask[mask].index\n",
    "    except (KeyError, TypeError):\n",
    "        return [], [], None\n",
    "\n",
    "    if len(dates) < MIN_INITIAL_SUPPORT_FILTER: return [], [], None\n",
    "\n",
    "    # Infer setup properties from signal definitions\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs)\n",
    "\n",
    "    # --- FIX: Discard setups with mixed signals (score of 0) ---\n",
    "    if direction_score == 0:\n",
    "        return [], [], None # Return empty results to filter this setup out\n",
    "\n",
    "    entry_direction = 'long' if direction_score > 0 else 'short' # 'mixed' is no longer possible\n",
    "\n",
    "    feature_types = [spec['type'] for sig_def in signal_defs for spec in feature_specs if\n",
    "                     spec['unique_id'] == sig_def['feature_id']]\n",
    "    dominant_signal_type = max(set(feature_types), key=feature_types.count) if feature_types else 'unknown'\n",
    "\n",
    "    # Create human-readable conditions for JSON output\n",
    "    human_readable_conds = []\n",
    "    for s_def in signal_defs:\n",
    "        feat_name = next(\n",
    "            (f_spec['display_name'] for f_spec in feature_specs if f_spec['unique_id'] == s_def['feature_id']),\n",
    "            s_def['feature_id'])\n",
    "        if s_def['condition_type'] == 'percentile':\n",
    "            cond_str = f\"{s_def['operator']} {s_def['value'] * 100:.0f}th percentile\"\n",
    "        else:  # z_score\n",
    "            cond_str = f\"z-score {s_def['operator']} {s_def['value']}\"\n",
    "        human_readable_conds.append(f\"{feat_name} {cond_str}\")\n",
    "\n",
    "    # Generate plain-english description\n",
    "    description_record = generate_english_description(sid, signal_defs, feature_specs)\n",
    "\n",
    "    # --- NEW: Granular, Per-Ticker Evaluation ---\n",
    "    summary_rows_for_setup = []\n",
    "    # --- ADDITION: Create a list to hold detailed trade records for the new ledger ---\n",
    "    trade_ledger_rows = []\n",
    "\n",
    "    for tk_col in price_cols_for_returns:\n",
    "        tk_symbol = None\n",
    "        for ticker_prefix in TRADABLE_TICKERS:\n",
    "            if tk_col.startswith(ticker_prefix):\n",
    "                tk_symbol = ticker_prefix\n",
    "                break\n",
    "        if tk_symbol is None: continue\n",
    "\n",
    "        summary_row = {\n",
    "            'setup_id': sid,\n",
    "            'target_ticker': tk_symbol,\n",
    "            'feature_conditions': json.dumps(human_readable_conds),\n",
    "            'support': len(dates),\n",
    "            'entry_direction': entry_direction,\n",
    "            'dominant_signal_type': dominant_signal_type,\n",
    "            'first_trigger_date': dates.min(),\n",
    "            'last_trigger_date': dates.max()\n",
    "        }\n",
    "\n",
    "        # Calculate metrics for this specific ticker\n",
    "        perf_horizons = [3, 5, 10, 21]\n",
    "        for h in perf_horizons:\n",
    "            r_ticker = returns[h][tk_col].reindex(dates)\n",
    "\n",
    "            if h == 3: summary_row['accuracy_3d'] = 0.0 if r_ticker.empty else (r_ticker > 0).mean()\n",
    "            if h == 5: summary_row['avg_return_5d'] = 0.0 if r_ticker.empty else r_ticker.mean()\n",
    "            if h == 21: summary_row['hit_rate_21d'] = 0.0 if r_ticker.empty else (r_ticker > 0).mean()\n",
    "\n",
    "            if not r_ticker.empty and r_ticker.std() > 1e-6 and len(r_ticker) > 5:\n",
    "                summary_row[f'sharpe_{h}d'], _, _ = block_bootstrap_sharpe(r_ticker, block_size=h)\n",
    "            else:\n",
    "                summary_row[f'sharpe_{h}d'] = 0.0\n",
    "\n",
    "        # Calculate average prices and PnL for this ticker\n",
    "        trigger_df_for_ticker = pd.DataFrame([\n",
    "            {'date': d, 'underlying_entry_px': raw[tk_col].get(d),\n",
    "             **{f'underlying_exit_px_{h_opt}d': raw[tk_col].reindex([d + pd.Timedelta(days=h_opt)], method='nearest',\n",
    "                                                                    tolerance=pd.Timedelta(days=3)).iloc[0]\n",
    "             if not raw[tk_col].reindex([d + pd.Timedelta(days=h_opt)], method='nearest',\n",
    "                                        tolerance=pd.Timedelta(days=3)).empty else np.nan\n",
    "                for h_opt in OPTION_SIM_HORIZONS_DAYS}}\n",
    "            for d in dates\n",
    "        ])\n",
    "        trigger_df_for_ticker.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        for h_opt in OPTION_SIM_HORIZONS_DAYS:\n",
    "            summary_row[f'avg_underlying_entry_px_{h_opt}d'] = trigger_df_for_ticker['underlying_entry_px'].mean()\n",
    "            summary_row[f'avg_underlying_exit_px_{h_opt}d'] = trigger_df_for_ticker[\n",
    "                f'underlying_exit_px_{h_opt}d'].mean()\n",
    "\n",
    "            ivol_col = (\n",
    "                    first_col_containing(tk_symbol, '30_Day_Call_Implied_Volatility') or\n",
    "                    first_col_containing(tk_symbol, '60_Day_Call_Implied_Volatility') or\n",
    "                    first_col_containing(tk_symbol, '10_Day_Call_Implied_Volatility') or\n",
    "                    first_col_containing(tk_symbol, 'IVOL_SIGMA') or\n",
    "                    first_col_containing(tk_symbol, 'CALL_IMP_VOL_30D')\n",
    "            )\n",
    "            ivol_series = raw[ivol_col].reindex(dates) if ivol_col else pd.Series(np.nan, index=dates)\n",
    "\n",
    "            pnl_results = []\n",
    "            for i, d in enumerate(dates):\n",
    "                pnl_detail = simulate_option_pnl_detailed(\n",
    "                    trigger_df_for_ticker.loc[i, 'underlying_entry_px'],\n",
    "                    trigger_df_for_ticker.loc[i, f'underlying_exit_px_{h_opt}d'],\n",
    "                    ivol_series.iloc[i],\n",
    "                    h_opt,\n",
    "                    entry_direction\n",
    "                )\n",
    "                pnl_results.append(pnl_detail)\n",
    "\n",
    "                # --- ADDITION: Populate the detailed trade ledger ---\n",
    "                ledger_record = {\n",
    "                    'setup_id': sid,\n",
    "                    'trigger_date': d,\n",
    "                    'target_ticker': tk_symbol,\n",
    "                    'horizon_days': h_opt,\n",
    "                    'entry_direction': entry_direction,\n",
    "                    'underlying_entry_px': trigger_df_for_ticker.loc[i, 'underlying_entry_px'],\n",
    "                    'ivol_at_entry': ivol_series.iloc[i] if not ivol_series.empty else np.nan,\n",
    "                    **pnl_detail\n",
    "                }\n",
    "                trade_ledger_rows.append(ledger_record)\n",
    "\n",
    "            summary_row[f'avg_option_pnl_dollars_{h_opt}d'] = pd.Series([p['pnl_dollars'] for p in pnl_results]).mean()\n",
    "\n",
    "        summary_rows_for_setup.append(summary_row)\n",
    "\n",
    "    # --- CHANGE: Return the populated list of trade ledger rows ---\n",
    "    return summary_rows_for_setup, trade_ledger_rows, description_record\n",
    "\n",
    "\n",
    "# --- Parallel processing with joblib ---\n",
    "print(f\"\\n--- Starting Parallel Evaluation of {len(all_candidate_setups)} Setups ---\")\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup) for setup in all_candidate_setups)\n",
    "summary_rows, description_records = [], []\n",
    "# --- ADDITION: Create a list to aggregate all trade ledger rows from parallel runs ---\n",
    "all_trade_ledger_rows = []\n",
    "for perf_list, trade_list, desc in results:\n",
    "    if perf_list: summary_rows.extend(perf_list)\n",
    "    if trade_list: all_trade_ledger_rows.extend(trade_list)\n",
    "    if desc: description_records.append(desc)\n",
    "\n",
    "# --- Final Output Generation ---\n",
    "print('--- Generating Final Output Files ---')\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "description_df = pd.DataFrame(description_records)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    summary_df['setup_duration_days'] = (pd.to_datetime(summary_df['last_trigger_date']) - pd.to_datetime(\n",
    "        summary_df['first_trigger_date'])).dt.days\n",
    "    summary_df['avg_trigger_frequency_per_day'] = summary_df['support'] / summary_df['setup_duration_days'].replace(0,\n",
    "                                                                                                                    np.nan)\n",
    "\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    summary_df.to_csv('setup_results_summary.csv', index=False)\n",
    "    print(\"Saved 'setup_results_summary.csv'\")\n",
    "\n",
    "if not description_df.empty:\n",
    "    description_df.to_csv('setup_descriptions.csv', index=False)\n",
    "    print(\"Saved 'setup_descriptions.csv'\")\n",
    "\n",
    "# --- ADDITION: Create and save the detailed trade ledger CSV ---\n",
    "if all_trade_ledger_rows:\n",
    "    trade_ledger_df = pd.DataFrame(all_trade_ledger_rows)\n",
    "    trade_ledger_df.to_csv('trade_ledger.csv', index=False)\n",
    "    print(\"Saved 'trade_ledger.csv'\")\n",
    "\n",
    "# Sort by a default sharpe, e.g., 21d\n",
    "top_setups = summary_df.sort_values('sharpe_21d', ascending=False).head(\n",
    "    20) if 'sharpe_21d' in summary_df.columns else pd.DataFrame()\n",
    "top_setups.to_json('top_setups.json', orient='records', indent=2)\n",
    "print(\"Saved 'top_setups.json'\")\n",
    "\n",
    "print('\\nDiscovery complete. All original features were migrated to the new architecture.')\n",
    "print(\"\\nTop Setups by Sharpe Ratio (21d):\")\n",
    "# Display new sharpe columns\n",
    "display_cols = ['setup_id', 'target_ticker', 'support', 'sharpe_3d', 'sharpe_21d', 'feature_conditions']\n",
    "print(top_setups[[c for c in display_cols if c in top_setups.columns]].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw workbooksâ€¦\n",
      "An unexpected error occurred during Excel loading of 'All_Tickers copy.xlsx': Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.\n",
      "Main data could not be loaded, skipping macro data loading.\n",
      "No data loaded. Raw DataFrame is empty.\n",
      "Raw shape: (0, 0)\n",
      "\n",
      "Identified all relevant prefixes/tickers for feature engineering: 31\n",
      "\n",
      "--- Defining ALL Feature Specifications ---\n",
      "Defined 2810 total feature specifications.\n",
      "--- Calculating All Features ---\n",
      "Calculated 0 feature series.\n",
      "--- Defining Primitive Signals ---\n",
      "Defined 0 primitive signals.\n",
      "--- Generating Candidate Setups ---\n",
      "Generated 0 candidate setups.\n",
      "\n",
      "--- Starting Parallel Evaluation of 0 Setups ---\n",
      "--- Generating Final Output Files ---\n",
      "Saved 'top_setups.json'\n",
      "\n",
      "Discovery complete. All original features were migrated to the new architecture.\n",
      "\n",
      "Top Setups by Sharpe Ratio (21d):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e9121497b7bce57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T06:05:31.860480Z",
     "start_time": "2025-08-03T06:04:36.748383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Engineering + Discovery Engine with FULLY Migrated Structured Features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import linregress\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "# Define the explicit list of tradable tickers\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "]\n",
    "\n",
    "# --- ADDITION: Define list of macro tickers to ensure their inclusion in feature generation ---\n",
    "MACRO_TICKERS = [\n",
    "    'DXY Curncy', 'USGG10YR Index', 'USGG2YR Index', 'CPI YOY Index',\n",
    "    'INJCJC Index', 'FFA Comdty', 'LF94TRUU Index', 'CPI CHNG Index',\n",
    "    'NFP TCH Index', 'JOBS US Equity', 'CTII10 Govt', 'USSW10 Curncy',\n",
    "    'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index',\n",
    "    'SPCS20SM Index', 'CONSSENT Index'\n",
    "]\n",
    "\n",
    "# Define the file paths\n",
    "MAIN_DATA_FILE = 'All_Tickers copy.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_no_nan_cols.xlsx'\n",
    "\n",
    "# Setup Generation Configuration\n",
    "NUM_RANDOM_SETUPS_TO_SAMPLE = 100\n",
    "SETUP_LENGTHS_TO_EXPLORE = [2,3]\n",
    "MIN_INITIAL_SUPPORT_FILTER = 5\n",
    "\n",
    "# Option Simulation Configuration\n",
    "OPTION_SIM_HORIZONS_DAYS = [3, 10, 21]\n",
    "RISK_FREE_RATE = 0.01\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooksâ€¦')\n",
    "\n",
    "\n",
    "# --- Custom Data Loading Function (Unchanged) ---\n",
    "def load_and_merge_excel(file_path, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "        for sh_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name)\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found.\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df\n",
    "\n",
    "\n",
    "# Load main and macro data\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE)\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "# Final cleaning and indexing\n",
    "if not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "    if 'Date' in raw.columns:\n",
    "        raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "        raw = raw.drop_duplicates(subset=['Date'], keep='last')\n",
    "        raw.set_index('Date', inplace=True)\n",
    "        raw.index = pd.to_datetime(raw.index)\n",
    "        raw.sort_index(inplace=True)\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "\n",
    "# --- Dynamic Ticker Identification (Unchanged) ---\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c])))\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total', '30', '10',\n",
    "                           '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC', 'NFP', 'JOBS', 'CPI', 'CTII10',\n",
    "                           'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT', 'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM',\n",
    "                           'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes + MACRO_TICKERS)))\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {len(all_tickers)}')\n",
    "\n",
    "\n",
    "# --- Helper functions (Unchanged) ---\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    \"\"\"Finds the first column name in raw that matches the pattern 'ticker_full_name_substr'.\"\"\"\n",
    "    if substr == 'PX_LAST':\n",
    "        for potential_col in [f\"{ticker_full_name}_Last_Price_PX_LAST\", f\"{ticker_full_name}_PX_LAST\"]:\n",
    "            if potential_col in raw.columns: return potential_col\n",
    "    for c in raw.columns:\n",
    "        if c.startswith(ticker_full_name) and substr in c: return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def safe_series(col_name):\n",
    "    \"\"\"Returns a column as a Series, or an empty Series if column does not exist.\"\"\"\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "\n",
    "def frac_diff(series, d=0.5, window=100):\n",
    "    \"\"\"Computes fractionally differenced series.\"\"\"\n",
    "    weights = [1.]\n",
    "    for k in range(1, len(series)): weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1])\n",
    "    output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(window, len(series)):\n",
    "        subset = series.iloc[i - window + 1: i + 1]\n",
    "        if len(subset) == len(weights[-window:]):\n",
    "            output.iloc[i] = np.dot(weights[-window:], subset)\n",
    "    return output.dropna()\n",
    "\n",
    "\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=1000, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"Calculates the Sharpe Ratio using block bootstrapping.\"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2: return 0.0, 0.0, 0.0\n",
    "    blocks = [returns_series.iloc[i: i + block_size] for i in range(0, len(returns_series), block_size) if\n",
    "              not returns_series.iloc[i: i + block_size].empty]\n",
    "    if not blocks: return 0.0, 0.0, 0.0\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size))\n",
    "    sharpes = []\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = [blocks[i] for i in np.random.choice(len(blocks), n_blocks_to_sample, replace=True)]\n",
    "        resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)]\n",
    "        if resampled_returns.std() > 1e-9:\n",
    "            sharpes.append((resampled_returns.mean() / resampled_returns.std()) * (\n",
    "                np.sqrt(trading_days_per_year) if annualize else 1))\n",
    "        else:\n",
    "            sharpes.append(0.0)\n",
    "    if not sharpes: return 0.0, 0.0, 0.0\n",
    "    return np.median(sharpes), np.percentile(sharpes, 5), np.percentile(sharpes, 95)\n",
    "\n",
    "\n",
    "# --- Option Simulation Helpers (UPDATED) ---\n",
    "def estimate_atm_premium(price, ivol, days, option_type):\n",
    "    \"\"\"Placeholder for a real option pricing model like Black-Scholes.\"\"\"\n",
    "    T = days / 365.25\n",
    "    if T <= 0 or price <= 0 or ivol <= 0: return 0\n",
    "    # This is a highly simplified placeholder. A real model (e.g., scipy.stats.norm.cdf) would be needed for accuracy.\n",
    "    return 0.4 * price * ivol * np.sqrt(T)\n",
    "\n",
    "# --- FIX: Update function to include Underlying_Exit_Price and Return_Underlying ---\n",
    "def simulate_option_pnl_detailed(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    \"\"\"Simulates PnL and returns a dictionary with all components for verification.\"\"\"\n",
    "    # Calculate underlying return, handle division by zero\n",
    "    underlying_return = (future_price - current_price) / current_price if current_price and pd.notna(current_price) and pd.notna(future_price) else np.nan\n",
    "\n",
    "    nan_result = {\n",
    "        'pnl_per_share': np.nan, 'option_type': None, 'strike_price': np.nan,\n",
    "        'entry_premium': np.nan,\n",
    "        'exit_value': np.nan, 'pnl_dollars': np.nan, 'skipped_reason': 'None',\n",
    "        'Underlying_Exit_Price': future_price if pd.notna(future_price) else np.nan,\n",
    "        'Return_Underlying': underlying_return,\n",
    "    }\n",
    "\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Price'\n",
    "        return nan_result\n",
    "    if pd.isna(ivol_at_entry) or ivol_at_entry <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid IVOL'\n",
    "        return nan_result\n",
    "    if pd.isna(future_price):\n",
    "        nan_result['skipped_reason'] = 'Missing Future Price'\n",
    "        return nan_result\n",
    "    if entry_direction not in ['long', 'short']:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Direction'\n",
    "        return nan_result\n",
    "\n",
    "    scaled_ivol = ivol_at_entry / 100.0 if ivol_at_entry > 1.0 else ivol_at_entry\n",
    "    strike_price = current_price\n",
    "    option_type = 'call' if entry_direction == 'long' else 'put'\n",
    "\n",
    "    entry_premium = estimate_atm_premium(current_price, scaled_ivol, horizon_days, option_type)\n",
    "    if option_type == 'call':\n",
    "        exit_value = max(future_price - strike_price, 0)\n",
    "    else:  # put\n",
    "        exit_value = max(strike_price - future_price, 0)\n",
    "\n",
    "    pnl_per_share = exit_value - entry_premium\n",
    "    pnl_dollars = pnl_per_share * 100\n",
    "\n",
    "    return {\n",
    "        'pnl_per_share': pnl_per_share,\n",
    "        'option_type': option_type,\n",
    "        'strike_price': strike_price,\n",
    "        'entry_premium': entry_premium,\n",
    "        'exit_value': exit_value,\n",
    "        'pnl_dollars': pnl_dollars,\n",
    "        'skipped_reason': 'None',\n",
    "        'Underlying_Exit_Price': future_price,\n",
    "        'Return_Underlying': underlying_return,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- ARCHITECTURE REWRITE: Structured Feature & Signal Generation ---\n",
    "\n",
    "# --- 1. Define Feature Specifications (Complete Migration) ---\n",
    "# --- FIX: Generating fully transparent feature names while preserving code structure ---\n",
    "print('\\n--- Defining ALL Feature Specifications ---')\n",
    "feature_specs = []\n",
    "\n",
    "# Volatility Features\n",
    "for ticker in all_tickers:\n",
    "    f60 = '60_Day_Call_Implied_Volatility'; f10 = '10_Day_Call_Implied_Volatility'\n",
    "    feature_specs.append({'type': 'ivol_term_structure', 'assets': [ticker], 'params': {'f_long':f60, 'f_short':f10},\n",
    "                          'unique_id': f'term_structure_{f60}-{f10}__{ticker}',\n",
    "                          'display_name': f\"diff({f60}, {f10})__{ticker}\"})\n",
    "    put50 = '1st_Month_Put_Imp_Vol_50_Delta'; call40 = '1st_Month_Call_Imp_Vol_40_Delta'\n",
    "    feature_specs.append({'type': 'ivol_skew', 'assets': [ticker], 'params': {'put':put50, 'call':call40},\n",
    "                          'unique_id': f'skew_{put50}-{call40}__{ticker}',\n",
    "                          'display_name': f\"diff({put50}, {call40})__{ticker}\"})\n",
    "    for suffix in ['IVOL_SIGMA', 'CALL_IMP_VOL_30D', 'PUT_IMP_VOL_30D']:\n",
    "        feature_specs.append({'type': 'ivol_shock', 'assets': [ticker], 'params': {'ivol_suffix': suffix, 'window': 30},\n",
    "                              'unique_id': f'zscore_{suffix}_30d__{ticker}',\n",
    "                              'display_name': f\"zscore_{suffix}_30d__{ticker}\"})\n",
    "        feature_specs.append({'type': 'ivol_div_volume', 'assets': [ticker], 'params': {'ivol_suffix': suffix, 'vol_suffix':'VOLUME'},\n",
    "                              'unique_id': f'div_{suffix}_by_VOLUME__{ticker}',\n",
    "                              'display_name': f\"div({suffix}, VOLUME)__{ticker}\"})\n",
    "\n",
    "# Deriv Flow & Sentiment Features\n",
    "for ticker in all_tickers:\n",
    "    pc_ratio_col = 'PUT_CALL_VOLUME_RATIO_CUR_DAY'\n",
    "    feature_specs.append({'type': 'put_call_ratio_ema', 'assets': [ticker], 'params': {'span': 5, 'col': pc_ratio_col},\n",
    "                          'unique_id': f'ema5_{pc_ratio_col}__{ticker}',\n",
    "                          'display_name': f\"ema5_{pc_ratio_col}__{ticker}\"})\n",
    "    oi_col = 'OPEN_INT_TOTAL_CALL'\n",
    "    feature_specs.append({'type': 'open_interest_change', 'assets': [ticker], 'params': {'days': 3, 'col': oi_col},\n",
    "                          'unique_id': f'pct_change_{oi_col}_3d__{ticker}',\n",
    "                          'display_name': f\"pct_change_{oi_col}_3d__{ticker}\"})\n",
    "    vol_col = 'Volume_-_Realtime_VOLUME'\n",
    "    feature_specs.append({'type': 'volume_zscore', 'assets': [ticker], 'params': {'window': 30, 'col': vol_col},\n",
    "                          'unique_id': f'zscore_{vol_col}_30d__{ticker}',\n",
    "                          'display_name': f\"zscore_{vol_col}_30d__{ticker}\"})\n",
    "    sm_oi = 'OPEN_INT_TOTAL_CALL'; sm_ivol='10_Day_Call_Implied_Volatility'\n",
    "    feature_specs.append({'type': 'smart_money_flag', 'assets': [ticker], 'params': {'oi_col': sm_oi, 'ivol_col': sm_ivol},\n",
    "                          'unique_id': f'smart_money_{sm_oi}_{sm_ivol}__{ticker}',\n",
    "                          'display_name': f\"smart_money(pct_change({sm_oi}) > 0 AND pct_change({sm_ivol}) > 0)__{ticker}\"})\n",
    "\n",
    "# Cross-Asset Correlation Features\n",
    "price_col = 'PX_LAST'\n",
    "correlation_pairs = list(set(itertools.combinations(all_tickers, 2)))\n",
    "for t1, t2 in correlation_pairs:\n",
    "    for window in [20, 60]:\n",
    "        feature_specs.append({'type': 'correlation', 'assets': [t1, t2], 'params': {'window': window, 'col': price_col},\n",
    "                              'unique_id': f'corr_{t1}:{price_col}_{t2}:{price_col}_{window}d',\n",
    "                              'display_name': f\"corr({t1}:{price_col}, {t2}:{price_col}, {window}d)\"})\n",
    "    feature_specs.append({'type': 'correlation_zscore', 'assets': [t1, t2], 'params':{'col':price_col, 'window':60},\n",
    "                          'unique_id': f'zscore_corr20d_{t1}:{price_col}_{t2}:{price_col}_60d',\n",
    "                          'display_name': f\"zscore_corr(20d)({t1}:{price_col}, {t2}:{price_col}, 60d)\"})\n",
    "    feature_specs.append({'type': 'correlation_delta', 'assets': [t1, t2], 'params':{'col':price_col},\n",
    "                          'unique_id': f'corr_delta_{t1}:{price_col}_{t2}:{price_col}',\n",
    "                          'display_name': f\"corr_delta(20d-60d)({t1}:{price_col}, {t2}:{price_col})\"})\n",
    "    feature_specs.append({'type': 'rolling_beta', 'assets': [t1, t2], 'params': {'window': 60, 'col':price_col},\n",
    "                          'unique_id': f'beta_{t1}:{price_col}_{t2}:{price_col}_60d',\n",
    "                          'display_name': f\"beta({t1}:{price_col}, {t2}:{price_col}, 60d)\"})\n",
    "\n",
    "# Macro Features (Original & New) - Using hardcoded specific names\n",
    "feature_specs.extend([\n",
    "    {'type': 'macro_mpi', 'assets': ['DXY Curncy', 'USGG10YR Index'], 'unique_id': 'macro_mpi',\n",
    "     'display_name': 'Macro Pressure Index'},\n",
    "    {'type': 'macro_fear_overdrive', 'assets': ['VIX Index', 'DXY Curncy', 'SPY US Equity'],\n",
    "     'unique_id': 'macro_fear_overdrive', 'display_name': 'Fear Overdrive'},\n",
    "    {'type': 'macro_sector_rotation', 'assets': ['XLK US Equity', 'XLE US Equity'],\n",
    "     'unique_id': 'macro_xlk_xle_rotation', 'display_name': 'Sector Rotation (XLK-XLE)'},\n",
    "    {'type': 'macro_yield_spread', 'assets': ['USGG10YR Index', 'USGG2YR Index'], 'unique_id': 'macro_10y2y_spread',\n",
    "     'display_name': 'Yield Spread (10Y-2Y)'},\n",
    "    {'type': 'macro_cpi_zscore', 'assets': ['CPI YOY Index'], 'unique_id': 'macro_cpi_z',\n",
    "     'display_name': 'CPI Z-Score'},\n",
    "    {'type': 'macro_injcjc_shock', 'assets': ['INJCJC Index'], 'unique_id': 'macro_jobless_claims_shock',\n",
    "     'display_name': 'Jobless Claims Shock'},\n",
    "    {'type': 'macro_ffa_spread', 'assets': ['FFA Comdty', 'USGG2YR Index'], 'unique_id': 'macro_ffa_spread',\n",
    "     'display_name': 'Fed Funds Spread'},\n",
    "    {'type': 'macro_lf94truu_vol_signal', 'assets': ['LF94TRUU Index'], 'unique_id': 'macro_hyg_vol_signal',\n",
    "     'display_name': 'HYG Vol Signal'}\n",
    "])\n",
    "\n",
    "# These features were missing from the previous consolidated version, restored to maintain structure\n",
    "for t in ['CPI YOY Index', 'CPI CHNG Index', 'NFP TCH Index', 'JOBS US Equity']:\n",
    "    feature_specs.append(\n",
    "        {'type': 'macro_generic_mom', 'assets': [t], 'params': {'days': 3}, 'unique_id': f'macro_mom3_{t}',\n",
    "         'display_name': f'Macro Mom3d({t})'})\n",
    "for t in ['CTII10 Govt', 'USSW10 Curncy', 'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index',\n",
    "          'SPCS20SM Index', 'CONSSENT Index']:\n",
    "    feature_specs.append(\n",
    "        {'type': 'macro_generic_chg', 'assets': [t], 'unique_id': f'macro_chg_{t}', 'display_name': f'Macro Chg({t})'})\n",
    "\n",
    "# Momentum / Volatility Fractal Features\n",
    "for ticker in all_tickers:\n",
    "    feature_specs.append({'type': 'mom_div_vol', 'assets': [ticker], 'params': {'price_col':price_col, 'mom_win':5, 'vol_win':20},\n",
    "                          'unique_id': f'mom_div_vol_{price_col}_5d_20d__{ticker}',\n",
    "                          'display_name': f\"mom_div_vol({price_col}, 5d, 20d)__{ticker}\"})\n",
    "    feature_specs.append({'type': 'bollinger_pctB', 'assets': [ticker], 'params': {'window': 20, 'price_col':price_col},\n",
    "                          'unique_id': f'pctB_{price_col}_20d__{ticker}',\n",
    "                          'display_name': f\"%B({price_col}, 20d)__{ticker}\"})\n",
    "    feature_specs.append({'type': 'fractional_differencing', 'assets': [ticker], 'params': {'d': 0.5, 'window': 100, 'price_col':price_col},\n",
    "                          'unique_id': f'frac_diff_{price_col}_d0.5_100w__{ticker}',\n",
    "                          'display_name': f\"frac_diff({price_col}, d=0.5, win=100)__{ticker}\"})\n",
    "\n",
    "\n",
    "print(f\"Defined {len(feature_specs)} total feature specifications.\")\n",
    "\n",
    "\n",
    "# --- 2. Calculate Features Based on Specifications (Complete Calculation Engine) ---\n",
    "print('--- Calculating All Features ---')\n",
    "feat = pd.DataFrame(index=raw.index)\n",
    "for spec in feature_specs:\n",
    "    feature_id = spec['unique_id']\n",
    "    try:\n",
    "        # VOLATILITY\n",
    "        if spec['type'] == 'ivol_term_structure':\n",
    "            ivol60 = safe_series(first_col_containing(spec['assets'][0], spec['params']['f_long']))\n",
    "            ivol10 = safe_series(first_col_containing(spec['assets'][0], spec['params']['f_short']))\n",
    "            if not ivol60.empty and not ivol10.empty: feat[feature_id] = ivol60 - ivol10\n",
    "        elif spec['type'] == 'ivol_skew':\n",
    "            put50 = safe_series(first_col_containing(spec['assets'][0], spec['params']['put']))\n",
    "            call40 = safe_series(first_col_containing(spec['assets'][0], spec['params']['call']))\n",
    "            if not put50.empty and not call40.empty: feat[feature_id] = put50 - call40\n",
    "        elif spec['type'] == 'ivol_shock':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix']))\n",
    "            if not ivol_s.empty: feat[feature_id] = (ivol_s.diff() - ivol_s.diff().rolling(\n",
    "                spec['params']['window']).mean()) / ivol_s.diff().rolling(spec['params']['window']).std()\n",
    "        elif spec['type'] == 'ivol_div_volume':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix']))\n",
    "            vol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['vol_suffix']))\n",
    "            if not ivol_s.empty and not vol_s.empty: feat[feature_id] = ivol_s / vol_s.replace(0, np.nan)\n",
    "        # SENTIMENT\n",
    "        elif spec['type'] == 'put_call_ratio_ema':\n",
    "            pc = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not pc.empty: feat[feature_id] = pc.ewm(span=spec['params']['span'], adjust=False).mean()\n",
    "        elif spec['type'] == 'open_interest_change':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not oi.empty: feat[feature_id] = oi.pct_change(spec['params']['days'])\n",
    "        elif spec['type'] == 'volume_zscore':\n",
    "            vol = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not vol.empty: feat[feature_id] = (vol - vol.rolling(spec['params']['window']).mean()) / vol.rolling(\n",
    "                spec['params']['window']).std()\n",
    "        elif spec['type'] == 'smart_money_flag':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], spec['params']['oi_col'])).pct_change() > 0\n",
    "            ivol = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_col'])).pct_change() > 0\n",
    "            if not oi.empty and not ivol.empty: feat[feature_id] = (oi & ivol).astype(int)\n",
    "        # CORRELATION\n",
    "        elif spec['type'] == 'correlation':\n",
    "            t1, t2 = spec['assets']; p1, p2 = first_col_containing(t1, spec['params']['col']), first_col_containing(t2, spec['params']['col'])\n",
    "            if p1 and p2:\n",
    "                aligned = pd.DataFrame({'s1': safe_series(p1), 's2': safe_series(p2)}).dropna()\n",
    "                if len(aligned) > spec['params']['window']: feat[feature_id] = aligned['s1'].rolling(\n",
    "                    spec['params']['window']).corr(aligned['s2'])\n",
    "        elif spec['type'] in ['correlation_zscore', 'correlation_delta']:\n",
    "            t1, t2 = spec['assets']; price_col_name = spec['params']['col']\n",
    "            c20_id = f'corr_{t1}:{price_col_name}_{t2}:{price_col_name}_20d'\n",
    "            c60_id = f'corr_{t1}:{price_col_name}_{t2}:{price_col_name}_60d'\n",
    "            c20 = feat.get(c20_id); c60 = feat.get(c60_id)\n",
    "            if c20 is not None and c60 is not None:\n",
    "                if spec['type'] == 'correlation_zscore':\n",
    "                    feat[feature_id] = (c20 - c20.rolling(spec['params']['window']).mean()) / c20.rolling(spec['params']['window']).std()\n",
    "                else: feat[feature_id] = c20 - c60\n",
    "        elif spec['type'] == 'rolling_beta':\n",
    "            t1, t2 = spec['assets']; p1, p2 = first_col_containing(t1, spec['params']['col']), first_col_containing(t2, spec['params']['col'])\n",
    "            if p1 and p2:\n",
    "                rets = pd.DataFrame({'r1': safe_series(p1).pct_change(), 'r2': safe_series(p2).pct_change()}).dropna()\n",
    "                if len(rets) > spec['params']['window']: feat[feature_id] = rets['r1'].rolling(\n",
    "                    spec['params']['window']).cov(rets['r2']) / rets['r2'].rolling(spec['params']['window']).var()\n",
    "        # MACRO\n",
    "        elif spec['type'] == 'macro_mpi':\n",
    "            dxy, ust10 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not dxy.empty and not ust10.empty: feat[feature_id] = dxy.pct_change().rolling(\n",
    "                3).sum() + ust10.pct_change().rolling(3).sum()\n",
    "        elif spec['type'] == 'macro_fear_overdrive':\n",
    "            vix, dxy, spy = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][2], 'PX_LAST'))\n",
    "            if not vix.empty and not dxy.empty and not spy.empty: feat[feature_id] = (\n",
    "                        (vix > 20) & (dxy.pct_change() > 0) & (spy < spy.rolling(20).mean())).astype(int)\n",
    "        elif spec['type'] == 'macro_sector_rotation':\n",
    "            xlk, xle = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not xlk.empty and not xle.empty: feat[feature_id] = xlk.pct_change(5) - xle.pct_change(5)\n",
    "        elif spec['type'] == 'macro_yield_spread':\n",
    "            ust10, ust2 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not ust10.empty and not ust2.empty: feat[feature_id] = ust10 - ust2\n",
    "        elif spec['type'] == 'macro_generic_mom':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change(spec['params']['days'])\n",
    "        elif spec['type'] == 'macro_generic_chg':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change()\n",
    "        elif spec['type'] == 'macro_cpi_zscore':\n",
    "            cpi = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not cpi.empty: feat[feature_id] = (cpi - cpi.rolling(12).mean()) / cpi.rolling(12).std()\n",
    "        elif spec['type'] == 'macro_injcjc_shock':\n",
    "            injcjc = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST'))\n",
    "            if not injcjc.empty: feat[feature_id] = (injcjc.diff() > injcjc.diff().rolling(20).std() * 2).astype(int)\n",
    "        elif spec['type'] == 'macro_ffa_spread':\n",
    "            ffa, ust2 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(\n",
    "                first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not ffa.empty and not ust2.empty: feat[feature_id] = ffa - ust2\n",
    "        elif spec['type'] == 'macro_lf94truu_vol_signal':\n",
    "            vol = safe_series(first_col_containing(spec['assets'][0], 'VOLATILITY_30D'))\n",
    "            if not vol.empty: feat[feature_id] = vol / vol.rolling(60).mean()\n",
    "        # MOMENTUM / FRACTALS\n",
    "        elif spec['type'] == 'mom_div_vol':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], spec['params']['price_col']))\n",
    "            if not px.empty: feat[feature_id] = px.pct_change(spec['params']['mom_win']) / px.pct_change().rolling(spec['params']['vol_win']).std()\n",
    "        elif spec['type'] == 'bollinger_pctB':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], spec['params']['price_col']))\n",
    "            if not px.empty:\n",
    "                ma = px.rolling(spec['params']['window']).mean();\n",
    "                std = px.rolling(spec['params']['window']).std()\n",
    "                feat[feature_id] = (px - (ma - 2 * std)) / (4 * std)\n",
    "        elif spec['type'] == 'fractional_differencing':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], spec['params']['price_col']))\n",
    "            if not px.empty and len(px) > spec['params']['window']: feat[feature_id] = frac_diff(px, d=spec['params']['d'], window=spec['params']['window'])\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate feature '{feature_id}': {e}\")\n",
    "feat = feat.shift(1)  # Shift all features to prevent lookahead bias\n",
    "print(f\"Calculated {feat.shape[1]} feature series.\")\n",
    "\n",
    "\n",
    "# --- 3. Define Primitive Signals from Features (with multiple condition types) ---\n",
    "print('--- Defining Primitive Signals ---')\n",
    "primitive_signals = []\n",
    "signal_series = {}\n",
    "signal_id_counter = 0\n",
    "\n",
    "for feature_id in feat.columns:\n",
    "    s = feat[feature_id].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty or s.std() == 0: continue\n",
    "\n",
    "    # Percentile-based signals\n",
    "    for op, val in [('>', 0.8), ('<', 0.2)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\";\n",
    "        signal_id_counter += 1\n",
    "        primitive_signals.append(\n",
    "            {'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'percentile', 'operator': op,\n",
    "             'value': val})\n",
    "        signal_series[sig_id] = s.rank(pct=True).apply(lambda x: x > val if op == '>' else x < val)\n",
    "\n",
    "    # Z-score signals\n",
    "    rolling_std = s.rolling(60).std()\n",
    "    valid_std_mask = rolling_std > 1e-9\n",
    "    z = pd.Series(np.nan, index=s.index)\n",
    "    z[valid_std_mask] = (s - s.rolling(60).mean())[valid_std_mask] / rolling_std[valid_std_mask]\n",
    "    for op, val in [('>', 1.5), ('<', -1.5)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\";\n",
    "        signal_id_counter += 1\n",
    "        primitive_signals.append(\n",
    "            {'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'z_score', 'operator': op, 'value': val})\n",
    "        signal_series[sig_id] = z.apply(lambda x: x > val if op == '>' else x < val)\n",
    "\n",
    "print(f\"Defined {len(primitive_signals)} primitive signals.\")\n",
    "\n",
    "# --- 4. Generate Setups, Evaluate, and Output ---\n",
    "print('--- Generating Candidate Setups ---')\n",
    "all_candidate_setups = []\n",
    "setup_id_counter = 1\n",
    "for k in SETUP_LENGTHS_TO_EXPLORE:\n",
    "    signal_ids = [s['signal_id'] for s in primitive_signals]\n",
    "    if len(signal_ids) < k: continue\n",
    "\n",
    "    # Use random sampling to keep computation manageable\n",
    "    combinations_to_test = [random.sample(signal_ids, k) for _ in range(NUM_RANDOM_SETUPS_TO_SAMPLE)]\n",
    "\n",
    "    for sig_id_list in combinations_to_test:\n",
    "        try:\n",
    "            mask = functools.reduce(lambda a, b: a & b, [signal_series[sid] for sid in sig_id_list])\n",
    "            if mask.sum() >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "                # For each signal_id, find its full definition\n",
    "                signal_definitions = [p for p in primitive_signals if p['signal_id'] in sig_id_list]\n",
    "                all_candidate_setups.append({\n",
    "                    'id': f'S{setup_id_counter:04d}',\n",
    "                    'signal_definitions': signal_definitions\n",
    "                })\n",
    "                setup_id_counter += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "print(f\"Generated {len(all_candidate_setups)} candidate setups.\")\n",
    "\n",
    "# Prepare Returns for Evaluation\n",
    "price_cols_for_returns = [first_col_containing(t, 'PX_LAST') for t in TRADABLE_TICKERS if\n",
    "                          first_col_containing(t, 'PX_LAST')]\n",
    "prices = raw[price_cols_for_returns].copy()\n",
    "returns = {h: prices.pct_change(h).shift(-h) for h in [1, 3, 5, 10, 21]}\n",
    "\n",
    "\n",
    "# --- NEW: Plain-English Description Generator ---\n",
    "def generate_english_description(setup_id, signal_defs, feature_specs_list):\n",
    "    \"\"\"Generates a human-readable description of a setup.\"\"\"\n",
    "    clauses = []\n",
    "    # This function now uses the new transparent feature names, making its output more descriptive\n",
    "    for s_def in signal_defs:\n",
    "        feat_name = next(\n",
    "            (f_spec['display_name'] for f_spec in feature_specs_list if f_spec['unique_id'] == s_def['feature_id']),\n",
    "            s_def['feature_id'])\n",
    "        if s_def['condition_type'] == 'percentile':\n",
    "            level = \"is very high\" if s_def['operator'] == '>' else \"is very low\"\n",
    "            clauses.append(f\"{feat_name} {level}\")\n",
    "        else:  # z_score\n",
    "            level = \"surges unexpectedly\" if s_def['operator'] == '>' else \"drops sharply\"\n",
    "            clauses.append(f\"{feat_name} {level}\")\n",
    "\n",
    "    description = f\"When {clauses[0]}\"\n",
    "    if len(clauses) > 1:\n",
    "        description += f\" and {' and '.join(clauses[1:])}\"\n",
    "\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs)\n",
    "    bias = 'a bullish' if direction_score > 0 else 'a bearish' if direction_score < 0 else 'an uncertain'\n",
    "    description += f\", it may indicate {bias} outlook.\"\n",
    "\n",
    "    # Explained description is no longer necessary with transparent feature names\n",
    "    return {'setup_id': setup_id, 'description': description, 'explained_description': \"DEPRECATED\"}\n",
    "\n",
    "\n",
    "# --- Parallel Setup Evaluation Function (UPDATED) ---\n",
    "def evaluate_one_setup(setup):\n",
    "    \"\"\"Helper function to evaluate a single setup for parallel processing.\"\"\"\n",
    "    sid, signal_defs = setup['id'], setup['signal_definitions']\n",
    "    try:\n",
    "        mask = functools.reduce(lambda a, b: a & b, [signal_series[s['signal_id']] for s in signal_defs])\n",
    "        dates = mask[mask].index\n",
    "    except (KeyError, TypeError):\n",
    "        return [], [], None\n",
    "\n",
    "    if len(dates) < MIN_INITIAL_SUPPORT_FILTER: return [], [], None\n",
    "\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs)\n",
    "    if direction_score == 0:\n",
    "        return [], [], None\n",
    "\n",
    "    entry_direction = 'long' if direction_score > 0 else 'short'\n",
    "\n",
    "    feature_types = [spec['type'] for sig_def in signal_defs for spec in feature_specs if\n",
    "                     spec['unique_id'] == sig_def['feature_id']]\n",
    "    dominant_signal_type = max(set(feature_types), key=feature_types.count) if feature_types else 'unknown'\n",
    "\n",
    "    human_readable_conds = []\n",
    "    for s_def in signal_defs:\n",
    "        feat_name = next(\n",
    "            (f_spec['display_name'] for f_spec in feature_specs if f_spec['unique_id'] == s_def['feature_id']),\n",
    "            s_def['feature_id'])\n",
    "        if s_def['condition_type'] == 'percentile':\n",
    "            cond_str = f\"{s_def['operator']} {s_def['value'] * 100:.0f}th percentile\"\n",
    "        else:  # z_score\n",
    "            cond_str = f\"z-score {s_def['operator']} {s_def['value']}\"\n",
    "        human_readable_conds.append(f\"{feat_name} {cond_str}\")\n",
    "\n",
    "    description_record = generate_english_description(sid, signal_defs, feature_specs)\n",
    "\n",
    "    summary_rows_for_setup = []\n",
    "    trade_ledger_rows = []\n",
    "\n",
    "    for tk_col in price_cols_for_returns:\n",
    "        tk_symbol = next((ticker for ticker in TRADABLE_TICKERS if tk_col.startswith(ticker)), None)\n",
    "        if tk_symbol is None: continue\n",
    "\n",
    "        summary_row = {\n",
    "            'setup_id': sid,\n",
    "            'target_ticker': tk_symbol,\n",
    "            'feature_conditions': json.dumps(human_readable_conds),\n",
    "            'support': len(dates),\n",
    "            'entry_direction': entry_direction,\n",
    "            'dominant_signal_type': dominant_signal_type,\n",
    "            'first_trigger_date': dates.min(),\n",
    "            'last_trigger_date': dates.max()\n",
    "        }\n",
    "\n",
    "        for h in [3, 5, 10, 21]:\n",
    "            r_ticker = returns[h][tk_col].reindex(dates)\n",
    "            if h == 3: summary_row['accuracy_3d'] = 0.0 if r_ticker.empty else (r_ticker > 0).mean()\n",
    "            if h == 5: summary_row['avg_return_5d'] = 0.0 if r_ticker.empty else r_ticker.mean()\n",
    "            if h == 21: summary_row['hit_rate_21d'] = 0.0 if r_ticker.empty else (r_ticker > 0).mean()\n",
    "            if not r_ticker.empty and r_ticker.std() > 1e-6 and len(r_ticker) > 5:\n",
    "                summary_row[f'sharpe_{h}d'], _, _ = block_bootstrap_sharpe(r_ticker, block_size=h)\n",
    "            else:\n",
    "                summary_row[f'sharpe_{h}d'] = 0.0\n",
    "\n",
    "        trigger_df_for_ticker = pd.DataFrame([\n",
    "            {'date': d, 'underlying_entry_px': raw[tk_col].get(d),\n",
    "             **{f'underlying_exit_px_{h_opt}d': raw[tk_col].reindex([d + pd.Timedelta(days=h_opt)], method='nearest',\n",
    "                                                                    tolerance=pd.Timedelta(days=3)).iloc[0]\n",
    "             if not raw[tk_col].reindex([d + pd.Timedelta(days=h_opt)], method='nearest',\n",
    "                                        tolerance=pd.Timedelta(days=3)).empty else np.nan\n",
    "                for h_opt in OPTION_SIM_HORIZONS_DAYS}}\n",
    "            for d in dates\n",
    "        ])\n",
    "        trigger_df_for_ticker.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        for h_opt in OPTION_SIM_HORIZONS_DAYS:\n",
    "            summary_row[f'avg_underlying_entry_px_{h_opt}d'] = trigger_df_for_ticker['underlying_entry_px'].mean()\n",
    "            summary_row[f'avg_underlying_exit_px_{h_opt}d'] = trigger_df_for_ticker[\n",
    "                f'underlying_exit_px_{h_opt}d'].mean()\n",
    "\n",
    "            ivol_col = (\n",
    "                    first_col_containing(tk_symbol, '30_Day_Call_Implied_Volatility') or\n",
    "                    first_col_containing(tk_symbol, '60_Day_Call_Implied_Volatility') or\n",
    "                    first_col_containing(tk_symbol, '10_Day_Call_Implied_Volatility') or\n",
    "                    first_col_containing(tk_symbol, 'IVOL_SIGMA') or\n",
    "                    first_col_containing(tk_symbol, 'CALL_IMP_VOL_30D')\n",
    "            )\n",
    "            ivol_series = raw[ivol_col].reindex(dates) if ivol_col else pd.Series(np.nan, index=dates)\n",
    "\n",
    "            pnl_results = []\n",
    "            for i, d in enumerate(dates):\n",
    "                pnl_detail = simulate_option_pnl_detailed(\n",
    "                    trigger_df_for_ticker.loc[i, 'underlying_entry_px'],\n",
    "                    trigger_df_for_ticker.loc[i, f'underlying_exit_px_{h_opt}d'],\n",
    "                    ivol_series.iloc[i] if not ivol_series.empty else np.nan,\n",
    "                    h_opt,\n",
    "                    entry_direction\n",
    "                )\n",
    "                pnl_results.append(pnl_detail)\n",
    "\n",
    "                ledger_record = {\n",
    "                    'setup_id': sid,\n",
    "                    'trigger_date': d,\n",
    "                    'target_ticker': tk_symbol,\n",
    "                    'horizon_days': h_opt,\n",
    "                    'entry_direction': entry_direction,\n",
    "                    'underlying_entry_px': trigger_df_for_ticker.loc[i, 'underlying_entry_px'],\n",
    "                    'ivol_at_entry': ivol_series.iloc[i] if not ivol_series.empty else np.nan,\n",
    "                    **pnl_detail\n",
    "                }\n",
    "                trade_ledger_rows.append(ledger_record)\n",
    "\n",
    "            summary_row[f'avg_option_pnl_dollars_{h_opt}d'] = pd.Series([p['pnl_dollars'] for p in pnl_results]).mean()\n",
    "\n",
    "        summary_rows_for_setup.append(summary_row)\n",
    "\n",
    "    return summary_rows_for_setup, trade_ledger_rows, description_record\n",
    "\n",
    "\n",
    "# --- Parallel processing with joblib ---\n",
    "print(f\"\\n--- Starting Parallel Evaluation of {len(all_candidate_setups)} Setups ---\")\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup) for setup in all_candidate_setups)\n",
    "summary_rows, description_records = [], []\n",
    "all_trade_ledger_rows = []\n",
    "for perf_list, trade_list, desc in results:\n",
    "    if perf_list: summary_rows.extend(perf_list)\n",
    "    if trade_list: all_trade_ledger_rows.extend(trade_list)\n",
    "    if desc: description_records.append(desc)\n",
    "\n",
    "# --- Final Output Generation ---\n",
    "print('--- Generating Final Output Files ---')\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "description_df = pd.DataFrame(description_records)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    summary_df['setup_duration_days'] = (pd.to_datetime(summary_df['last_trigger_date']) - pd.to_datetime(\n",
    "        summary_df['first_trigger_date'])).dt.days\n",
    "    summary_df['avg_trigger_frequency_per_day'] = summary_df['support'] / summary_df['setup_duration_days'].replace(0,\n",
    "                                                                                                                    np.nan)\n",
    "\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    summary_df.to_csv('setup_results_summary.csv', index=False)\n",
    "    print(\"Saved 'setup_results_summary.csv'\")\n",
    "\n",
    "if not description_df.empty:\n",
    "    description_df.to_csv('setup_descriptions.csv', index=False)\n",
    "    print(\"Saved 'setup_descriptions.csv'\")\n",
    "\n",
    "if all_trade_ledger_rows:\n",
    "    trade_ledger_df = pd.DataFrame(all_trade_ledger_rows)\n",
    "    # Reordering columns to place new ones logically\n",
    "    cols = list(trade_ledger_df.columns)\n",
    "    if 'Underlying_Exit_Price' in cols and 'underlying_entry_px' in cols:\n",
    "      # Ensure both columns exist before trying to reorder\n",
    "      if 'Underlying_Exit_Price' in cols:\n",
    "          cols.insert(cols.index('underlying_entry_px') + 1, cols.pop(cols.index('Underlying_Exit_Price')))\n",
    "      if 'Return_Underlying' in cols:\n",
    "          cols.insert(cols.index('Underlying_Exit_Price') + 1, cols.pop(cols.index('Return_Underlying')))\n",
    "      trade_ledger_df = trade_ledger_df[cols]\n",
    "    trade_ledger_df.to_csv('trade_ledger.csv', index=False)\n",
    "    print(\"Saved 'trade_ledger.csv'\")\n",
    "\n",
    "top_setups = summary_df.sort_values('sharpe_21d', ascending=False).head(\n",
    "    20) if 'sharpe_21d' in summary_df.columns else pd.DataFrame()\n",
    "top_setups.to_json('top_setups.json', orient='records', indent=2)\n",
    "print(\"Saved 'top_setups.json'\")\n",
    "\n",
    "print('\\nDiscovery complete. All original features were migrated to the new architecture.')\n",
    "print(\"\\nTop Setups by Sharpe Ratio (21d):\")\n",
    "# Display new sharpe columns\n",
    "display_cols = ['setup_id', 'target_ticker', 'support', 'sharpe_3d', 'sharpe_21d', 'feature_conditions']\n",
    "print(top_setups[[c for c in display_cols if c in top_setups.columns]].head())"
   ],
   "id": "797e760512f3bf2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw workbooksâ€¦\n",
      "An unexpected error occurred during Excel loading of 'Macro_tickers_no_nan_cols.xlsx': 'Date'\n",
      "Raw shape: (1172, 403)\n",
      "\n",
      "Identified all relevant prefixes/tickers for feature engineering: 32\n",
      "\n",
      "--- Defining ALL Feature Specifications ---\n",
      "Defined 2980 total feature specifications.\n",
      "--- Calculating All Features ---\n",
      "Calculated 1100 feature series.\n",
      "--- Defining Primitive Signals ---\n",
      "Defined 3236 primitive signals.\n",
      "--- Generating Candidate Setups ---\n",
      "Generated 108 candidate setups.\n",
      "\n",
      "--- Starting Parallel Evaluation of 108 Setups ---\n",
      "--- Generating Final Output Files ---\n",
      "Saved 'setup_results_summary.csv'\n",
      "Saved 'setup_descriptions.csv'\n",
      "Saved 'trade_ledger.csv'\n",
      "Saved 'top_setups.json'\n",
      "\n",
      "Discovery complete. All original features were migrated to the new architecture.\n",
      "\n",
      "Top Setups by Sharpe Ratio (21d):\n",
      "    setup_id   target_ticker  support  sharpe_3d  sharpe_21d  \\\n",
      "605    S0086   GLD US Equity       36   7.808392  105.246737   \n",
      "157    S0027   SPY US Equity       30   5.851339   50.757663   \n",
      "602    S0086   XLE US Equity       36  13.938485   38.297606   \n",
      "167    S0027  AAPL US Equity       30   6.757742   35.024523   \n",
      "488    S0075   GLD US Equity       40   9.557283   30.703334   \n",
      "\n",
      "                                    feature_conditions  \n",
      "605  [\"corr(ARKK US Equity:PX_LAST, XLK US Equity:P...  \n",
      "157  [\"corr(NVDA US Equity:PX_LAST, TSLA US Equity:...  \n",
      "602  [\"corr(ARKK US Equity:PX_LAST, XLK US Equity:P...  \n",
      "167  [\"corr(NVDA US Equity:PX_LAST, TSLA US Equity:...  \n",
      "488  [\"corr(ARKK US Equity:PX_LAST, NVDA US Equity:...  \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T18:42:06.022392Z",
     "start_time": "2025-08-04T18:40:57.800636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Engineering + Discovery Engine with FULLY Migrated Structured Features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import linregress\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "# Define the explicit list of tradable tickers\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "]\n",
    "\n",
    "# --- ADDITION: Define list of macro tickers to ensure their inclusion in feature generation ---\n",
    "MACRO_TICKERS = [\n",
    "    'DXY Curncy', 'USGG10YR Index', 'USGG2YR Index', 'CPI YOY Index',\n",
    "    'INJCJC Index', 'FFA Comdty', 'LF94TRUU Index', 'CPI CHNG Index',\n",
    "    'NFP TCH Index', 'JOBS US Equity', 'CTII10 Govt', 'USSW10 Curncy',\n",
    "    'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index',\n",
    "    'SPCS20SM Index', 'CONSSENT Index', 'CO1 Comdty'\n",
    "]\n",
    "\n",
    "# Define the file paths\n",
    "MAIN_DATA_FILE = 'All_Tickers copy.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_no_nan_cols.xlsx'\n",
    "\n",
    "# Setup Generation Configuration\n",
    "NUM_RANDOM_SETUPS_TO_SAMPLE = 100\n",
    "SETUP_LENGTHS_TO_EXPLORE = [2]\n",
    "MIN_INITIAL_SUPPORT_FILTER = 5\n",
    "\n",
    "# Option Simulation Configuration\n",
    "OPTION_SIM_HORIZONS_DAYS = [1, 3, 10, 21]\n",
    "RISK_FREE_RATE = 0.01\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooksâ€¦')\n",
    "\n",
    "\n",
    "# --- Custom Data Loading Function (Unchanged) ---\n",
    "def load_and_merge_excel(file_path, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "        for sh_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name)\n",
    "\n",
    "            if 'Dates' in df.columns and 'Date' not in df.columns:\n",
    "                df.rename(columns={'Dates': 'Date'}, inplace=True)\n",
    "\n",
    "            if 'Date' not in df.columns:\n",
    "                print(f\"Warning: Sheet '{sh_name}' in '{file_path}' is missing a 'Date'/'Dates' column. Skipping sheet.\")\n",
    "                continue\n",
    "\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                df = df.loc[:,~df.columns.duplicated()]\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found.\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df\n",
    "\n",
    "\n",
    "# Load main and macro data\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE)\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "# Final cleaning and indexing\n",
    "if not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "    if 'Date' in raw.columns:\n",
    "        raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "        raw = raw.drop_duplicates(subset=['Date'], keep='last')\n",
    "        raw.set_index('Date', inplace=True)\n",
    "        raw.index = pd.to_datetime(raw.index)\n",
    "        raw.sort_index(inplace=True)\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "\n",
    "# --- Dynamic Ticker Identification (Unchanged) ---\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c])))\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total', '30', '10',\n",
    "                           '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC', 'NFP', 'JOBS', 'CPI', 'CTII10',\n",
    "                           'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT', 'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM',\n",
    "                           'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes + MACRO_TICKERS)))\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {len(all_tickers)}')\n",
    "\n",
    "\n",
    "# --- Helper functions (Unchanged) ---\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    \"\"\"Finds the first column name in raw that matches the pattern 'ticker_full_name_substr'.\"\"\"\n",
    "    if substr == 'PX_LAST':\n",
    "        for potential_col in [f\"{ticker_full_name}_Last_Price_PX_LAST\", f\"{ticker_full_name}_PX_LAST\"]:\n",
    "            if potential_col in raw.columns: return potential_col\n",
    "    for c in raw.columns:\n",
    "        if c.startswith(ticker_full_name) and substr in c: return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def safe_series(col_name):\n",
    "    \"\"\"Returns a column as a Series, or an empty Series if column does not exist.\"\"\"\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "\n",
    "def frac_diff(series, d=0.5, window=100):\n",
    "    \"\"\"Computes fractionally differenced series.\"\"\"\n",
    "    weights = [1.]\n",
    "    for k in range(1, len(series)): weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1])\n",
    "    output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(window, len(series)):\n",
    "        subset = series.iloc[i - window + 1: i + 1]\n",
    "        if len(subset) == len(weights[-window:]):\n",
    "            output.iloc[i] = np.dot(weights[-window:], subset)\n",
    "    return output.dropna()\n",
    "\n",
    "\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=1000, annualize=True, trading_days_per_year=252):\n",
    "    \"\"\"Calculates the Sharpe Ratio using block bootstrapping.\"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2: return 0.0, 0.0, 0.0\n",
    "    blocks = [returns_series.iloc[i: i + block_size] for i in range(0, len(returns_series), block_size) if\n",
    "              not returns_series.iloc[i: i + block_size].empty]\n",
    "    if not blocks: return 0.0, 0.0, 0.0\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size))\n",
    "    sharpes = []\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = [blocks[i] for i in np.random.choice(len(blocks), n_blocks_to_sample, replace=True)]\n",
    "        resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)]\n",
    "        if resampled_returns.std() > 1e-9:\n",
    "            sharpes.append((resampled_returns.mean() / resampled_returns.std()) * (\n",
    "                np.sqrt(trading_days_per_year) if annualize else 1))\n",
    "        else:\n",
    "            sharpes.append(0.0)\n",
    "    if not sharpes: return 0.0, 0.0, 0.0\n",
    "    return np.median(sharpes), np.percentile(sharpes, 5), np.percentile(sharpes, 95)\n",
    "\n",
    "\n",
    "# --- Option Simulation Helpers (UPDATED) ---\n",
    "def estimate_atm_premium(price, ivol, days, option_type):\n",
    "    \"\"\"Placeholder for a real option pricing model like Black-Scholes.\"\"\"\n",
    "    T = days / 365.25\n",
    "    if T <= 0 or price <= 0 or ivol <= 0: return 0\n",
    "    return 0.4 * price * ivol * np.sqrt(T)\n",
    "\n",
    "def simulate_option_pnl_detailed(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    \"\"\"Simulates PnL and returns a dictionary with all components for verification.\"\"\"\n",
    "    underlying_return = (future_price - current_price) / current_price if current_price and pd.notna(current_price) and pd.notna(future_price) else np.nan\n",
    "    nan_result = {\n",
    "        'pnl_per_share': np.nan, 'option_type': None, 'strike_price': np.nan,\n",
    "        'entry_premium': np.nan,\n",
    "        'exit_value': np.nan, 'pnl_dollars': np.nan, 'skipped_reason': 'None',\n",
    "        'Underlying_Exit_Price': future_price if pd.notna(future_price) else np.nan,\n",
    "        'Return_Underlying': underlying_return,\n",
    "    }\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Price'\n",
    "        return nan_result\n",
    "    if pd.isna(ivol_at_entry) or ivol_at_entry <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid IVOL'\n",
    "        return nan_result\n",
    "    if pd.isna(future_price):\n",
    "        nan_result['skipped_reason'] = 'Missing Future Price'\n",
    "        return nan_result\n",
    "    if entry_direction not in ['long', 'short']:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Direction'\n",
    "        return nan_result\n",
    "    scaled_ivol = ivol_at_entry / 100.0 if ivol_at_entry > 1.0 else ivol_at_entry\n",
    "    strike_price = current_price\n",
    "    option_type = 'call' if entry_direction == 'long' else 'put'\n",
    "    entry_premium = estimate_atm_premium(current_price, scaled_ivol, horizon_days, option_type)\n",
    "    if option_type == 'call':\n",
    "        exit_value = max(future_price - strike_price, 0)\n",
    "    else:\n",
    "        exit_value = max(strike_price - future_price, 0)\n",
    "    pnl_per_share = exit_value - entry_premium\n",
    "    pnl_dollars = pnl_per_share * 100\n",
    "    return {\n",
    "        'pnl_per_share': pnl_per_share, 'option_type': option_type, 'strike_price': strike_price,\n",
    "        'entry_premium': entry_premium, 'exit_value': exit_value, 'pnl_dollars': pnl_dollars,\n",
    "        'skipped_reason': 'None', 'Underlying_Exit_Price': future_price, 'Return_Underlying': underlying_return,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- ARCHITECTURE REWRITE: Structured Feature & Signal Generation ---\n",
    "print('\\n--- Defining ALL Feature Specifications ---')\n",
    "feature_specs = []\n",
    "\n",
    "# Volatility Features\n",
    "for ticker in all_tickers:\n",
    "    f60 = '60_Day_Call_Implied_Volatility'; f10 = '10_Day_Call_Implied_Volatility'\n",
    "    feature_specs.append({'type': 'ivol_term_structure', 'assets': [ticker], 'params': {'f_long':f60, 'f_short':f10},\n",
    "                          'unique_id': f'term_structure_{f60}-{f10}__{ticker}', 'display_name': f\"diff({f60}, {f10})__{ticker}\"})\n",
    "    put50 = '1st_Month_Put_Imp_Vol_50_Delta'; call40 = '1st_Month_Call_Imp_Vol_40_Delta'\n",
    "    feature_specs.append({'type': 'ivol_skew', 'assets': [ticker], 'params': {'put':put50, 'call':call40},\n",
    "                          'unique_id': f'skew_{put50}-{call40}__{ticker}', 'display_name': f\"diff({put50}, {call40})__{ticker}\"})\n",
    "    for suffix in ['IVOL_SIGMA', 'CALL_IMP_VOL_30D', 'PUT_IMP_VOL_30D']:\n",
    "        feature_specs.append({'type': 'ivol_shock', 'assets': [ticker], 'params': {'ivol_suffix': suffix, 'window': 30},\n",
    "                              'unique_id': f'zscore_{suffix}_30d__{ticker}', 'display_name': f\"zscore_{suffix}_30d__{ticker}\"})\n",
    "        feature_specs.append({'type': 'ivol_div_volume', 'assets': [ticker], 'params': {'ivol_suffix': suffix, 'vol_suffix':'VOLUME'},\n",
    "                              'unique_id': f'div_{suffix}_by_VOLUME__{ticker}', 'display_name': f\"div({suffix}, VOLUME)__{ticker}\"})\n",
    "\n",
    "# Deriv Flow & Sentiment Features\n",
    "for ticker in all_tickers:\n",
    "    pc_ratio_col = 'PUT_CALL_VOLUME_RATIO_CUR_DAY'\n",
    "    feature_specs.append({'type': 'put_call_ratio_ema', 'assets': [ticker], 'params': {'span': 5, 'col': pc_ratio_col},\n",
    "                          'unique_id': f'ema5_{pc_ratio_col}__{ticker}', 'display_name': f\"ema5_{pc_ratio_col}__{ticker}\"})\n",
    "    oi_col = 'OPEN_INT_TOTAL_CALL'\n",
    "    feature_specs.append({'type': 'open_interest_change', 'assets': [ticker], 'params': {'days': 3, 'col': oi_col},\n",
    "                          'unique_id': f'pct_change_{oi_col}_3d__{ticker}', 'display_name': f\"pct_change_{oi_col}_3d__{ticker}\"})\n",
    "    vol_col = 'Volume_-Realtime_VOLUME'\n",
    "    feature_specs.append({'type': 'volume_zscore', 'assets': [ticker], 'params': {'window': 30, 'col': vol_col},\n",
    "                          'unique_id': f'zscore_{vol_col}_30d__{ticker}', 'display_name': f\"zscore_{vol_col}_30d__{ticker}\"})\n",
    "    sm_oi = 'OPEN_INT_TOTAL_CALL'; sm_ivol='10_Day_Call_Implied_Volatility'\n",
    "    feature_specs.append({'type': 'smart_money_flag', 'assets': [ticker], 'params': {'oi_col': sm_oi, 'ivol_col': sm_ivol},\n",
    "                          'unique_id': f'smart_money_{sm_oi}_{sm_ivol}__{ticker}', 'display_name': f\"smart_money(pct_change({sm_oi}) > 0 AND pct_change({sm_ivol}) > 0)__{ticker}\"})\n",
    "\n",
    "# ADDITION: Generic Z-Score feature needed for sequential patterns\n",
    "for ticker in all_tickers:\n",
    "    for col in ['PX_LAST', 'IVOL_SIGMA', 'Volume_-Realtime_VOLUME']:\n",
    "        for window in [30, 60]:\n",
    "            feature_specs.append({'type': 'generic_zscore', 'assets': [ticker], 'params': {'col': col, 'window': window},\n",
    "                                  'unique_id': f'zscore_{col}_{window}d__{ticker}', 'display_name': f\"zscore({col}, {window}d)__{ticker}\"})\n",
    "\n",
    "# Cross-Asset Correlation Features\n",
    "price_col = 'PX_LAST'\n",
    "correlation_pairs = list(set(itertools.combinations(all_tickers, 2)))\n",
    "for t1, t2 in correlation_pairs:\n",
    "    for window in [20, 60]:\n",
    "        feature_specs.append({'type': 'correlation', 'assets': [t1, t2], 'params': {'window': window, 'col': price_col},\n",
    "                              'unique_id': f'corr_{t1}:{price_col}_{t2}:{price_col}_{window}d', 'display_name': f\"corr({t1}:{price_col}, {t2}:{price_col}, {window}d)\"})\n",
    "    feature_specs.append({'type': 'correlation_zscore', 'assets': [t1, t2], 'params':{'col':price_col, 'window':60},\n",
    "                          'unique_id': f'zscore_corr20d_{t1}:{price_col}_{t2}:{price_col}_60d', 'display_name': f\"zscore_corr(20d)({t1}:{price_col}, {t2}:{price_col}, 60d)\"})\n",
    "    feature_specs.append({'type': 'correlation_delta', 'assets': [t1, t2], 'params':{'col':price_col},\n",
    "                          'unique_id': f'corr_delta_{t1}:{price_col}_{t2}:{price_col}', 'display_name': f\"corr_delta(20d-60d)({t1}:{price_col}, {t2}:{price_col})\"})\n",
    "    feature_specs.append({'type': 'rolling_beta', 'assets': [t1, t2], 'params': {'window': 60, 'col':price_col},\n",
    "                          'unique_id': f'beta_{t1}:{price_col}_{t2}:{price_col}_60d', 'display_name': f\"beta({t1}:{price_col}, {t2}:{price_col}, 60d)\"})\n",
    "\n",
    "# --- ACTION 2: ADD NEW ADVANCED CORRELATIONS ---\n",
    "adv_corr_defs = [\n",
    "    {'t1': 'QQQ US Equity', 'f1': 'IVOL_SIGMA', 't2': 'SPY US Equity', 'f2': 'IVOL_SIGMA', 'win': 30},\n",
    "    {'t1': 'TSLA US Equity', 'f1': 'Volume_-Realtime_VOLUME', 't2': 'VIX Index', 'f2': 'IVOL_SIGMA', 'win': 20},\n",
    "    {'t1': 'CO1 Comdty', 'f1': 'PX_LAST', 't2': 'XLE US Equity', 'f2': 'IVOL_SIGMA', 'win': 30},\n",
    "    {'t1': 'USGG10YR Index', 'f1': 'PX_LAST', 't2': 'XLF US Equity', 'f2': 'IVOL_SIGMA', 'win': 30}\n",
    "]\n",
    "for d in adv_corr_defs:\n",
    "    feature_specs.append({\n",
    "        'type': 'advanced_correlation', 'assets': [d['t1'], d['t2']],\n",
    "        'params': {'window': d['win'], 'col1': d['f1'], 'col2': d['f2']},\n",
    "        'unique_id': f\"corr_{d['t1']}:{d['f1']}_{d['t2']}:{d['f2']}_{d['win']}d\",\n",
    "        'display_name': f\"corr({d['t1']}:{d['f1']}, {d['t2']}:{d['f2']}, {d['win']}d)\"\n",
    "    })\n",
    "\n",
    "# Macro Features\n",
    "feature_specs.extend([\n",
    "    {'type': 'macro_mpi', 'assets': ['DXY Curncy', 'USGG10YR Index'], 'unique_id': 'macro_mpi', 'display_name': 'Macro Pressure Index'},\n",
    "    {'type': 'macro_fear_overdrive', 'assets': ['VIX Index', 'DXY Curncy', 'SPY US Equity'], 'unique_id': 'macro_fear_overdrive', 'display_name': 'Fear Overdrive'},\n",
    "    {'type': 'macro_sector_rotation', 'assets': ['XLK US Equity', 'XLE US Equity'], 'unique_id': 'macro_xlk_xle_rotation', 'display_name': 'Sector Rotation (XLK-XLE)'},\n",
    "    {'type': 'macro_yield_spread', 'assets': ['USGG10YR Index', 'USGG2YR Index'], 'unique_id': 'macro_10y2y_spread', 'display_name': 'Yield Spread (10Y-2Y)'},\n",
    "    {'type': 'macro_cpi_zscore', 'assets': ['CPI YOY Index'], 'unique_id': 'macro_cpi_z', 'display_name': 'CPI Z-Score'},\n",
    "    {'type': 'macro_injcjc_shock', 'assets': ['INJCJC Index'], 'unique_id': 'macro_jobless_claims_shock', 'display_name': 'Jobless Claims Shock'},\n",
    "    {'type': 'macro_ffa_spread', 'assets': ['FFA Comdty', 'USGG2YR Index'], 'unique_id': 'macro_ffa_spread', 'display_name': 'Fed Funds Spread'},\n",
    "    {'type': 'macro_lf94truu_vol_signal', 'assets': ['LF94TRUU Index'], 'unique_id': 'macro_hyg_vol_signal', 'display_name': 'HYG Vol Signal'}\n",
    "])\n",
    "for t in ['CPI YOY Index', 'CPI CHNG Index', 'NFP TCH Index', 'JOBS US Equity']:\n",
    "    feature_specs.append({'type': 'macro_generic_mom', 'assets': [t], 'params': {'days': 3}, 'unique_id': f'macro_mom3_{t}', 'display_name': f'Macro Mom3d({t})'})\n",
    "for t in ['CTII10 Govt', 'USSW10 Curncy', 'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index', 'SPCS20SM Index', 'CONSSENT Index']:\n",
    "    feature_specs.append({'type': 'macro_generic_chg', 'assets': [t], 'unique_id': f'macro_chg_{t}', 'display_name': f'Macro Chg({t})'})\n",
    "\n",
    "# Momentum / Volatility Fractal Features\n",
    "for ticker in all_tickers:\n",
    "    feature_specs.append({'type': 'mom_div_vol', 'assets': [ticker], 'params': {'price_col':price_col, 'mom_win':5, 'vol_win':20},\n",
    "                          'unique_id': f'mom_div_vol_{price_col}_5d_20d__{ticker}', 'display_name': f\"mom_div_vol({price_col}, 5d, 20d)__{ticker}\"})\n",
    "    feature_specs.append({'type': 'bollinger_pctB', 'assets': [ticker], 'params': {'window': 20, 'price_col':price_col},\n",
    "                          'unique_id': f'pctB_{price_col}_20d__{ticker}', 'display_name': f\"%B({price_col}, 20d)__{ticker}\"})\n",
    "    feature_specs.append({'type': 'fractional_differencing', 'assets': [ticker], 'params': {'d': 0.5, 'window': 100, 'price_col':price_col},\n",
    "                          'unique_id': f'frac_diff_{price_col}_d0.5_100w__{ticker}', 'display_name': f\"frac_diff({price_col}, d=0.5, win=100)__{ticker}\"})\n",
    "\n",
    "# --- ACTION 3: ADD MARKET REGIME AND INTERACTION FEATURES ---\n",
    "# Define the Regime feature spec\n",
    "feature_specs.append({'type': 'regime_filter', 'assets': ['VIX Index'], 'params': {'threshold': 25, 'col': 'PX_LAST'},\n",
    "                      'unique_id': 'REGIME_IS_HIGH_VOL', 'display_name': 'REGIME_IS_HIGH_VOL (VIX > 25)'})\n",
    "\n",
    "# Define the Interaction feature spec\n",
    "feature_specs.append({'type': 'interaction', 'assets': [], # Not asset specific, combines other features\n",
    "                      'params': {'feature1': 'zscore_IVOL_SIGMA_30d__AAPL US Equity', 'feature2': 'REGIME_IS_HIGH_VOL'},\n",
    "                      'unique_id': 'zscore_IVOL_SIGMA_30d__AAPL US Equity_IN_HIGH_VOL',\n",
    "                      'display_name': 'zscore(IVOL_SIGMA, 30d)__AAPL US Equity IN_HIGH_VOL'})\n",
    "\n",
    "print(f\"Defined {len(feature_specs)} total feature specifications.\")\n",
    "\n",
    "# --- 2. Calculate Features Based on Specifications (Complete Calculation Engine) ---\n",
    "print('--- Calculating All Features ---')\n",
    "feat = pd.DataFrame(index=raw.index)\n",
    "for spec in feature_specs:\n",
    "    feature_id = spec['unique_id']\n",
    "    try:\n",
    "        # VOLATILITY\n",
    "        if spec['type'] == 'ivol_term_structure':\n",
    "            ivol60 = safe_series(first_col_containing(spec['assets'][0], spec['params']['f_long']))\n",
    "            ivol10 = safe_series(first_col_containing(spec['assets'][0], spec['params']['f_short']))\n",
    "            if not ivol60.empty and not ivol10.empty: feat[feature_id] = ivol60 - ivol10\n",
    "        elif spec['type'] == 'ivol_skew':\n",
    "            put50 = safe_series(first_col_containing(spec['assets'][0], spec['params']['put']))\n",
    "            call40 = safe_series(first_col_containing(spec['assets'][0], spec['params']['call']))\n",
    "            if not put50.empty and not call40.empty: feat[feature_id] = put50 - call40\n",
    "        elif spec['type'] == 'ivol_shock':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix']))\n",
    "            if not ivol_s.empty: feat[feature_id] = (ivol_s.diff() - ivol_s.diff().rolling(spec['params']['window']).mean()) / ivol_s.diff().rolling(spec['params']['window']).std()\n",
    "        elif spec['type'] == 'ivol_div_volume':\n",
    "            ivol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_suffix']))\n",
    "            vol_s = safe_series(first_col_containing(spec['assets'][0], spec['params']['vol_suffix']))\n",
    "            if not ivol_s.empty and not vol_s.empty: feat[feature_id] = ivol_s / vol_s.replace(0, np.nan)\n",
    "        # SENTIMENT\n",
    "        elif spec['type'] == 'put_call_ratio_ema':\n",
    "            pc = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not pc.empty: feat[feature_id] = pc.ewm(span=spec['params']['span'], adjust=False).mean()\n",
    "        elif spec['type'] == 'open_interest_change':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not oi.empty: feat[feature_id] = oi.pct_change(spec['params']['days'])\n",
    "        elif spec['type'] == 'volume_zscore':\n",
    "            vol = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not vol.empty: feat[feature_id] = (vol - vol.rolling(spec['params']['window']).mean()) / vol.rolling(spec['params']['window']).std()\n",
    "        elif spec['type'] == 'smart_money_flag':\n",
    "            oi = safe_series(first_col_containing(spec['assets'][0], spec['params']['oi_col'])).pct_change() > 0\n",
    "            ivol = safe_series(first_col_containing(spec['assets'][0], spec['params']['ivol_col'])).pct_change() > 0\n",
    "            if not oi.empty and not ivol.empty: feat[feature_id] = (oi & ivol).astype(int)\n",
    "        # ADDITION: Generic Z-score calculation\n",
    "        elif spec['type'] == 'generic_zscore':\n",
    "            s = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not s.empty:\n",
    "                feat[feature_id] = (s - s.rolling(spec['params']['window']).mean()) / s.rolling(spec['params']['window']).std()\n",
    "        # CORRELATION\n",
    "        elif spec['type'] == 'correlation':\n",
    "            t1, t2 = spec['assets']; p1, p2 = first_col_containing(t1, spec['params']['col']), first_col_containing(t2, spec['params']['col'])\n",
    "            if p1 and p2:\n",
    "                aligned = pd.DataFrame({'s1': safe_series(p1), 's2': safe_series(p2)}).dropna()\n",
    "                if len(aligned) > spec['params']['window']: feat[feature_id] = aligned['s1'].rolling(spec['params']['window']).corr(aligned['s2'])\n",
    "        elif spec['type'] == 'advanced_correlation':\n",
    "            t1, t2 = spec['assets']\n",
    "            s1_col = first_col_containing(t1, spec['params']['col1']); s2_col = first_col_containing(t2, spec['params']['col2'])\n",
    "            if s1_col and s2_col:\n",
    "                aligned = pd.DataFrame({'s1': safe_series(s1_col), 's2': safe_series(s2_col)}).dropna()\n",
    "                if len(aligned) > spec['params']['window']: feat[feature_id] = aligned['s1'].rolling(spec['params']['window']).corr(aligned['s2'])\n",
    "        elif spec['type'] in ['correlation_zscore', 'correlation_delta']:\n",
    "            t1, t2 = spec['assets']; price_col_name = spec['params']['col']\n",
    "            c20_id = f'corr_{t1}:{price_col_name}_{t2}:{price_col_name}_20d'; c60_id = f'corr_{t1}:{price_col_name}_{t2}:{price_col_name}_60d'\n",
    "            c20 = feat.get(c20_id); c60 = feat.get(c60_id)\n",
    "            if c20 is not None and c60 is not None:\n",
    "                if spec['type'] == 'correlation_zscore':\n",
    "                    feat[feature_id] = (c20 - c20.rolling(spec['params']['window']).mean()) / c20.rolling(spec['params']['window']).std()\n",
    "                else: feat[feature_id] = c20 - c60\n",
    "        elif spec['type'] == 'rolling_beta':\n",
    "            t1, t2 = spec['assets']; p1, p2 = first_col_containing(t1, spec['params']['col']), first_col_containing(t2, spec['params']['col'])\n",
    "            if p1 and p2:\n",
    "                rets = pd.DataFrame({'r1': safe_series(p1).pct_change(), 'r2': safe_series(p2).pct_change()}).dropna()\n",
    "                if len(rets) > spec['params']['window']: feat[feature_id] = rets['r1'].rolling(spec['params']['window']).cov(rets['r2']) / rets['r2'].rolling(spec['params']['window']).var()\n",
    "        # MACRO\n",
    "        elif spec['type'] == 'macro_mpi':\n",
    "            dxy, ust10 = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not dxy.empty and not ust10.empty: feat[feature_id] = dxy.pct_change().rolling(3).sum() + ust10.pct_change().rolling(3).sum()\n",
    "        elif spec['type'] == 'macro_fear_overdrive':\n",
    "            vix, dxy, spy = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][2], 'PX_LAST'))\n",
    "            if not vix.empty and not dxy.empty and not spy.empty: feat[feature_id] = ((vix > 20) & (dxy.pct_change() > 0) & (spy < spy.rolling(20).mean())).astype(int)\n",
    "        elif spec['type'] == 'macro_sector_rotation':\n",
    "            xlk, xle = safe_series(first_col_containing(spec['assets'][0], 'PX_LAST')), safe_series(first_col_containing(spec['assets'][1], 'PX_LAST'))\n",
    "            if not xlk.empty and not xle.empty: feat[feature_id] = xlk.pct_change(5) - xle.pct_change(5)\n",
    "        # ... (rest of the macro features) ...\n",
    "        # --- ACTION 3: ADD CALCULATION LOGIC FOR REGIME AND INTERACTION ---\n",
    "        elif spec['type'] == 'regime_filter':\n",
    "            px = safe_series(first_col_containing(spec['assets'][0], spec['params']['col']))\n",
    "            if not px.empty: feat[feature_id] = px > spec['params']['threshold']\n",
    "        elif spec['type'] == 'interaction':\n",
    "            f1 = spec['params']['feature1']; f2 = spec['params']['feature2']\n",
    "            if f1 in feat.columns and f2 in feat.columns:\n",
    "                feat[feature_id] = feat[f1] * feat[f2]\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate feature '{feature_id}': {e}\")\n",
    "feat = feat.shift(1)\n",
    "print(f\"Calculated {feat.shape[1]} feature series.\")\n",
    "\n",
    "# --- ACTION 1: ADD NEW SEQUENTIAL FEATURES ---\n",
    "try:\n",
    "    # --- SEQ 1: VIX SPIKE -> CORR DROP ---\n",
    "    vix_vol_zscore_feat_name = 'zscore_IVOL_SIGMA_30d__VIX Index'\n",
    "    qqq_spy_corr_zscore_feat_name = 'zscore_corr20d_QQQ US Equity:PX_LAST_SPY US Equity:PX_LAST_60d'\n",
    "    event_A_series = (feat[vix_vol_zscore_feat_name] > 1.5)\n",
    "    event_B_series = (feat[qqq_spy_corr_zscore_feat_name] < -1.5)\n",
    "    sequential_feature_name = 'SEQ_VIX_SPIKE_THEN_CORR_DROP'\n",
    "    feat[sequential_feature_name] = event_B_series & event_A_series.shift(1)\n",
    "    print(f\"Successfully created sequential feature: '{sequential_feature_name}'\")\n",
    "\n",
    "    # --- SEQ 2: YIELD DROP -> GOLD VOL SPIKE ---\n",
    "    yield_zscore_name = 'zscore_PX_LAST_60d__USGG10YR Index'\n",
    "    gold_vol_zscore_name = 'zscore_IVOL_SIGMA_30d__GLD US Equity'\n",
    "    event_A_series = (feat[yield_zscore_name] < -1.5)\n",
    "    event_B_series = (feat[gold_vol_zscore_name] > 1.5)\n",
    "    sequential_feature_name = 'SEQ_YIELD_DROP_THEN_GOLD_VOL_SPIKE'\n",
    "    feat[sequential_feature_name] = event_B_series & event_A_series.shift(1)\n",
    "    print(f\"Successfully created sequential feature: '{sequential_feature_name}'\")\n",
    "\n",
    "    # --- SEQ 3: NVDA VOL -> QQQ PRICE ---\n",
    "    nvda_vol_zscore_name = 'zscore_Volume_-Realtime_VOLUME_30d__NVDA US Equity'\n",
    "    qqq_price_zscore_name = 'zscore_PX_LAST_60d__QQQ US Equity'\n",
    "    event_A_series = (feat[nvda_vol_zscore_name] > 1.5)\n",
    "    event_B_series = (feat[qqq_price_zscore_name] > 1.5)\n",
    "    sequential_feature_name = 'SEQ_NVDA_VOL_SPIKE_THEN_QQQ_PRICE_RISE'\n",
    "    feat[sequential_feature_name] = event_B_series & event_A_series.shift(1)\n",
    "    print(f\"Successfully created sequential feature: '{sequential_feature_name}'\")\n",
    "    # --- ADD THIS DIAGNOSTIC BLOCK ---\n",
    "    print(\"\\n--- DIAGNOSTIC: Checking Sequential Feature Support ---\")\n",
    "    print(f\"Support for SEQ_VIX_SPIKE_THEN_CORR_DROP: {feat['SEQ_VIX_SPIKE_THEN_CORR_DROP'].sum()}\")\n",
    "    print(f\"Support for SEQ_YIELD_DROP_THEN_GOLD_VOL_SPIKE: {feat['SEQ_YIELD_DROP_THEN_GOLD_VOL_SPIKE'].sum()}\")\n",
    "    print(f\"Support for SEQ_NVDA_VOL_SPIKE_THEN_QQQ_PRICE_RISE: {feat['SEQ_NVDA_VOL_SPIKE_THEN_QQQ_PRICE_RISE'].sum()}\")\n",
    "    print(\"---------------------------------------------------\\n\")\n",
    "    # --- END OF DIAGNOSTIC BLOCK ---\n",
    "except KeyError as e:\n",
    "    print(f\"Warning: Could not create sequential feature. A component feature was not found: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Define Primitive Signals from Features (with multiple condition types) ---\n",
    "print('--- Defining Primitive Signals ---')\n",
    "primitive_signals = []\n",
    "signal_series = {}\n",
    "signal_id_counter = 0\n",
    "for feature_id in feat.columns:\n",
    "    s = feat[feature_id].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty or (s.dtype != 'bool' and s.std() == 0): continue\n",
    "    # For boolean features (like our new sequential one), just check if it's true.\n",
    "    if s.dtype == 'bool':\n",
    "        sig_id = f\"SIG_{signal_id_counter}\"; signal_id_counter += 1\n",
    "        primitive_signals.append({'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'boolean', 'operator': '==', 'value': True})\n",
    "        signal_series[sig_id] = (s == True)\n",
    "        continue\n",
    "    # Percentile-based signals\n",
    "    for op, val in [('>', 0.8), ('<', 0.2)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\"; signal_id_counter += 1\n",
    "        primitive_signals.append({'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'percentile', 'operator': op, 'value': val})\n",
    "        signal_series[sig_id] = s.rank(pct=True).apply(lambda x: x > val if op == '>' else x < val)\n",
    "    # Z-score signals\n",
    "    rolling_std = s.rolling(60).std()\n",
    "    valid_std_mask = rolling_std > 1e-9\n",
    "    z = pd.Series(np.nan, index=s.index)\n",
    "    z[valid_std_mask] = (s - s.rolling(60).mean())[valid_std_mask] / rolling_std[valid_std_mask]\n",
    "    for op, val in [('>', 1.5), ('<', -1.5)]:\n",
    "        sig_id = f\"SIG_{signal_id_counter}\"; signal_id_counter += 1\n",
    "        primitive_signals.append({'signal_id': sig_id, 'feature_id': feature_id, 'condition_type': 'z_score', 'operator': op, 'value': val})\n",
    "        signal_series[sig_id] = z.apply(lambda x: x > val if op == '>' else x < val)\n",
    "print(f\"Defined {len(primitive_signals)} primitive signals.\")\n",
    "\n",
    "# --- 4. Generate Setups, Evaluate, and Output (The rest of the script is unchanged in structure) ---\n",
    "print('--- Generating Candidate Setups ---')\n",
    "all_candidate_setups = []\n",
    "setup_id_counter = 1\n",
    "for k in SETUP_LENGTHS_TO_EXPLORE:\n",
    "    signal_ids = [s['signal_id'] for s in primitive_signals]\n",
    "    if len(signal_ids) < k: continue\n",
    "    combinations_to_test = [random.sample(signal_ids, k) for _ in range(NUM_RANDOM_SETUPS_TO_SAMPLE)]\n",
    "    for sig_id_list in combinations_to_test:\n",
    "        try:\n",
    "            mask = functools.reduce(lambda a, b: a & b, [signal_series[sid] for sid in sig_id_list])\n",
    "            if mask.sum() >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "                signal_definitions = [p for p in primitive_signals if p['signal_id'] in sig_id_list]\n",
    "                all_candidate_setups.append({'id': f'S{setup_id_counter:04d}', 'signal_definitions': signal_definitions})\n",
    "                setup_id_counter += 1\n",
    "        except KeyError: continue\n",
    "print(f\"Generated {len(all_candidate_setups)} candidate setups.\")\n",
    "\n",
    "# Prepare Returns for Evaluation\n",
    "price_cols_for_returns = [first_col_containing(t, 'PX_LAST') for t in TRADABLE_TICKERS if first_col_containing(t, 'PX_LAST')]\n",
    "prices = raw[price_cols_for_returns].copy()\n",
    "returns = {h: prices.pct_change(h).shift(-h) for h in [1, 3, 5, 10, 21]}\n",
    "\n",
    "\n",
    "# --- NEW: Plain-English Description Generator ---\n",
    "def generate_english_description(setup_id, signal_defs, feature_specs_list):\n",
    "    clauses = []\n",
    "    for s_def in signal_defs:\n",
    "        feat_name = next((f_spec['display_name'] for f_spec in feature_specs_list if f_spec['unique_id'] == s_def['feature_id']), s_def.get('feature_id', 'unknown_feature'))\n",
    "        if s_def['condition_type'] == 'boolean':\n",
    "             clauses.append(f\"{feat_name} is true\")\n",
    "        elif s_def['condition_type'] == 'percentile':\n",
    "            level = \"is very high\" if s_def['operator'] == '>' else \"is very low\"\n",
    "            clauses.append(f\"{feat_name} {level}\")\n",
    "        else:\n",
    "            level = \"surges unexpectedly\" if s_def['operator'] == '>' else \"drops sharply\"\n",
    "            clauses.append(f\"{feat_name} {level}\")\n",
    "    description = f\"When {clauses[0]}\"\n",
    "    if len(clauses) > 1: description += f\" and {' and '.join(clauses[1:])}\"\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs)\n",
    "    bias = 'a bullish' if direction_score > 0 else 'a bearish' if direction_score < 0 else 'an uncertain'\n",
    "    description += f\", it may indicate {bias} outlook.\"\n",
    "    return {'setup_id': setup_id, 'description': description, 'explained_description': \"DEPRECATED\"}\n",
    "\n",
    "\n",
    "# --- Parallel Setup Evaluation Function (UPDATED) ---\n",
    "def evaluate_one_setup(setup):\n",
    "    sid, signal_defs = setup['id'], setup['signal_definitions']\n",
    "    try:\n",
    "        mask = functools.reduce(lambda a, b: a & b, [signal_series[s['signal_id']] for s in signal_defs])\n",
    "        dates = mask[mask].index\n",
    "    except (KeyError, TypeError): return [], [], None\n",
    "    if len(dates) < MIN_INITIAL_SUPPORT_FILTER: return [], [], None\n",
    "    direction_score = sum(1 if s['operator'] == '>' else -1 for s in signal_defs if s['condition_type'] != 'boolean')\n",
    "    if direction_score == 0 and any(s['condition_type'] != 'boolean' for s in signal_defs):\n",
    "        return [], [], None\n",
    "    elif all(s['condition_type'] == 'boolean' for s in signal_defs):\n",
    "        direction_score = 1 # Defaulting boolean/sequential setups to a long bias for now\n",
    "    entry_direction = 'long' if direction_score > 0 else 'short'\n",
    "    feature_types = [spec.get('type', 'unknown') for sig_def in signal_defs for spec in feature_specs if spec.get('unique_id') == sig_def.get('feature_id')]\n",
    "    dominant_signal_type = max(set(feature_types), key=feature_types.count) if feature_types else 'sequential' if any('SEQ' in s['feature_id'] for s in signal_defs) else 'unknown'\n",
    "    human_readable_conds = []\n",
    "    for s_def in signal_defs:\n",
    "        feat_name = s_def['feature_id'] # Use the direct feature ID for clarity\n",
    "        if s_def['condition_type'] == 'boolean':\n",
    "            cond_str = \"is true\"\n",
    "        elif s_def['condition_type'] == 'percentile':\n",
    "            cond_str = f\"{s_def['operator']} {s_def['value'] * 100:.0f}th percentile\"\n",
    "        else:\n",
    "            cond_str = f\"z-score {s_def['operator']} {s_def['value']}\"\n",
    "        human_readable_conds.append(f\"{feat_name} {cond_str}\")\n",
    "    description_record = generate_english_description(sid, signal_defs, feature_specs)\n",
    "    summary_rows_for_setup = []; trade_ledger_rows = []\n",
    "    for tk_col in price_cols_for_returns:\n",
    "        tk_symbol = next((ticker for ticker in TRADABLE_TICKERS if tk_col.startswith(ticker)), None)\n",
    "        if tk_symbol is None: continue\n",
    "        summary_row = {\n",
    "            'setup_id': sid, 'target_ticker': tk_symbol, 'feature_conditions': json.dumps(human_readable_conds),\n",
    "            'support': len(dates), 'entry_direction': entry_direction, 'dominant_signal_type': dominant_signal_type,\n",
    "            'first_trigger_date': dates.min(), 'last_trigger_date': dates.max()}\n",
    "        for h in [3, 5, 10, 21]:\n",
    "            r_ticker = returns[h][tk_col].reindex(dates)\n",
    "            if h == 3: summary_row['accuracy_3d'] = 0.0 if r_ticker.empty else (r_ticker > 0).mean()\n",
    "            if h == 5: summary_row['avg_return_5d'] = 0.0 if r_ticker.empty else r_ticker.mean()\n",
    "            if h == 21: summary_row['hit_rate_21d'] = 0.0 if r_ticker.empty else (r_ticker > 0).mean()\n",
    "            if not r_ticker.empty and r_ticker.std() > 1e-6 and len(r_ticker) > 5:\n",
    "                summary_row[f'sharpe_{h}d'], _, _ = block_bootstrap_sharpe(r_ticker, block_size=h)\n",
    "            else: summary_row[f'sharpe_{h}d'] = 0.0\n",
    "        trigger_df_for_ticker = pd.DataFrame([\n",
    "            {'date': d, 'underlying_entry_px': raw[tk_col].get(d),\n",
    "             **{f'underlying_exit_px_{h_opt}d': raw[tk_col].reindex([d + pd.Timedelta(days=h_opt)], method='nearest', tolerance=pd.Timedelta(days=3)).iloc[0]\n",
    "                if not raw[tk_col].reindex([d + pd.Timedelta(days=h_opt)], method='nearest', tolerance=pd.Timedelta(days=3)).empty else np.nan\n",
    "                for h_opt in OPTION_SIM_HORIZONS_DAYS}}\n",
    "            for d in dates]).reset_index(drop=True)\n",
    "        for h_opt in OPTION_SIM_HORIZONS_DAYS:\n",
    "            summary_row[f'avg_underlying_entry_px_{h_opt}d'] = trigger_df_for_ticker['underlying_entry_px'].mean()\n",
    "            summary_row[f'avg_underlying_exit_px_{h_opt}d'] = trigger_df_for_ticker[f'underlying_exit_px_{h_opt}d'].mean()\n",
    "            ivol_col = (first_col_containing(tk_symbol, '30_Day_Call_Implied_Volatility') or first_col_containing(tk_symbol, 'IVOL_SIGMA'))\n",
    "            ivol_series = raw[ivol_col].reindex(dates) if ivol_col else pd.Series(np.nan, index=dates)\n",
    "            for i, d in enumerate(dates):\n",
    "                pnl_detail = simulate_option_pnl_detailed(\n",
    "                    trigger_df_for_ticker.loc[i, 'underlying_entry_px'],\n",
    "                    trigger_df_for_ticker.loc[i, f'underlying_exit_px_{h_opt}d'],\n",
    "                    ivol_series.iloc[i] if not ivol_series.empty else np.nan, h_opt, entry_direction)\n",
    "                ledger_record = {'setup_id': sid, 'trigger_date': d, 'target_ticker': tk_symbol, 'horizon_days': h_opt,\n",
    "                                 'entry_direction': entry_direction, 'underlying_entry_px': trigger_df_for_ticker.loc[i, 'underlying_entry_px'],\n",
    "                                 'ivol_at_entry': ivol_series.iloc[i] if not ivol_series.empty else np.nan, **pnl_detail}\n",
    "                trade_ledger_rows.append(ledger_record)\n",
    "            pnl_series = pd.Series([trade['pnl_dollars'] for trade in trade_ledger_rows if trade['horizon_days'] == h_opt])\n",
    "            summary_row[f'avg_option_pnl_dollars_{h_opt}d'] = pnl_series.mean()\n",
    "        summary_rows_for_setup.append(summary_row)\n",
    "    return summary_rows_for_setup, trade_ledger_rows, description_record\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(f\"\\n--- Starting Parallel Evaluation of {len(all_candidate_setups)} Setups ---\")\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup) for setup in all_candidate_setups)\n",
    "summary_rows, description_records, all_trade_ledger_rows = [], [], []\n",
    "for perf_list, trade_list, desc in results:\n",
    "    if perf_list: summary_rows.extend(perf_list)\n",
    "    if trade_list: all_trade_ledger_rows.extend(trade_list)\n",
    "    if desc: description_records.append(desc)\n",
    "print('--- Generating Final Output Files ---')\n",
    "summary_df = pd.DataFrame(summary_rows); description_df = pd.DataFrame(description_records)\n",
    "if not summary_df.empty:\n",
    "    summary_df['setup_duration_days'] = (pd.to_datetime(summary_df['last_trigger_date']) - pd.to_datetime(summary_df['first_trigger_date'])).dt.days\n",
    "    summary_df['avg_trigger_frequency_per_day'] = summary_df['support'] / summary_df['setup_duration_days'].replace(0, np.nan)\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    summary_df.to_csv('setup_results_summary.csv', index=False); print(\"Saved 'setup_results_summary.csv'\")\n",
    "if not description_df.empty:\n",
    "    description_df.to_csv('setup_descriptions.csv', index=False); print(\"Saved 'setup_descriptions.csv'\")\n",
    "if all_trade_ledger_rows:\n",
    "    trade_ledger_df = pd.DataFrame(all_trade_ledger_rows)\n",
    "    cols = list(trade_ledger_df.columns)\n",
    "    if 'Underlying_Exit_Price' in cols and 'underlying_entry_px' in cols:\n",
    "      if 'Underlying_Exit_Price' in cols: cols.insert(cols.index('underlying_entry_px') + 1, cols.pop(cols.index('Underlying_Exit_Price')))\n",
    "      if 'Return_Underlying' in cols: cols.insert(cols.index('Underlying_Exit_Price') + 1, cols.pop(cols.index('Return_Underlying')))\n",
    "      trade_ledger_df = trade_ledger_df[cols]\n",
    "    trade_ledger_df.to_csv('trade_ledger.csv', index=False); print(\"Saved 'trade_ledger.csv'\")\n",
    "top_setups = summary_df.sort_values('sharpe_21d', ascending=False).head(20) if 'sharpe_21d' in summary_df.columns else pd.DataFrame()\n",
    "top_setups.to_json('top_setups.json', orient='records', indent=2); print(\"Saved 'top_setups.json'\")\n",
    "print('\\nDiscovery complete.')\n",
    "print(\"\\nTop Setups by Sharpe Ratio (21d):\")\n",
    "display_cols = ['setup_id', 'target_ticker', 'support', 'sharpe_3d', 'sharpe_21d', 'feature_conditions']\n",
    "print(top_setups[[c for c in display_cols if c in top_setups.columns]].head())"
   ],
   "id": "9c8e7735a0ac9721",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw workbooksâ€¦\n",
      "Raw shape: (1195, 469)\n",
      "\n",
      "Identified all relevant prefixes/tickers for feature engineering: 32\n",
      "\n",
      "--- Defining ALL Feature Specifications ---\n",
      "Defined 3178 total feature specifications.\n",
      "--- Calculating All Features ---\n",
      "Calculated 2845 feature series.\n",
      "Successfully created sequential feature: 'SEQ_VIX_SPIKE_THEN_CORR_DROP'\n",
      "Successfully created sequential feature: 'SEQ_YIELD_DROP_THEN_GOLD_VOL_SPIKE'\n",
      "Successfully created sequential feature: 'SEQ_NVDA_VOL_SPIKE_THEN_QQQ_PRICE_RISE'\n",
      "\n",
      "--- DIAGNOSTIC: Checking Sequential Feature Support ---\n",
      "Support for SEQ_VIX_SPIKE_THEN_CORR_DROP: 25\n",
      "Support for SEQ_YIELD_DROP_THEN_GOLD_VOL_SPIKE: 14\n",
      "Support for SEQ_NVDA_VOL_SPIKE_THEN_QQQ_PRICE_RISE: 0\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Defining Primitive Signals ---\n",
      "Defined 9459 primitive signals.\n",
      "--- Generating Candidate Setups ---\n",
      "Generated 77 candidate setups.\n",
      "\n",
      "--- Starting Parallel Evaluation of 77 Setups ---\n",
      "--- Generating Final Output Files ---\n",
      "Saved 'setup_results_summary.csv'\n",
      "Saved 'setup_descriptions.csv'\n",
      "Saved 'trade_ledger.csv'\n",
      "Saved 'top_setups.json'\n",
      "\n",
      "Discovery complete.\n",
      "\n",
      "Top Setups by Sharpe Ratio (21d):\n",
      "    setup_id   target_ticker  support  sharpe_3d  sharpe_21d  \\\n",
      "107    S0015   XLF US Equity       26   8.767395   27.679510   \n",
      "463    S0072  NBIS US Equity       27   1.637003   23.538317   \n",
      "105    S0015   SPY US Equity       26   9.625987   21.951908   \n",
      "462    S0072   GLD US Equity       27   1.502484   20.963304   \n",
      "116    S0015  NVDA US Equity       26  13.488854   20.066195   \n",
      "\n",
      "                                    feature_conditions  \n",
      "107  [\"corr_delta_NVDA US Equity:PX_LAST_SPCS20SM I...  \n",
      "463  [\"corr_DXY Curncy:PX_LAST_MLCX3CRT Index:PX_LA...  \n",
      "105  [\"corr_delta_NVDA US Equity:PX_LAST_SPCS20SM I...  \n",
      "462  [\"corr_DXY Curncy:PX_LAST_MLCX3CRT Index:PX_LA...  \n",
      "116  [\"corr_delta_NVDA US Equity:PX_LAST_SPCS20SM I...  \n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T19:56:34.745793Z",
     "start_time": "2025-08-04T19:11:33.524873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Engineering + Discovery Engine with FULLY Migrated Structured Features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import functools\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import linregress\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- DEFINITIONS AND CONFIGURATION ---\n",
    "# Define the explicit list of tradable tickers\n",
    "TRADABLE_TICKERS = [\n",
    "    'QQQ US Equity', 'SPY US Equity', 'XLK US Equity', 'XLF US Equity',\n",
    "    'XLE US Equity', 'ARKK US Equity', 'VIX Index', 'GLD US Equity',\n",
    "    'NBIS US Equity', 'LLY US Equity', 'TSLA US Equity', 'AAPL US Equity',\n",
    "    'NVDA US Equity'\n",
    "]\n",
    "\n",
    "# Define list of macro tickers to ensure their inclusion in feature generation\n",
    "MACRO_TICKERS = [\n",
    "    'DXY Curncy', 'USGG10YR Index', 'USGG2YR Index', 'CPI YOY Index',\n",
    "    'INJCJC Index', 'FFA Comdty', 'LF94TRUU Index', 'CPI CHNG Index',\n",
    "    'NFP TCH Index', 'JOBS US Equity', 'CTII10 Govt', 'USSW10 Curncy',\n",
    "    'MLCX3CRT Index', 'FARBAST Index', 'BSPGCPUS Index', 'SPCSUSA Index',\n",
    "    'SPCS20SM Index', 'CONSSENT Index', 'CO1 Comdty'\n",
    "]\n",
    "\n",
    "# Define the file paths\n",
    "MAIN_DATA_FILE = 'All_Tickers copy.xlsx'\n",
    "MACRO_DATA_FILE = 'Macro_tickers_no_nan_cols.xlsx'\n",
    "\n",
    "# --- ADDITION: GENETIC ALGORITHM CONFIGURATION ---\n",
    "NUM_GENERATIONS = 50       # How many evolutionary cycles to run\n",
    "POPULATION_SIZE = 50      # How many setups (individuals) in each generation\n",
    "SETUP_LENGTHS_TO_EXPLORE = [2, 3] # Allow setups of 2 or 3 conditions\n",
    "ELITISM_RATE = 0.1         # Percentage of the best setups to keep untouched for the next generation\n",
    "MUTATION_RATE = 0.05       # Probability of a random change in a setup's \"DNA\"\n",
    "\n",
    "# General Configuration\n",
    "MIN_INITIAL_SUPPORT_FILTER = 5\n",
    "OPTION_SIM_HORIZONS_DAYS = [1, 3, 10, 21]\n",
    "RISK_FREE_RATE = 0.01\n",
    "\n",
    "# --- END DEFINITIONS AND CONFIGURATION ---\n",
    "\n",
    "\n",
    "print('Loading raw workbooksâ€¦')\n",
    "\n",
    "\n",
    "# --- Custom Data Loading Function (Unchanged) ---\n",
    "def load_and_merge_excel(file_path, existing_df=None):\n",
    "    \"\"\"Loads an Excel file, prepends sheet names to columns (except Date), and merges.\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        current_df = existing_df.copy() if existing_df is not None else None\n",
    "        for sh_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name=sh_name)\n",
    "            if 'Dates' in df.columns and 'Date' not in df.columns:\n",
    "                df.rename(columns={'Dates': 'Date'}, inplace=True)\n",
    "            if 'Date' not in df.columns:\n",
    "                print(f\"Warning: Sheet '{sh_name}' in '{file_path}' is missing a 'Date'/'Dates' column. Skipping sheet.\")\n",
    "                continue\n",
    "            df.columns = [f\"{sh_name}_{col}\" if col != 'Date' else col for col in df.columns]\n",
    "            if current_df is None:\n",
    "                current_df = df\n",
    "            else:\n",
    "                df = df.loc[:,~df.columns.duplicated()]\n",
    "                current_df = current_df.merge(df, on='Date', how='outer')\n",
    "        return current_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{file_path}' not found.\")\n",
    "        return existing_df\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Excel loading of '{file_path}': {e}\")\n",
    "        return existing_df\n",
    "\n",
    "\n",
    "# Load main and macro data\n",
    "raw = load_and_merge_excel(MAIN_DATA_FILE)\n",
    "if raw is not None and not raw.empty:\n",
    "    raw = load_and_merge_excel(MACRO_DATA_FILE, existing_df=raw)\n",
    "else:\n",
    "    print(\"Main data could not be loaded, skipping macro data loading.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "# Final cleaning and indexing\n",
    "if not raw.empty:\n",
    "    raw = raw.sort_values('Date').reset_index(drop=True)\n",
    "    raw.fillna(method='ffill', inplace=True)\n",
    "    if 'Date' in raw.columns:\n",
    "        raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "        raw = raw.drop_duplicates(subset=['Date'], keep='last')\n",
    "        raw.set_index('Date', inplace=True)\n",
    "        raw.index = pd.to_datetime(raw.index)\n",
    "        raw.sort_index(inplace=True)\n",
    "else:\n",
    "    print(\"No data loaded. Raw DataFrame is empty.\")\n",
    "    raw = pd.DataFrame()\n",
    "\n",
    "print('Raw shape:', raw.shape)\n",
    "\n",
    "# --- Dynamic Ticker Identification (Unchanged) ---\n",
    "all_column_prefixes = sorted(list(set([c.split('_')[0] for c in raw.columns if '_' in c])))\n",
    "COMMON_FEATURE_PREFIXES = ['Last', 'Open', 'High', 'Low', 'VWAP', 'Volume', 'IVOL', 'Implied', 'Total', '30', '10',\n",
    "                           '60', 'Hist.', '1st', 'Put', 'Dates', 'CHG', 'FFA', 'INJCJC', 'NFP', 'JOBS', 'CPI', 'CTII10',\n",
    "                           'LF94TRUU', 'SPX', 'USSW10', 'MLCX3CRT', 'FARBAST', 'BSPGCPUS', 'SPCSUSA', 'SPCS20SM',\n",
    "                           'CONSSENT']\n",
    "actual_ticker_prefixes = [p for p in all_column_prefixes if p not in COMMON_FEATURE_PREFIXES]\n",
    "all_tickers = sorted(list(set(TRADABLE_TICKERS + actual_ticker_prefixes + MACRO_TICKERS)))\n",
    "print(f'\\nIdentified all relevant prefixes/tickers for feature engineering: {len(all_tickers)}')\n",
    "\n",
    "\n",
    "# --- Helper functions (Unchanged) ---\n",
    "def first_col_containing(ticker_full_name, substr=''):\n",
    "    if substr == 'PX_LAST':\n",
    "        for potential_col in [f\"{ticker_full_name}_Last_Price_PX_LAST\", f\"{ticker_full_name}_PX_LAST\"]:\n",
    "            if potential_col in raw.columns: return potential_col\n",
    "    for c in raw.columns:\n",
    "        if c.startswith(ticker_full_name) and substr in c: return c\n",
    "    return None\n",
    "\n",
    "def safe_series(col_name):\n",
    "    return raw[col_name] if col_name and col_name in raw.columns else pd.Series(index=raw.index, dtype=float)\n",
    "\n",
    "def frac_diff(series, d=0.5, window=100):\n",
    "    weights = [1.];\n",
    "    for k in range(1, len(series)): weights.append(-weights[-1] * (d - k + 1) / k)\n",
    "    weights = np.array(weights[::-1]); output = pd.Series(index=series.index, dtype=float)\n",
    "    for i in range(window, len(series)):\n",
    "        subset = series.iloc[i - window + 1: i + 1]\n",
    "        if len(subset) == len(weights[-window:]): output.iloc[i] = np.dot(weights[-window:], subset)\n",
    "    return output.dropna()\n",
    "\n",
    "def block_bootstrap_sharpe(returns_series, block_size, num_iterations=1000, annualize=True, trading_days_per_year=252):\n",
    "    returns_series = returns_series.dropna()\n",
    "    if len(returns_series) < block_size or len(returns_series) < 2: return 0.0, 0.0, 0.0\n",
    "    blocks = [returns_series.iloc[i: i + block_size] for i in range(0, len(returns_series), block_size) if not returns_series.iloc[i: i + block_size].empty]\n",
    "    if not blocks: return 0.0, 0.0, 0.0\n",
    "    n_blocks_to_sample = int(np.ceil(len(returns_series) / block_size)); sharpes = []\n",
    "    for _ in range(num_iterations):\n",
    "        resampled_returns_list = [blocks[i] for i in np.random.choice(len(blocks), n_blocks_to_sample, replace=True)]\n",
    "        resampled_returns = pd.concat(resampled_returns_list).iloc[:len(returns_series)]\n",
    "        if resampled_returns.std() > 1e-9:\n",
    "            sharpes.append((resampled_returns.mean() / resampled_returns.std()) * (np.sqrt(trading_days_per_year) if annualize else 1))\n",
    "        else: sharpes.append(0.0)\n",
    "    if not sharpes: return 0.0, 0.0, 0.0\n",
    "    return np.median(sharpes), np.percentile(sharpes, 5), np.percentile(sharpes, 95)\n",
    "\n",
    "# --- Option Simulation Helpers (Unchanged) ---\n",
    "def estimate_atm_premium(price, ivol, days, option_type):\n",
    "    T = days / 365.25\n",
    "    if T <= 0 or price <= 0 or ivol <= 0: return 0\n",
    "    return 0.4 * price * ivol * np.sqrt(T)\n",
    "\n",
    "def simulate_option_pnl_detailed(current_price, future_price, ivol_at_entry, horizon_days, entry_direction):\n",
    "    underlying_return = (future_price - current_price) / current_price if current_price and pd.notna(current_price) and pd.notna(future_price) else np.nan\n",
    "    nan_result = {'pnl_per_share': np.nan, 'option_type': None, 'strike_price': np.nan, 'entry_premium': np.nan,\n",
    "                  'exit_value': np.nan, 'pnl_dollars': np.nan, 'skipped_reason': 'None',\n",
    "                  'Underlying_Exit_Price': future_price if pd.notna(future_price) else np.nan, 'Return_Underlying': underlying_return,}\n",
    "    if pd.isna(current_price) or current_price <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Price'; return nan_result\n",
    "    if pd.isna(ivol_at_entry) or ivol_at_entry <= 0:\n",
    "        nan_result['skipped_reason'] = 'Invalid IVOL'; return nan_result\n",
    "    if pd.isna(future_price):\n",
    "        nan_result['skipped_reason'] = 'Missing Future Price'; return nan_result\n",
    "    if entry_direction not in ['long', 'short']:\n",
    "        nan_result['skipped_reason'] = 'Invalid Entry Direction'; return nan_result\n",
    "    scaled_ivol = ivol_at_entry / 100.0 if ivol_at_entry > 1.0 else ivol_at_entry\n",
    "    strike_price = current_price\n",
    "    option_type = 'call' if entry_direction == 'long' else 'put'\n",
    "    entry_premium = estimate_atm_premium(current_price, scaled_ivol, horizon_days, option_type)\n",
    "    if option_type == 'call': exit_value = max(future_price - strike_price, 0)\n",
    "    else: exit_value = max(strike_price - future_price, 0)\n",
    "    pnl_per_share = exit_value - entry_premium\n",
    "    pnl_dollars = pnl_per_share * 100\n",
    "    return {'pnl_per_share': pnl_per_share, 'option_type': option_type, 'strike_price': strike_price,\n",
    "            'entry_premium': entry_premium, 'exit_value': exit_value, 'pnl_dollars': pnl_dollars,\n",
    "            'skipped_reason': 'None', 'Underlying_Exit_Price': future_price, 'Return_Underlying': underlying_return,}\n",
    "\n",
    "# --- Feature & Signal Generation (Unchanged) ---\n",
    "# NOTE: The entire block for defining and calculating features and signals is unchanged.\n",
    "# For brevity, it is collapsed here. The script will use the same features as before.\n",
    "print('\\n--- SKIPPING FEATURE/SIGNAL DEFINITION FOR BREVITY (LOGIC UNCHANGED) ---')\n",
    "# --- [PASTE THE ENTIRE FEATURE & SIGNAL GENERATION SCRIPT BLOCK HERE] ---\n",
    "# It should start with: print('\\n--- Defining ALL Feature Specifications ---')\n",
    "# And end with: print(f\"Defined {len(primitive_signals)} primitive signals.\")\n",
    "# --- [END OF COLLAPSED SCRIPT BLOCK] ---\n",
    "\n",
    "# --- 4. GENETIC ALGORITHM: Evolve Powerful Setups ---\n",
    "\n",
    "# --- ADDITION: GENETIC ALGORITHM HELPERS ---\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"Creates a new child setup by combining DNA from two parents.\"\"\"\n",
    "    # Simple crossover: take one signal from each parent\n",
    "    child_signals = [random.choice(parent1['signal_definitions']), random.choice(parent2['signal_definitions'])]\n",
    "    # Ensure no duplicate signals in the child\n",
    "    child_signals = list({s['signal_id']: s for s in child_signals}.values())\n",
    "    return {'id': 'child', 'signal_definitions': child_signals}\n",
    "\n",
    "def mutate(setup, all_signal_ids, mutation_rate):\n",
    "    \"\"\"Randomly changes one signal in a setup's DNA.\"\"\"\n",
    "    if random.random() < mutation_rate:\n",
    "        # Select a random signal in the setup to replace\n",
    "        idx_to_mutate = random.randint(0, len(setup['signal_definitions']) - 1)\n",
    "        # Select a new random signal from the entire pool\n",
    "        new_signal_id = random.choice(all_signal_ids)\n",
    "        # Find the full definition of the new signal\n",
    "        new_signal_def = next(p for p in primitive_signals if p['signal_id'] == new_signal_id)\n",
    "        setup['signal_definitions'][idx_to_mutate] = new_signal_def\n",
    "    return setup\n",
    "\n",
    "# --- Step 1: Create Initial Population (Generation 0) ---\n",
    "print('\\n--- GENETIC ALGORITHM: Creating Initial Population (Generation 0) ---')\n",
    "all_signal_ids = [s['signal_id'] for s in primitive_signals]\n",
    "current_population = []\n",
    "setup_id_counter = 0\n",
    "while len(current_population) < POPULATION_SIZE:\n",
    "    k = random.choice(SETUP_LENGTHS_TO_EXPLORE)\n",
    "    sig_id_list = random.sample(all_signal_ids, k)\n",
    "    mask = functools.reduce(lambda a, b: a & b, [signal_series[sid] for sid in sig_id_list])\n",
    "    if mask.sum() >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "        signal_definitions = [p for p in primitive_signals if p['signal_id'] in sig_id_list]\n",
    "        current_population.append({'id': f'S{setup_id_counter:04d}', 'signal_definitions': signal_definitions})\n",
    "        setup_id_counter += 1\n",
    "print(f\"Created initial population of {len(current_population)} setups.\")\n",
    "\n",
    "# --- Step 2: The Main Evolutionary Loop ---\n",
    "for generation in range(NUM_GENERATIONS):\n",
    "    # --- Step 3: Evaluate Fitness of the current population ---\n",
    "    print(f\"\\n--- Evaluating Generation {generation + 1}/{NUM_GENERATIONS} ---\")\n",
    "    results = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup) for setup in current_population)\n",
    "\n",
    "    # Process results and calculate fitness (average sharpe_21d across tickers)\n",
    "    setup_fitness = {}\n",
    "    for i, (perf_list, _, _) in enumerate(results):\n",
    "        setup_id = current_population[i]['id']\n",
    "        if perf_list:\n",
    "            sharpes = [row.get('sharpe_21d', 0) for row in perf_list]\n",
    "            avg_sharpe = np.mean(sharpes) if sharpes else 0\n",
    "            setup_fitness[setup_id] = avg_sharpe\n",
    "        else:\n",
    "            setup_fitness[setup_id] = -99 # Penalize setups that failed evaluation\n",
    "\n",
    "    # Combine setups with their fitness scores\n",
    "    population_df = pd.DataFrame(current_population)\n",
    "    population_df['fitness'] = population_df['id'].map(setup_fitness).fillna(-99)\n",
    "    population_df = population_df.sort_values('fitness', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    best_fitness = population_df['fitness'].iloc[0]\n",
    "    print(f\"Generation {generation + 1} Complete. Best Fitness (Avg Sharpe_21d): {best_fitness:.2f}\")\n",
    "\n",
    "    # --- Step 4: Selection & Breeding to Create the Next Generation ---\n",
    "    next_generation = []\n",
    "\n",
    "    # Elitism: Keep the best individuals\n",
    "    num_elites = int(POPULATION_SIZE * ELITISM_RATE)\n",
    "    elites = population_df.iloc[:num_elites].to_dict('records')\n",
    "    next_generation.extend(elites)\n",
    "\n",
    "    # Breeding Pool: Select parents from the top 50% of the population\n",
    "    breeding_pool = population_df.iloc[:int(POPULATION_SIZE / 2)].to_dict('records')\n",
    "\n",
    "    # Crossover & Mutation\n",
    "    while len(next_generation) < POPULATION_SIZE:\n",
    "        parent1 = random.choice(breeding_pool)\n",
    "        parent2 = random.choice(breeding_pool)\n",
    "        child = crossover(parent1, parent2)\n",
    "        child = mutate(child, all_signal_ids, MUTATION_RATE)\n",
    "\n",
    "        # Check if the new child is valid before adding\n",
    "        try:\n",
    "            mask = functools.reduce(lambda a, b: a & b, [signal_series[s['signal_id']] for s in child['signal_definitions']])\n",
    "            if mask.sum() >= MIN_INITIAL_SUPPORT_FILTER:\n",
    "                child['id'] = f'S{setup_id_counter:04d}'; setup_id_counter += 1\n",
    "                next_generation.append(child)\n",
    "        except (KeyError, TypeError):\n",
    "            continue\n",
    "\n",
    "    current_population = next_generation\n",
    "\n",
    "# --- Final Evaluation and Output Generation ---\n",
    "print(\"\\n--- Genetic Algorithm Complete. Running Final Evaluation ---\")\n",
    "final_results = Parallel(n_jobs=-1)(delayed(evaluate_one_setup)(setup) for setup in current_population)\n",
    "\n",
    "summary_rows, description_records, all_trade_ledger_rows = [], [], []\n",
    "for perf_list, trade_list, desc in final_results:\n",
    "    if perf_list: summary_rows.extend(perf_list)\n",
    "    if trade_list: all_trade_ledger_rows.extend(trade_list)\n",
    "    if desc: description_records.append(desc)\n",
    "\n",
    "print('--- Generating Final Output Files ---')\n",
    "# ... (The rest of the output generation script is unchanged) ...\n",
    "# --- [PASTE THE ENTIRE \"Final Output Generation\" SCRIPT BLOCK HERE] ---"
   ],
   "id": "c1a012e5afa78d89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw workbooksâ€¦\n",
      "Raw shape: (1195, 469)\n",
      "\n",
      "Identified all relevant prefixes/tickers for feature engineering: 32\n",
      "\n",
      "--- SKIPPING FEATURE/SIGNAL DEFINITION FOR BREVITY (LOGIC UNCHANGED) ---\n",
      "\n",
      "--- GENETIC ALGORITHM: Creating Initial Population (Generation 0) ---\n",
      "Created initial population of 50 setups.\n",
      "\n",
      "--- Evaluating Generation 1/50 ---\n",
      "Generation 1 Complete. Best Fitness (Avg Sharpe_21d): 12.97\n",
      "\n",
      "--- Evaluating Generation 2/50 ---\n",
      "Generation 2 Complete. Best Fitness (Avg Sharpe_21d): 20.06\n",
      "\n",
      "--- Evaluating Generation 3/50 ---\n",
      "Generation 3 Complete. Best Fitness (Avg Sharpe_21d): 20.15\n",
      "\n",
      "--- Evaluating Generation 4/50 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 236\u001B[39m\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m generation \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(NUM_GENERATIONS):\n\u001B[32m    234\u001B[39m     \u001B[38;5;66;03m# --- Step 3: Evaluate Fitness of the current population ---\u001B[39;00m\n\u001B[32m    235\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m--- Evaluating Generation \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneration\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_GENERATIONS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m ---\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     results = \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_one_setup\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43msetup\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msetup\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcurrent_population\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    238\u001B[39m     \u001B[38;5;66;03m# Process results and calculate fitness (average sharpe_21d across tickers)\u001B[39;00m\n\u001B[32m    239\u001B[39m     setup_fitness = {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/alpha_discoveryV2/.venv/lib/python3.13/site-packages/joblib/parallel.py:2072\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2070\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2072\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/alpha_discoveryV2/.venv/lib/python3.13/site-packages/joblib/parallel.py:1682\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1679\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1681\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1684\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/alpha_discoveryV2/.venv/lib/python3.13/site-packages/joblib/parallel.py:1800\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1797\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1798\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1799\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1800\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1801\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1803\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1805\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
